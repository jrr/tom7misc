Initializing the network and learning rate is a dark art!
For several of my initial attempts, the network would just predict the same
thing apparently forever.


-- for these, weights and biases were initialized to .000025 * (rand() - 0.5f). --

I got it to do something useful with a single intermediate layer,
16x16, fully connected. Examples per round was a paltry 12, so it was
doing like 30+ rounds per second (and GPU utilization was only like
50%). The learning rate ended up being 0.95 (!) and it still takes 3k
rounds to get somewhere. The symptom is that the intermediate layer is
all very close to 0.5f activation, so I guess that starting with
really small random weights means that these nodes are not very
differentiated.


Increasing it to 3 channels on the intermediate layer, it was stuck
even after 30k rounds. Then I let the learning rate go up to 2.82, and
it still stayed in the grey zone. After 50k rounds it was still stuck.


Back to 1 channel, but unlimited learning rate; got stuck in 15k.

1 channel, 0.95 (same as first success): After 6k rounds it is doing
something.

With 10x10 intermediate layer (still fully connected), it's "working"
pretty much right away (100 rounds).

With 32x32 intermediate layer, still stuck after 14k rounds.

With 32x32 intermediate layer, but only 100 connections for the last
layer, immediately starts to do something.

So maybe what happens is that we sum so many input edges that the
sigmoids are totally saturated, so the derivative is nearly flat and
error propagation does very little even if the learning rate is high.
(This sounds like it's a common problem: See wikipedia on the
"Vanishing gradient problem".) According to one rando on the internet,
"there is no vanishing gradient problem for biases. they can be
safely initialized to 0."

Then I did 6 internal layers, each 32x32 with 64 incoming edges.
This took a lot longer to fire up (top two layers were grey for a
while) but by 30k rounds it is at least not grey on each layer.
But at 40k rounds it was stuck predicting the same thing always.


ok, same network, but using leaky_relu for all but the last layer.
after 30k rounds (64 examples per round) it just seems stuck...


Started drawing/printing the updates ("errors" = error * derivative).
They are indeed extremely close to zero, even after a few rounds.

Temporarily switched to leaky_relu everywhere. This yielded a different
problem, which was NaNs after just 3 rounds. What was happening was
that the biases (also weights) were getting crazy big ("gradient
explosion"?); I guess the updates can compound exponentially per layer
if there are lots of large ones. I did two things to fix:
  1. cap the update to be in [-1, 1] within updateweights.cl
  2. Set the learning rate to depend on the number of examples per
     round. Without this, the errors can easily be very large
     (e.g. if there are 100 examples, then it is common to have
     an error of 100!). 
(1) didn't help on its own, so maybe it should just be removed; it
makes the slowest step slower!
(2) Did make a huge difference; with just that change a two-hidden-layer
fully connected network of just leaky_relu is producing reasonable
outputs. (After 1200 rounds, total error = 18)

Phew!

 with 3 hidden relu layers, after 1300 rounds (64 ex per round), ~18 error
 at 2800 rounds, ~15.3 error
 at 17k rounds, ~13 error (note learning rate has dropped to 0.0020)
 so at 17900 I switched to a linear decline from 0.1 to 0.002 with a target
 of 200,000 rounds.
 at 31164 rounds, 13.74 error
 50k rounds, 13-14 error
 64k rounds, 12 error
 82k rounds, 12 error
 96k rounds, 11.5-13 error
 118k rounds, 10.9-12.5 error
 141k rounds, 10.6-12.8
 
Good writeup with some actual parameters used (although for a pretty different
problem):
https://engineering.flipboard.com/2015/05/scaling-convnets

TODO:
 - might want historic graph of total error over time?


After 64k rounds, it does seem to be making slow progress still, but
it has some common mistakes, like it loves putting multiple kings on
the board. Is it possible that

 haha omg, it's just that I was printing both knights and kings as K. :)

 wow much better.

Still, there are cases where it does silly stuff like more than 8
pawns of one color, or two same-color bishops. I don't see rampant
multiple kings any more but nothing prevents it. Actually it does seem
fairly common for there to be no kings, or for one side to be missing
a king. Could we help it produce the right features by structuring the
error more? Like we could ask it to produce the single position for
each king, or the locations of each of the 32 pieces, or something?
32 * 65 bits (one for "dead") is 2080, which is reasonable. Note that
this does not supersede the current board represenation, since it would
not be able to tell us about promotions without additional data.

Some hard constraints:
 - If all of one side's pawns are on the board,
    - No more than 2 rooks, 2 bishops (different colors), 2 knights, 1 queen
 - Exactly one king on each side
 - If check, then this determines the move
 - Both sides can't both be in check
 - No pawns in the extremal rows

There are many soft heuristics, too. Three knights in the early game
is just exceptionally rare, even if technically possible through promotion.

It also seems pretty reluctant to put white pieces in the black ranks and
vice versa. Makes sense, but this will be a weakness when used to play!


New offline bench, 126k rounds:
In 50000 positions:
  10482 exactly correct (20.96%)
  156987 piece mistakes (3.14/pos)
  1552 castling mistakes (0.03/pos)
  18623 move mistakes (0.37/pos)
  177162 total mistakes (3.54/pos)

141k rounds:
In 50000 positions:
  10579 exactly correct (21.16%)
  156156 piece mistakes (3.12/pos)
  1544 castling mistakes (0.03/pos)
  18606 move mistakes (0.37/pos)
  176306 total mistakes (3.53/pos)

...
In 50000 positions:
  10377 exactly correct (20.75%)
  156376 piece mistakes (3.13/pos)
  1556 castling mistakes (0.03/pos)
  19242 move mistakes (0.38/pos)
  177174 total mistakes (3.54/pos)


-- vacuuming --
Training speed before: 27.6725 eps
