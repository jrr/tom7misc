Initializing the network and learning rate is a dark art!
For several of my initial attempts, the network would just predict the same
thing apparently forever.


-- for these, weights and biases were initialized to .000025 * (rand() - 0.5f). --

I got it to do something useful with a single intermediate layer,
16x16, fully connected. Examples per round was a paltry 12, so it was
doing like 30+ rounds per second (and GPU utilization was only like
50%). The learning rate ended up being 0.95 (!) and it still takes 3k
rounds to get somewhere. The symptom is that the intermediate layer is
all very close to 0.5f activation, so I guess that starting with
really small random weights means that these nodes are not very
differentiated.


Increasing it to 3 channels on the intermediate layer, it was stuck
even after 30k rounds. Then I let the learning rate go up to 2.82, and
it still stayed in the grey zone. After 50k rounds it was still stuck.


Back to 1 channel, but unlimited learning rate; got stuck in 15k.

1 channel, 0.95 (same as first success): After 6k rounds it is doing
something.

With 10x10 intermediate layer (still fully connected), it's "working"
pretty much right away (100 rounds).

With 32x32 intermediate layer, still stuck after 14k rounds.

With 32x32 intermediate layer, but only 100 connections for the last
layer, immediately starts to do something.

So maybe what happens is that we sum so many input edges that the
sigmoids are totally saturated, so the derivative is nearly flat and
error propagation does very little even if the learning rate is high.
(This sounds like it's a common problem: See wikipedia on the
"Vanishing gradient problem".) According to one rando on the internet,
"there is no vanishing gradient problem for biases. they can be
safely initialized to 0."

Then I did 6 internal layers, each 32x32 with 64 incoming edges.
This took a lot longer to fire up (top two layers were grey for a
while) but by 30k rounds it is at least not grey on each layer.
But at 40k rounds it was stuck predicting the same thing always.


ok, same network, but using leaky_relu for all but the last layer.
after 30k rounds (64 examples per round) it just seems stuck...


Started drawing/printing the updates ("errors" = error * derivative).
They are indeed extremely close to zero, even after a few rounds.

Temporarily switched to leaky_relu everywhere. This yielded a different
problem, which was NaNs after just 3 rounds. What was happening was
that the biases (also weights) were getting crazy big ("gradient
explosion"?); I guess the updates can compound exponentially per layer
if there are lots of large ones. I did two things to fix:
  1. cap the update to be in [-1, 1] within updateweights.cl
  2. Set the learning rate to depend on the number of examples per
     round. Without this, the errors can easily be very large
     (e.g. if there are 100 examples, then it is common to have
     an error of 100!). 
(1) didn't help on its own, so maybe it should just be removed; it
makes the slowest step slower!
(2) Did make a huge difference; with just that change a two-hidden-layer
fully connected network of just leaky_relu is producing reasonable
outputs. (After 1200 rounds, total error = 18)

Phew!

 with 3 hidden relu layers, after 1300 rounds (64 ex per round), ~18 error
 at 2800 rounds, ~15.3 error

Good writeup with some actual parameters used (although for a pretty different
problem):
https://engineering.flipboard.com/2015/05/scaling-convnets

TODO:
 - might want historic graph of total error over time?

