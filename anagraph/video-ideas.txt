
TO-DO:
 - make regular letters version
    (should allow multiple paths? or just ugly i,j?)
Optional:
 - kerning?!
 - could improve routing in planner
 - generate tree in sml, but explore anagrams in js (table idea)

fun stuff:
 - pixels
 - 7 segment display
 - absurd letter decompositions like:

 s =    -
       c
        -
         )
        -
        
computer science stuff:
 - post correspondence problem


 video ideas:
 start with like, "if I can rearrange the letters,
                   why can't I flip them around?"
 .. then generalize.
 hamburgefontsiv
 linear logic
 loops u -> v -> u -> v -> ...
 extreme version: 7 segment display
 make a font that actually realizes this compositionality
 animate word transitions

"Self driving car-achters"

ANAGRAPH GENERATOR

very educational video

some that don't need to break apart letters:

b and p are not actually the same shape tho :(
  preposterous = obstreperous

  digital = pigtail

some nice pairs:

  impeach groper = orange diarrhea

  vulnerability = authenticity

  donald trump = unnatural clod
                 plutocrat man

  covfefe = pee off

  youtube = fun alone = worry etc

  they might be giants = gesticulating mutely

pixel font:
  facebook = priapism = typeface = ...

  donald trump = worst hair job

Unless you've been "undemonstrably workaday",
"glarorously thumbtacked", 
you know what anagrams are.

Unless you've been portmanteau wordplay shy,
you know what anagrams are.

Unless you wage a tyrannosaur mohawk,
you know what anagrams are.


INV
- donald trump = drum up a plot
- youtube = by tenon
- quad / band, 
- pd: flipping / fiddling, pigtail / digital
- qb: question / bounties, 
- un: tofu / font
- all: quip, bind

"allow rotating letters" inv:
  - gaslit waterloo turtle
  - tolerate rotting walls !
  
worst anagrams
photomicrograph
microphotograph
 jeremy's iron

stuff to try:
 - juicero



[ MORE ABOUT WORDS ! ]

$ make && time ./anagraph knowwhatanagramsare -maxwords 3 -banned raga,ragas,agar,agars,grana,skag,manana,kasha,sanka,kanas,sanga,ankara,agama,gratae,anagram,organa,ogham,swagman,tanka,astrakhan,tanager,anagrams,mananas,anna,ghana,wark,know,hank,khan,rara,rowan,wantage,goshawk,shako,rowans,warks,anther,thanes > deleteme2

Unless you warrant a swank homage,
you know what anagrams are!

Rearranging letters to make different words.


But if you're doing this with physical letters
you know, not imaginary letters but real ones,
then why not allow me to rearrange the letters
any way I want? Like I could rearrange

  jaw to jam

or speaking of jaw jams,

  bechamel     to     chewable




I call these anagraphs, because "anaglyph" is already taken
by these kind of 3d glasses (zelda graphic) for some reason.
anagraph comes from "anag", which means anagram, and "raph",
which means "rotate and pluck h______"


kerfning for the cut away part

next:
 - bad ways, like s = two cs
 - show the whole alphabet
 - show the font program
 - talk about how to generate anagraphs recursively
  - decompose a letter into its set of atoms
  - for single words, we can just check to see if they have the
    same atoms.
  - if not, pick some word that's a subset, and then recurse
  - inception rant
 - mention how this requires canonical breakdowns
   - show problem with d = "cl" or "ol"
   - this allows me to make infinite cs, os, etc.
   - the c's get kinda dumpy. conservation of matter
   - should clarify in this part that I'm thinking about the case
     that I allow for really weird letters and rules
   - note that in the real world, there's this "kerf"
      aka "kerfning".. btw kerning is impossible
   - reference other vid
 - further generalizations (pixels)
 - end with some good anagraphs I guess


** undecidability of "generalized kerning" ** 


my editorial board (THIS GUY) decided that this was "too boring"
to include in the original video

(Reference other video as intro and more fun. Tom Academy)
What is the point of this video? 

This video is pretty much a serious educational video about a topic
that I find interesting. If you feel you already know this stuff,
I encourage you to try to anticipate where I'm going. In particular,
I'd like to make a point about what passes for a proof and how easy
it is to be mistaken. So try to notice the places where I make
a mistep before I point them out.

So I talked about the anagraph problem, and the related kerning problem.
These are very similar but one is possible and the other is impossible!
We use a similar technique to prove both results, called reduction.
So in this episode I'm going to describe those two problems precisely,
then wield that reduction technique in two opposite ways to prove that
one problem is possible and the other is not.



   
Let's first recap the problems.
(Should briefly describe anagraphing
to make this standalone from the other video)
In the other video I was a bit loose about this, but it's important
to be precise if we're going to prove something. I also want to emphasize
that these problems are both easy for English letters. So we're really
talking about the generalized problem. You have some alphabet of letters:

    a b  c   ... alien letters

Again, I'm generalizing, so I'll have letters that don't exist in
English. Also since we break apart letters, there may be some letters
in that list that aren't part of English, like the hook. In the previous
video I distinguished between "atoms" and "letters", but here they are
all the same.
Then a word is just made up of a sequence of letters. The essence
here is then these rules that let me "break apart" and "combine" letter
sequences to make other letter sequences. For example we might have

    rn <=> m

or even

    AA <=> TVT

The rules operate in either direction. There can be more than one way
to rewrite a particular sequence. We saw

    d <=> ol
    d <=> cl

and how this leads to the generation of an infinite number of Cs. So
we don't have conservation of matter or anything. But in the
generalized problem, we don't even require that the rules be
physically realistic. I could just have

    l <=> ll

which is a little bit plausible, or like

    cat <=> dog

(BTW it's probably more accurate to call this the "generalized ligature
problem," but kerning somehow commands a certain mystique. (...))

So now the generalized anagraph problem can be stated simply as

  Given an alphabet,
  and some rewrite rules,
  and the ability to rearrange letters,
  and two words,
  can you turn one word into the other?

This problem is solvable, but in my opinion it's not obvious. Let's
start with the unsolvable problem, the generalized kerning problem:

  Given an alphabet,
  and some rewrite rules,
  BUT NOT the ability to rearrange letters,
  and two words,
  can you turn one word into the other?

The only difference is that I can't rearrange letters. I can only
use a rewrite rule on one part of the word and leave it where it is.
So I can do

     kerning <=> keming

but for example not

     kerning <=> kenimg

(these could be generated by anagraph)

And actually, I'm going to prove something about a slightly different
problem so that this video doesn't get way too complicated. Specifically
the question

  can you turn word A into some other word where word B is a substring?


OK so I want to show that this problem is not possible. What do I mean
by that? Well, first of all to say we can solve the problem, we have
to solve it for ALL instances, not just easy ones. In fact, we have to
solve it for really devious ones, so it's useful to think of the
inputs as being supplied by a clever adversary. Skeletor gives us
an alphabet, the rewrite rules, and a pair of tricky words. To say
that we could solve the problem means that we have a procedure that
works no matter what Skeletor gives us. It's not just enough to
believe or assert that we'd be able to do it using our cleverness
applied to each instance. We have to give a procedure. So we're going
to prove that there is no such procedure.

Incredibly, in computer science we know that any procedure can be
performed by a simple computer called a Turing Machine. A turing
machine is simple. You have an arbitrarily long tape that contains
only 0s and 1s, like this:

      ...0000111100110100011000000...

We treat the ends as having zeroes in both directions, like how we can
think of a number as having as many invisible zeroes at the start as
we want, or as many zeroes at the end of the decimal part. And the turing
machine has a cursor

      ...0000111100110100011000000...
                   ^

like the read/write head in your VCR. The cursor always has some state,
which we'll give as a letter.

      ...0000111100110100011000000...
                   ^
                   a

And finally it has some rules, which tell the cursor what to do in
each case. The rules are the turing machine's program, and the tape is
the input.

       state   tape   write    new state   go
A0       a       0      1         f        right
A1               1      0         g        right
------------------------------------------------------
B0       b       0      1         a        left
B1               1      1         a        right         
          ...

That's it. The machine just follows the instruction at each step and
transforms the tape. It could signal when it's done by writing a special
sequence of bits to the tape, or moving the cursor to a designated spot.

It's pretty mind-blowing that something so simple could allow us to
build any procedure. If it can be done by a Pentium it can be done by
a Turing Machine. If it can be done by a quantum computer, it can be
done by a Turing Machine. We have never discovered, and don't expect
to discover, any physical procedure that allows us to solve more
problems than a turing machine. And actually there are lots of
alternatives to Turing Machines that are even simpler to describe, and
it isn't even my favorite, but it is traditional and useful for this
particular problem!

So we could prove that there's no turing machine (really, turing
machine program) that solves this problem. It's usually pretty hard to
prove that something doesn't exist, but there's already a powerful and
ancient non-existence proof for Turing machines. Specifically, it's
impossible to devise a procedure that takes any turing machine program
(like that table of rules) and any two tapes, and decides whether
running that machine would turn the one tape into the other. Let's call
this the turing machine prediction problem because we answer whether
a prediction for the turing machine's outcome is correct or not.

I'm not going to explain how that proof works in this video, but I'm
sure there are hundreds of good descriptions of it out there. You
could search for the "halting problem." But this proof is utterly
beautiful. Maybe the best of all time. The basic idea relies on that
idea that any procedure can be performed by a turing machine. We prove
that no such decision turing machine could exist. Because if it did,
then Skeletor could use it to make a "test" turing machine that tricks
the "decision" machine by first running its own simulation of the
decision machine with the test program as input, and then doing the
opposite of what it says. So the decision machine is always wrong. So
therefore no such decision machine exists.

Anyway, it is well known that there is NO procedure (turing machine)
that can take the rules of a (different) turing machine as input and
decide whether it turns one tape into another. When I say "tape" here
I mean both the contents of the tape and the position and state of
the cursor.

So now we can use this to prove that the kerning problem cannot be
solved. We'll say

   1. For any instance of the turing machine prediction problem,
      we can turn it into the kerning problem.
   2. So, imagine that we had a procedure for the kerning problem.
      We could use it to solve any any instance of the prediction
      problem.
   3. But this is impossible, because the prediction problem
      can't be solved in general. So it can't be the case that
      there's a procedure for solving the kerning problem.

This is a proof by contradiction. Since we transformed one problem
(whose solvability is in question) into the other (whose solvability
is known) it's also a reduction argument. We'll also use reduction
in the opposite way to prove that a problem IS solvable later.

So all that's left is to prove #1. What we'll do is show how, given
a turing machine (basically a set of rules), we can turn it into
an alphabet and rewrite rules for kerning. This will involve coming
up with a fairly devious alphabet, so we'll be playing the role of
Skeletor. We'll call this a Keming machine.

I hope some of the parallels are already clear. The input tape becomes
the input word, so we'll want the "letters" 0 and 1.

   turing machine on left                        keming machine

  input tape
       10010111                                    10010111


In order to represent where the cursor is, we'll introduce two letters
[ ], which will surround the letter where the cursor is.

       10010111                                    1001[0]111
           ^

There will always be just one [ and one ]. Next, the cursor has some
state associated with it, which we said was a letter. So we'll
include that letter right after the brackets in our word

       10010111                                    1001[0]a111
           ^
           a

and finally, we'll turn each rule in the turing machine into a rule
in the keming machine. Take this one:

       state   tape   write    new state   go
         a       0      1         f        right

it happens to apply in the example. We want it to write a 1 and move
the cursor to the right and become state f, so something like
   [0]a*  <=>    1[*]f

but we didn't say we could make wildcards like this. But there are only
two things that can be on the tape there, so we can just make two rules
   [0]a0  <=>    1[0]f
   [0]a1  <=>    1[1]f

Show a leftward rule too.

Cool! It's straightforward to translate the whole table of rules into
kerning rules. There is one problem with this, which is that we assume
the turing machine tape has an arbitrary supply of zeroes on either
side of it, but in the keming machine we just have some word of finite
length. So let's actually mark the ends of the word

       10010111                                   {1001[0]a111}
           ^
           a

and now for rules that move right, we include a case for when the
cursor is at the end of the word. This should behave exactly like when
there's a zero there, although it needs to preserve the end-of-word
marker.

   [0]a}  <=>    1[0]f}
   [0]a0  <=>    1[0]f
   [0]a1  <=>    1[1]f

That's it! To solve any instance of the turing prediction problem,
we'd translate the machine into a keming machine, use our hypothetical
procedure to determine whether the one word can be transformed into
the other (actually, whether it contains the wrapped string {...}), and
if so, then the answer is "YES" to the turing problem. Right?

Nope!

The problem is that the keming machine can do any step that the turing
machine could do, but it can also do some other steps. In particular,
it can run rules backwards. This isn't a problem on its own; when the
turing machine runs, it's going to go through some sequence of states
like this


     start ---- . --- . --- . ---- . --- . --- . ---- . ---- end
                1     2     3      4     5     6      7

Let's say that when we turned this into a keming machine and asked,
"can we get from start to end?" We said yes, and did it by going
from 1 to 2 to 3 to 4 but then back to 3, and then forward to 4, etc.
This is no big deal, because we could just cut out this pointless
backtracking and still see how the turing machine could do it by
only going forward. 

But the problem is that when we run a turing machine rule backwards,
we might not simply be "backtracking." This is because multiple
rules might produce the same output. Consider specifically something
like this:

turing rules

       state   tape   write    new state   go
         b       0      1         c        right
         b       1      1         c        right

become the keming rules (ignoring the end of word stuff for brevity)

        [0]b0  <=>   1[0]c
        [0]b1  <=>   1[1]c

        [1]b0  <=>   1[0]c
        [1]b1  <=>   1[1]c

So basically if we're in state b, we write a 1 and go to the right
no matter what. But note how the right hand side of these rules is
the same! That means that in the keming machine we can do this

                       !
    [0]b0  ->  1[0]c   <-  [1]b0

now we've turned the first symbol into a 1. This is impossible with
the original turing machine! To emphasize the consequences of this...
Here's a very simple turing machine that just wipes any tape into all
zeroes.

       state   tape   write    new state   go
         a       0      0         a        right
         a       1      0         a        right

no matter what it sees, it writes a zero and moves to the right. So
the whole tape gets overwritten with zeroes. If we allow running
these rules backwards, then we can "unwipe" a tape of all zeroes
into any bits we want! So if we ask, "can we turn this tape into
this other tape" (with suitable initial conditions because of the
way the tape head moves) the answer is always "YES", because we
can do this by wiping it to zeroes and then unwiping it to the
target bits. This is wrong because all the turing machine can
really do is wipe tapes to all zeroes.

So this is actually a serious problem with the proof. If you like
this kind of thing, maybe pause the video and give some thought
to how you'd solve it.

My approach is to get us back into the situation where every step
we take corresponds to something the turing machine can do, either
forward or backward. That is, backwards steps will always simply
undo the last legal step, rather than get us into states that we
could not have been in.

We're going to do this by recording a "trace" of the steps that
the turing machine used as it executed. This trace can be arbitrarily
long, but fortunately we have space for it because our word can also
be arbitrarily long.


  Word looks like this

  trace{bits[C]Sbits}

  where C is the current bit and S is the current state. Every time we
  simulate a turing machine rule, we'll record what we rule we used in
  the trace. This will prevent us from undoing a rule that we didn't
  use, because we'll only be able to undo the last instruction in that
  recorded trace.

  So how do I do that? Now when I execute a step,

                 0101      ->       1101
                 ^b       (B0)       ^a

  I go into an intermediate state that removes the cursor and instead
  uses these two new shuttle characters < and |.
  
                [0]b101           1<B0|1]a01

  B0 is the turing machine rule that we just applied. We need one
  symbol for each rule. I write b (ring) and b (acute) to look like 0
  and 1. These symbols aren't used anywhere else so they don't
  interfere with other rules. Also note that when I'm in this state
  there is no pair of brackets, so none of the normal rules apply.
  Instead I use this state to incrementally send the qrule used onto
  the trace, like so:

     0<?    <=>    <?0          (for all rule symbols)
     1<?    <=>    <?1

  and when it hits the end of the word,

     {<?    <=>    ?{>

  this pushes it onto the end of the trace, and then reverses the
  shuttle. Then we have rules for the shuttle's return:

     >0     <=>    0>
     >1     <=>    1>

  and for returning back to the normal mode

     >|     <=>    [

  which in the above example results in

    B0{...1[1]a01...}     as desired.


  It's still possible to run these rules in reverse. For example
  we could turn [ into >| and then run the > shuttle back to the
  trace, and pick up B0. And even bring B0 back to our word. And
  then we can run the B0 rule backwards if we want. But this is
  not a problem any more, because we can only undo that particular
  step. For example in the "wipe" counterexample, the trace ends
  up encoding which bit we erased from each spot on the tape,
  which ensures that as we "unwipe", we can only put back what
  was originally there.

  In fact now there is no ambiguity about what rule applies at any
  step. There's always exactly one "forward" rule corresponding to the
  next turing machine step (or intermediate shuttle movements), and
  one backward step that corresponds to undoing it.


  There's one additional bit of subtlety. The turing prediction
  problem we're trying to solve asks if we can get to a particular
  tape. 

    0111            --?-->         101
    ^a                             ^a
    
  So we'd like to just ask

   {[0]a111}        <==?==>        {[1]a01}

  but now that we're recording a trace, what we actually come up
  with is something like

   {[0]a111}        <==?==>        A0B1B1C0C0C0B1A0{[1]a01}

  and we can't know ahead of time what trace to ask for (if we
  had such a thing we'd be able to solve the turing prediction
  problem anyway!) This is why I said we'd prove that a slightly
  different kerning problem is undecidable, which is to ask if
  we can turn it into any word where the target word is a substring.
  So we ask whether {[1]a01} is a substring, and this ignores the
  contents of the trace. I couldn't see any easy way to work
  around this for the pure case of kerning one word into another.
  Maybe you have some ideas. 


Anyway, this completes the "proof" that generalized kerning is
undecidable. I say air-quotes "proof" because I think it's quite
easy to make small mistakes when doing this kind of work, and
that we should only accept very rigorous proofs as "proofs." In fact
when I was in graduate school one of the things I worked on was
computer proof systems, which allowed for making very detailed
proofs that can be checked by computer. Usually when we'd turn some
paper "proof" into this computer form, we would discover that there
were some non-obvious details missing from the original proof, that
the author (like us) just glossed over without realizing that there
was something a little tricky there. And sometimes we would just
find that the proof was wrong for some technical reason, and need
to do something significant to fix it. And occasionally we would
just find out that the proof was wrong and the theorem wasn't even
true!

So now we can move onto the other problem. Is generalized anagraphing
possible?

-- 

To recap: I just proved that the generalized kerning problem, where I
can rewrite sequences of letters to other sequences, is undecidable. I
proved that by demonstrating that if we could solve generalized
kerning, we could solve the turing machine oracle problem -- but since
we know that problem is impossible, we know that generalized kerning
then must be impossible. Showing that we could use one problem to
solve another is the essence of reduction.

Now let's look at the generalized anagraphing problem. This is
like the kerning problem, but we're also allowed to rearrange
letters whenever we want. This problem turns out to be solvable.
We can prove this by reduction as well, but in reverse. Here
we'll prove that if we could solve some other problem, we could
solve this one. Then we can use an off-the-shelf result to solve
that other problem.

That other problem is a kind of logic, called linear logic. My thesis
was about logic so this is a subject that's dear to my heart, and
so I'm in severe danger of this turning into an all-day lecture. So
I'm going to try to keep this to a sketch and just give the basic
intuitions.

There are lots of ways to present logic. You may be familiar with logic
presented as a set of axioms, like "modus ponens;" I'm going to use
a totally different one that we like in proof theory, which is to 
define an entailment relationship. That means we're only going
to make statements of this form

                   A1,....,An => A

where this symbol in the middle is just a separator that you could
read "proves", and all of the As are propositions. A proposition is a
statement about the world, like "it is raining," or a combination
of propositions like "it is raining   AND   i have an umbrella."
The things on the left are assumptions and the thing on the right
is what I prove. So as a really easy example, no matter what A is,

                  -------------
                   A, ... => A

Assuming A (and whatever else) I can prove A. I'll talk about this
line in a second but here it basically means I'm done. If I want to
prove A AND B, assuming some stuff C, there's a rule like this

               C, ... => A     C, ... => B
               ----------------------------
                     C, ...  => A ^ B

here the line tells me that I have two more "obligations", which is
to prove A from those assumptions, and also to prove B from those
assumptions. Putting these two rules together, we can make a full
proof of this

          ----------     -----------
           B, A => A      B, A => B
          -----------------------
              B, A   =>   A ^ B

so these proofs are read from the bottom up, and we have to proceed
until all our obligations are covered, so a complete proof looks like
a tree where everything at the top has a line on it. 

Also note that I'm rearranging my assumptions at will. We just say
syntactically that this ... thing is a multiset, regardless of the
order we wrote it on paper.

One more rule is the way we prove implication, that is, A IMPLIES B.

                  C, ... A => B
            ----------------------
               C, ...  =>  A -> B

We just add A as an assumption and then we have to prove B. Don't
stress out about the difference between => and ->, which I haven't
really explained well. It's not that important for today's lesson.
But -> is part of the language of propositions and => is part of
the metalanguage that we use to perform proofs.

Every one of these connectives like AND and IMPLIES have a rule
that tells us how we prove it (I just showed those), as well as a
rule that tells us how we can use it as an assumption. I'll show
you that rule for AND inside this example of proving "A ^ B -> A".
The proof looks like this:

               ---------------
                 A, B   =>  A
              ------------------
                  A ^ B  =>  A
             ------------------------
                    => (A ^ B) -> A

I get A ^ B as an assumption, and then, reading from bottom up,
I get to break A ^ B into the assumption A and also the assumption B,
and then I can use the A and whatever else proves A rule to complete
the proof.

So what I just showed you like "regular" logic, not "linear" logic.
What makes linear logic different is that I have to use all of my
assumptions, and I have to use them exactly once. So, contrasting
the rules above, we have

              ----------
                A => A

where just A is on the left of the proves symbol. If I have any
other assumptions, I can't use this rule. When I'm proving AND, which
I'll write as a circled x called tensor, I have to split up my
assumptions for each of the two obligations:

            C1, ... Cn => A     D1, ... Dm => B
           --------------------------------------
            C1, ... Cn, D1, ... Dm => A (x) B

so for any given assumption I chose to use it in the branch that's
proving A, or I use it in the branch that's proving B. Like before,
we allow mixing up the assumptions however we want. Note how this
resembles the anagraphing problem, where I have a bunch of letters
(assumptions) and I can mix them up at any time.

In linear logic the example I did above doesn't work, because of this
step

               ---------------
                 A, B   =>  A           <-------
              ------------------
                  A ^ B  =>  A
             ------------------------
                    => (A ^ B) -> A

I assumed both A and B but only used A. I could instead prove this though

               ------      -------
                B => B      A => A
               ----------------------
                 A, B   =>  B (x) A   
              --------------------------
                  A (x) B  =>  B (x) A
             ------------------------
                    => (A (x) B) -> (B (x) A)


which is like saying that AB anaGRAMS to BA. Not being allowed to
discard an assumption, or use an assumption more than once, directly
corresponds to the rules of anagramming. So we're going to express any
anagraphing problem as a proving problem like

          K (x) E (x) R (x) N (x) I (x) N (x) G =>
                  K (x) E (x) N (x) I (X) M (x) G

So where does anaGRAPHing come in? Basically we need a way to allow
rewriting rules like

           rn <=> m
          cat <=> dog

and this is simply done using the implication connective. We'll add
as assumptions

              R(x)N -> M                 M -> R (x) N
          C(x)A(x)T -> D(x)O(x)G

(and also backwards).

So the reduction looks like this. Take all of the anagraphing rules
and encode them in both direction as implications. Then take the
source word w1 and and all its letters together too. Do the same
to the destination word. Then ask

   R1, R2, R3, ... W1  =>  W2              ?

can I "prove" w2? The claim is that I can do this if and only if
the words are anagraphs with that set of rules.

Now there is one problem with this (maybe you spotted it): I said you
must use assumptions exactly one time, but to prove kerning becomes
kenimg I would use the RN -> M rule in the forward direction once, but
never use it in the backward direction, and I would never use either
of the CAT -> DOG rules. But those are all assumptions. Also, I'm
certainly allowed to turn more than on RN pair into M when
anagraphing, so I would want to be able to use that assumption more
than once too.

This kind of problem comes up in linear logic all the time too, so
in the rules of that logic we have this connective called ! that
can be kind of used to disable the linearness. If I have

            ------------------
              !A, C...    =>  B

then I can do this


               !A, A, C...  => B
            ------------------
              !A, C...    =>  B


to make another copy of the rule, or 

               C...  => B
            ------------------
              !A, C...    =>  B

to say that I don't want it any more.

So the real encoding looks like this:

   !R1, !R2, !R3, ... W1  =>  W2              ?

where I mark each of the rewrite rules as being kind of optional,
but not the letters of the word; those I have to use exactly once,
perhaps by transforming them into other letters.

So I showed how I could turn an instance of the anagraphing problem
into a problem of provability in linear logic. Specifically, I
only used (x), -o, and ! to do this. This particular set of operators
is called MELL, for Multiplicative Exponential Linear Logic. It's
a pretty powerful subset, and when I was in grad school nobody
knew whether it was solvable or not. Solvable here means, can I decide
whether an arbitrary statement  A => B  can be proved or not. But
just a couple years ago some wizards proved that it IS in fact
decidable. This is pretty cool. In contrast, if you add a few other
connectives like A OR B, then it's known to be undecidable.

So we've proved the generalized anagraphing problem solvable. Because
any anagraph problem we can turn into a linear logic problem, specifically
a MELL problem, and MELL is known to be solvable. This is a reduction
like our previous argument, but it's in the opposite direction; we turned
an anagraph problem into a MELL problem, instead of turning a turing
machine problem into a kerning problem. Generally that latter direction
is used for a proof by contradiction (to prove impossibility) and the
direction here is for a direct proof (to prove possibility).

What separates these two problems is whether we're allowed to
rearrange the letters in the word. If you're really astute and by some
miracle I managed to actually explain linear logic in that whirlwind,
you might have noticed the following. I allow rearranging my
hypotheses at will in linear logic. But I could make up another logic
where I didn't allow this; where everything has to happen in place.
This family of logics are called "ordered logics", and using the
encoding I just did but in ordered logic would corresponding to the
kerning problem, rather than the anagraph problem. This would give
us a path to proving that multiplicative exponential ORDERED linear
logic is undecidable, using our result about the generalized kerning
problem. I believe this is the case (and already known), but it is
a nice way of tying these three problems together.


Okay, we finally did it!




** Older notes **



Say we have an alphabet of symbols, and then a system of "kerning"
rules that let us rewrite sequences of symbols into other sequences.
Like

     rn   <=>   m

     vv   <=>   w

     ol   <=>   d
     lo   <=>   b
     
(both directions). Then in fact it's UNDECIDABLE whether one word
is a kernogram of another! With the ability to create an arbitrary
alphabet and kerning rules, we can make a turing machine


There's a single head, which we'll write as

 [ ]

around a symbol 1 or 0. The head has associated with it some state,
which we can write as a subscript on the brackets

 aaa[1]aaa
      S

Then we give the turing machine instructions like so,

  state        symbol        write        move    new state
    S             1            1            R         T

by expanding into rewrite rules

    [1]0  <=>   1[0]
      S            T

    [1]1  <=>   1[1]
      S            T

Note:
  - we expand for every possibility of the bit to the right;
    for a two-symbol turing machine this is just 0 and 1, easy

  - The rules can be run symmetrically, but we construct the
    encoding such that only one rule ever applies

But then we need to do something about the blank ends of the
tape. It doesn't obviously work to have a rule like

    [1]   <=>    1[0]
      S             T

that allows inserting zeroes if there's no symbol there, because this
rule would also apply in the case above when there ARE symbols
(inserting a new zero!). But we can simply arrange that for a delimeter
symbol (write |) that's on each end of the string by invariant. Then
each rule that moves right has three versions:

    [1]0  <=>   1[0]
      S            T

    [1]|  <=>   1[0]|
      S            T

    [1]1  <=>   1[1]
      S            T

and the embedded problem to turn 'stringone' into 'stringtwo' is actually
|stringone| into |stringtwo|.

we could also maybe make the encoding smaller by just adding generic
rules for this

    |   <=>   0|
    |   <=>   |0

or maybe you want them to be directional

    <   <=>   <0
    >   <=>   0>

I think this works fine, but it makes the argument harder, because now
there is always ambiguity in what rules apply (adding and removing
leading and trailing zeroes is always allowed).

The argument is as simple as that: If you have local rewrite rules,
you can easily build a turing machine. Now to be fair it doesn't really
look like kerning, since a rewrite like

      [1]0  <=>  0[0]
        S           T

is not very letter-like at all? 


As an aside: Notice that, other than the fact that I can run rules
backwards in the keming machine, there's only going to be one rule
that applies at any time. So this works even when the kerning problem
does not allow ambiguous rules. (Actually, not true! The RHS can be
duplicated.) (Actually, is it a problem that rules can run backwards?
Because two steps could yield the same string, allowing transitions
that are not permitted in the turing machine. Any simple way around
this? Maybe leave a breadcrumb of which rule is used at each step,
ensuring that the RHS is unique? And then blow up the rules to
wildcard over this breadcrumb on the LHS?)

We want to make sure that every backwards step corresponds to a
backwards step on the turing machine.

First take every rule and label
it with the Turing machine rule we used to generate it. We can write
this one before the cursor.

A0.
   [0]a>  <=>    1(a0)[0]f>
   [0]a0  <=>    1(a0)[0]f
   [0]a1  <=>    1(a0)[1]f

but can't leave these in the rule. So we then take every rule and
"wildcard" over all possible symbols that can be there.

   (**)[0]a0  <=>    1(a0)[0]f
   (**)[0]a1  <=>    1(a0)[1]f

but we don't really have wildcard so this blows up into a family
of rules, where we put each turing rule number in there.

   (a0)[0]a0  <=>    1(a0)[0]f
   (a1)[0]a0  <=>    1(a0)[0]f
   (b0)[0]a0  <=>    1(a0)[0]f
   (b1)[0]a0  <=>    1(a0)[0]f
     .
     .
     .
   (z0)[0]a0  <=>    1(a0)[0]f
   (z1)[0]a0  <=>    1(a0)[0]f

   (a0)[0]a1  <=>    1(a0)[1]f
     .
     .
     .

For example, let's say that we have this turing machine

       state   tape   write    new state   go
A0       a       0      1         f        right
A1               1      0         g        right
------------------------------------------------------
B0       b       0      1         a        right
B1               1      1         a        left         
          ...
C0       c       0      1         a        right
C1               1      1         a        left         


                 0101      ->     1101
                 ^b                ^a
                 
    and we do   [0]b101           0[1]a01

    but this state matches the RHS of rule C0 (and C1!); we have

                0[1]a01    <=>    [0]c101    (an unreachable state)
       or even  0[1]a01    <=>    01[1]c01

Neither of these correspond to possible turing machine moves. What
if one let us reach the target word?

If we annotated rules with which one we used, we'd have

   (A0)[0]a0  <=>    1(B0)[1]a
   (A1)[0]a0  <=>    1(B0)[1]a
   (B0)[0]a0  <=>    1(B0)[1]a
   (B1)[0]a0  <=>    1(B0)[1]a
     .
     .
     .
   (Z0)[0]a0  <=>    1(B0)[1]a
   (Z1)[0]a0  <=>    1(B0)[1]a   

Each time recording that we used rule B0. Now the rewrite above is
something like this

                 0101      ->       1101
                 ^b       (B0)       ^a
                 
    and we do   (??)[0]b101       0(B0)[1]a01

this can only be rewound with rules from the B0 set, which all
correspond to the same real turing machine rule. However, we could
choose a different () in the previous state, which allows the same
problem, ugh!


OK, last try. Let's say that we instead record the direction we
moved and the bit that we overwrote.

                 0101      ->     1101
                 ^b                ^a
                 
    is now   (??)[0]b101       (R0)0[1]a01

No, this possibly allows us to only rewind to states that will
do the exact same thing. But the real problem is that we wildcard
over ?? on the LHS, which means that we can diverge from the
original trace in just one step. I think if we wanted to make
this work, we'd need arbitrary storage.

But don't we have arbitrary storage in the keming machine?
Well sorta. If our cursor only ever moves right, then we can
do something like


)[0]b1   ->  B0)0[1]a

which "pushes B0 on the stack" inside the parens. But without doing
something fancy, how would we 

Semidecision approach.

  real answer          keming procedure     run turing machine

     yes                   yes                     yes

     no                     no                (loops forever?)

 keming no => real "no"
 turing yes => real "yes"
 but we could have keming "yes" and no answer from turing, and
 we wouldn't know whether it's real.


  So we simply run the turing machine in parallel with the keming
  reduction. If keming returns "no" then we know there is no trace.
  If it returns "yes" then we can't be sure. But unless the turing
  machine loops forever (in which case the answer is "no" and the
  keming machine must return no... no wait this is wrong! There
  may be no way to do it using the forward approach.

  DOES NOT WORK.



  J/K! Another bug. We originally said that our keming machine
  solver would tell us if one word translated into another. Given
  a turing problem like

    0111            --?-->         101
    ^a                             ^a
    
  we would ask

   {[0]a111}        <==?==>        {[1]a01}

  or something. But now there will always be some trace left, and we
  don't know the trace before we ask for it! So we don't know what
  question to ask the keming solver.

  So the last thing to do is to add a final phase. We have something
  like

  A0B1B1C0C0C0B1A0{001001[1]a010}

  and we want to turn it into a normalized string so that we know
  what question to ask. In this case it will be

  (1001[1]a01)

  and the way we accomplish this is that we allow turning { into
  finalizing parens (, like so:

  { <=> (

  (ok this is obviously pointless since then { and ( would be
   equivalent)

  So uh.. how? If I allow something simple like

  A0{ <=> {

  then this allows me to insert arbitrary steps into the trace,
  which has the same problems as before. And if deleting always
  gave us finalized parens, like

  A0{ <=> (            (for all states)

  then we could still do shenanigans by taking

  A0B1B1C0C0C0B1A0{001001[1]a010} ->
  A0B1B1C0C0C0B1(001001[1]a010} <-
  A0B1B1C0C0C0B1A1{001001[1]a010}

  again messing up the trace.


  (soooo.. we could change what we're trying to prove, to prove that
   "can w1 be kerned to any string where w2 is a substring?" That
   is undecidable because of the argument here. If we could solve
   it, then we could build a turing machine with the above argument,
   with no false positives because we actually ask if the bracketed
   string { } is a substring. (need to eliminate leading/trailing
   zeroes but this is safe.) But this is a weaker statement than
   the original kerning problem--it might be undecidable despite
   kerning an exact pair being decidable. Unless there is a reduction
   there? We'd need to prove that any instance of the substring
   problem can be turned into an exact problem. "if we can solve the
   exact problem, then we could solve the substring problem"

   so uh, how would we do that? 

   the substring problem looks like this:
   w1  <=?=>  ...w2...

   obviously if w1 <=> w2 then the answer is yes, but maybe it requires
   adding extra stuff. We could put sentinels on the ends and then allow
   deleting arbitrary characters there, i.e. we ask

   w1 <=> (w2)           (exact)
   with the rules
   (a <=> (          (for all letters a)
   a) <=> )

   which is sorta along the right lines, but fails for the same
   reason as such tricks above fail: We can use it to insert arbitrary
   strings at the ends of the word, so we might get a "yes" that's
   fake (doesn't really solve the problem) because it cheats.

   maybe some way to recover here?)

  Bidirectionality is the issue in the first place! If we had
  unidirectional kerning rules then we'd be fine (without the
  special shuttle stuff).

  So: Unidirectional kerning is undecidable.
  If we have a bidirectional kerning problem, we can't use
  unidirectional kerning to solve it, but it might still be decidable
  anyway.
  We'd have to show that if bidirectional kerning were decidable,
  unidirectional would be (a contradiction). This means taking any
  unidirectional problem and making it a bidirectional one. How?

  So we could just gloss over this in the first place and say that
  the generalized problem is the unidirectional one. This is indeed
  a generalization, and makes the turing machine reduction much more
  straightforward (we don't need the trace or anything). I do kind
  of like that trace idea though!

  So I know how to do these:
    - Prove that the bidi substring problem is undecidable
    - Prove that the uni exact problem is undecidable
