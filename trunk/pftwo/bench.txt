22:24

1344s

192340  0.065775s       FIND_NODE_TO_EXTEND
14060639353     4808.374588s    EXTEND_NODE
733370  0.250794s       UPDATE_TREE_A
0       0.000000s       UPDATE_TREE_B
4       0.000001s       UPDATE_TREE_C
0       0.000000s       UPDATE_TREE_D
123139236       42.110430s      PROCESS_EXPLORE_QUEUE_A
109735829       37.526812s      PROCESS_EXPLORE_QUEUE_B
1070562515      366.104660s     PROCESS_EXPLORE_QUEUE_C
119086763       40.724589s      PROCESS_EXPLORE_QUEUE_D
23659   0.008091s       SHOULD_DIE_EQ
11678   0.003994s       SHOULD_DIE_N

30 workers on threadripper

4804s /30 160s (11.9%)
485s in explore queue /30 = 16s (1.2%)

so as on RIVERCITY, the thing to improve is the EXTEND_NODE lock.


1:52:41, 60 threads
19698348        6.736325s       FIND_NODE_TO_EXTEND
146448587338    50081.624889s   EXTEND_NODE
170657780       58.360542s      UPDATE_TREE_A
4994    0.001708s       UPDATE_TREE_B
21      0.000007s       UPDATE_TREE_C
13      0.000004s       UPDATE_TREE_D
2215907606      757.783025s     PROCESS_EXPLORE_QUEUE_A
940542996       321.641351s     PROCESS_EXPLORE_QUEUE_B
10068988364     3443.333305s    PROCESS_EXPLORE_QUEUE_C
2385513637      815.783896s     PROCESS_EXPLORE_QUEUE_D
248261  0.084899s       SHOULD_DIE_EQ
86364   0.029534s       SHOULD_DIE_N


again w/ 60 on threadripper:

Perf counters:
3946475 1.349592s       0.000818%       L_FIND_NODE_TO_EXTEND
58774109444     20099.223597s   12.175094%      L_EXTEND_NODE
48792651        16.685823s      0.010107%       L_UPDATE_TREE_A
19      0.000006s       0.000000%       L_UPDATE_TREE_B
10      0.000003s       0.000000%       L_UPDATE_TREE_C
10      0.000003s       0.000000%       L_UPDATE_TREE_D
1142331385      390.647755s     0.236635%       L_PROCESS_EXPLORE_QUEUE_A
294973938       100.873449s     0.061104%       L_PROCESS_EXPLORE_QUEUE_B
4132366307      1413.162278s    0.856022%       L_PROCESS_EXPLORE_QUEUE_C
1007022778      344.375715s     0.208605%       L_PROCESS_EXPLORE_QUEUE_D
98487   0.033680s       0.000020%       L_SHOULD_DIE_EQ
39234   0.013417s       0.000008%       L_SHOULD_DIE_N
316738804545    108316.469865s  65.612642%      EXEC

... so we seem to be spending a lot of time somewhere else,
which doesn't happen on rivercity. Still good to reduce the
lock contention here anyway...


Now batch updates:

Perf counters:
23759653131     8125.211172s    17.761734%      L_COMMIT_QUEUE
11777   0.004027s       0.000009%       L_GET_NODES_TO_EXTEND
2604    0.000891s       0.000002%       L_UPDATE_TREE_A
2       0.000001s       0.000000%       L_UPDATE_TREE_B
2       0.000001s       0.000000%       L_UPDATE_TREE_C
0       0.000000s       0.000000%       L_UPDATE_TREE_D
908131  0.310558s       0.000679%       L_PROCESS_EXPLORE_QUEUE_A
603137325       206.257983s     0.450881%       L_PROCESS_EXPLORE_QUEUE_B
2346602475      802.479756s     1.754223%       L_PROCESS_EXPLORE_QUEUE_C
146842919       50.216631s      0.109774%       L_PROCESS_EXPLORE_QUEUE_D
7244    0.002477s       0.000005%       L_SHOULD_DIE_EQ
15677   0.005361s       0.000012%       L_SHOULD_DIE_N
101730605547    34789.340069s   76.049593%      EXEC
1961203 0.670683s       0.001466%       OBSERVE
23760336342     8125.444813s    17.762245%      COMMIT

note that we wait even longer for the commit lock, which is pretty
weird. We spend most of this time waiting for the lock?

With batch size of 100 nodes:
Perf counters:
2302204857      787.296873s     2.549002%       L_COMMIT_QUEUE
115     0.000039s       0.000000%       L_GET_NODES_TO_EXTEND
29412   0.010058s       0.000033%       L_UPDATE_TREE_A
1       0.000000s       0.000000%       L_UPDATE_TREE_B
1       0.000000s       0.000000%       L_UPDATE_TREE_C
0       0.000000s       0.000000%       L_UPDATE_TREE_D
2398153 0.820109s       0.002655%       L_PROCESS_EXPLORE_QUEUE_A
1199396963      410.163968s     1.327973%       L_PROCESS_EXPLORE_QUEUE_B
4260717475      1457.059539s    4.717468%       L_PROCESS_EXPLORE_QUEUE_C
278075729       95.094992s      0.307886%       L_PROCESS_EXPLORE_QUEUE_D
1986    0.000679s       0.000002%       L_SHOULD_DIE_EQ
15380   0.005260s       0.000017%       L_SHOULD_DIE_N
81895531774     28006.237550s   90.674769%      EXEC
1676343 0.573268s       0.001856%       OBSERVE
2302580911      787.425475s     2.549419%       COMMIT

... much better. Also, fixed NES frame counter, and we actually get
about 100k frames after it warms up.

Finer-grained locking in explore queue. EQ is not being exercised
much because update frequency was accidentally made very rare by
batching:


Perf counters:
8173913584      2795.275403s    4.080070%       L_COMMIT_QUEUE
13469   0.004606s       0.000007%       L_GET_NODES_TO_EXTEND
4003    0.001369s       0.000002%       L_GET_EXPLORE_NODE_E
29596   0.010121s       0.000015%       L_UPDATE_TREE_A
0       0.000000s       0.000000%       L_UPDATE_TREE_B
1       0.000000s       0.000000%       L_UPDATE_TREE_C
0       0.000000s       0.000000%       L_UPDATE_TREE_D
988624  0.338085s       0.000493%       L_PROCESS_EXPLORE_QUEUE_A
331     0.000113s       0.000000%       L_PROCESS_EXPLORE_QUEUE_ES
0       0.000000s       0.000000%       L_PROCESS_EXPLORE_QUEUE_B
400     0.000137s       0.000000%       L_PROCESS_EXPLORE_QUEUE_EB
8132070843      2780.966224s    4.059184%       L_PROCESS_EXPLORE_QUEUE_C
293771491       100.462553s     0.146638%       L_PROCESS_EXPLORE_QUEUE_D
2117    0.000724s       0.000001%       L_PROCESS_EXPLORE_QUEUE_ED
2007    0.000686s       0.000001%       L_SHOULD_DIE_EQ
37780   0.012920s       0.000019%       L_SHOULD_DIE_N
183230136712    62660.155247s   91.460700%      EXEC
3657398 1.250739s       0.001826%       OBSERVE
8174992049      2795.644211s    4.080609%       COMMIT


Explore queue executed more often now. We end up spending a lot
of time waiting on the tree lock. The one we stall on jumps around:


      1865884960  638.086307s   2.063399%       L_COMMIT_QUEUE
           13769  0.004709s     0.000015%       L_GET_NODES_TO_EXTEND
            8102  0.002771s     0.000009%       L_GET_EXPLORE_NODE_E
              40  0.000014s     0.000000%       L_UPDATE_TREE_A
               0  0.000000s     0.000000%       L_UPDATE_TREE_B
               1  0.000000s     0.000000%       L_UPDATE_TREE_C
               1  0.000000s     0.000000%       L_UPDATE_TREE_D
     20576794528  7036.752593s  22.754961%      L_PROCESS_EXPLORE_QUEUE_A
             753  0.000258s     0.000001%       L_PROCESS_EXPLORE_QUEUE_ES
               0  0.000000s     0.000000%       L_PROCESS_EXPLORE_QUEUE_B
             783  0.000268s     0.000001%       L_PROCESS_EXPLORE_QUEUE_EB
               0  0.000000s     0.000000%       L_PROCESS_EXPLORE_QUEUE_C
       488869454  167.181210s   0.540619%       L_PROCESS_EXPLORE_QUEUE_D
            3485  0.001192s     0.000004%       L_PROCESS_EXPLORE_QUEUE_EZ
              43  0.000015s     0.000000%       L_PROCESS_EXPLORE_QUEUE_ED
      1617583889  553.173509s   1.788814%       L_PROCESS_EXPLORE_QUEUE_R
            3478  0.001189s     0.000004%       L_SHOULD_DIE_EQ
            8894  0.003042s     0.000010%       L_SHOULD_DIE_N
     59407485464  20315.884323s 65.696092%      EXEC
          870907  0.297829s     0.000963%       OBSERVE
      1866132165  638.170845s   2.063672%       COMMIT


Now batch decrements of the reference count in the "bad" case, which
is the other place we were frequently taking tree_m:

      1513235490  517.488948s   1.351099%       L_COMMIT_QUEUE
           12381  0.004234s     0.000011%       L_GET_NODES_TO_EXTEND
           13545  0.004632s     0.000012%       L_GET_EXPLORE_NODE_E
              47  0.000016s     0.000000%       L_UPDATE_TREE_A
               0  0.000000s     0.000000%       L_UPDATE_TREE_B
               1  0.000000s     0.000000%       L_UPDATE_TREE_C
               3  0.000001s     0.000000%       L_UPDATE_TREE_D
     13474835922  4608.059165s  12.031070%      L_PROCESS_EXPLORE_QUEUE_A
            1289  0.000441s     0.000001%       L_PROCESS_EXPLORE_QUEUE_ES
               0  0.000000s     0.000000%       L_PROCESS_EXPLORE_QUEUE_B
            1301  0.000445s     0.000001%       L_PROCESS_EXPLORE_QUEUE_EB
               0  0.000000s     0.000000%       L_PROCESS_EXPLORE_QUEUE_C
       444841248  152.124657s   0.397179%       L_PROCESS_EXPLORE_QUEUE_D
            6757  0.002311s     0.000006%       L_PROCESS_EXPLORE_QUEUE_EZ
              90  0.000031s     0.000000%       L_PROCESS_EXPLORE_QUEUE_ED
      3392836289  1160.265731s  3.029310%       L_PROCESS_EXPLORE_QUEUE_R
            5614  0.001920s     0.000005%       L_SHOULD_DIE_EQ
           12465  0.004263s     0.000011%       L_SHOULD_DIE_N
     83950847213  28709.104375s 74.955903%      EXEC
         1489637  0.509419s     0.001330%       OBSERVE
      1513332405  517.522091s   1.351186%       COMMIT


Unclear why this lock is still so contentious. UI thread maybe?
Or perhaps all of the workers are doing the same amount of work and so
they tend to sync up? Hidden use of tree mutex?

From a long run (only difference here was to randomly stick with a node
during regular search):

Perf counters:
     10661407535  3645.936543s  0.904135%       L_COMMIT_QUEUE
          111789  0.038229s     0.000009%       L_GET_NODES_TO_EXTEND
          175245  0.059929s     0.000015%       L_GET_EXPLORE_NODE_E
           24964  0.008537s     0.000002%       L_UPDATE_TREE_A
               4  0.000001s     0.000000%       L_UPDATE_TREE_B
              10  0.000003s     0.000000%       L_UPDATE_TREE_C
               5  0.000002s     0.000000%       L_UPDATE_TREE_D
     97279232527  33267.081070s 8.249718%       L_PROCESS_EXPLORE_QUEUE_A
            9086  0.003107s     0.000001%       L_PROCESS_EXPLORE_QUEUE_ES
               0  0.000000s     0.000000%       L_PROCESS_EXPLORE_QUEUE_B
            6015  0.002057s     0.000001%       L_PROCESS_EXPLORE_QUEUE_EB
               0  0.000000s     0.000000%       L_PROCESS_EXPLORE_QUEUE_C
      2319571778  793.235929s   0.196710%       L_PROCESS_EXPLORE_QUEUE_D
           50777  0.017364s     0.000004%       L_PROCESS_EXPLORE_QUEUE_EZ
             891  0.000305s     0.000000%       L_PROCESS_EXPLORE_QUEUE_ED
     78772343974  26938.184903s 6.680250%       L_PROCESS_EXPLORE_QUEUE_R
           45319  0.015498s     0.000004%       L_SHOULD_DIE_EQ
          149454  0.051110s     0.000013%       L_SHOULD_DIE_N
    947903076540  324159.305893s        80.386459%      EXEC
        13764775  4.707211s     0.001167%       OBSERVE
     10664200222  3646.891573s  0.904372%       COMMIT


Also, this version of the code seemed pretty bad: We only got to the
beginning of the second level after running for 90 minutes...
It may now just take too long for a good node to make its way into
the tree and get picked up by other workers. :/

Using shared_mutex for should_die, before switching to shared_mutex for
everything:

      1633717930  558.689612s   0.679719%       L_COMMIT_QUEUE
            8426  0.002881s     0.000004%       L_GET_NODES_TO_EXTEND
           17805  0.006089s     0.000007%       L_GET_EXPLORE_NODE_E
              43  0.000015s     0.000000%       L_UPDATE_TREE_A
            5878  0.002010s     0.000002%       L_UPDATE_TREE_B
               2  0.000001s     0.000000%       L_UPDATE_TREE_C
               0  0.000000s     0.000000%       L_UPDATE_TREE_D
     25585903925  8749.722633s  10.645175%      L_PROCESS_EXPLORE_QUEUE_A
            1676  0.000573s     0.000001%       L_PROCESS_EXPLORE_QUEUE_ES
               0  0.000000s     0.000000%       L_PROCESS_EXPLORE_QUEUE_B
            1999  0.000684s     0.000001%       L_PROCESS_EXPLORE_QUEUE_EB
               0  0.000000s     0.000000%       L_PROCESS_EXPLORE_QUEUE_C
       743088803  254.117304s   0.309167%       L_PROCESS_EXPLORE_QUEUE_D
            8774  0.003000s     0.000004%       L_PROCESS_EXPLORE_QUEUE_EZ
             195  0.000067s     0.000000%       L_PROCESS_EXPLORE_QUEUE_ED
      2506264081  857.078007s   1.042747%       L_PROCESS_EXPLORE_QUEUE_R
           22850  0.007814s     0.000010%       L_SHOULD_DIE_EQ
          123299  0.042165s     0.000051%       L_SHOULD_DIE_N
    201335983309  68851.740208s 83.767090%      EXEC
         4015155  1.373080s     0.001671%       OBSERVE
      1634314604  558.893660s   0.679967%       COMMIT

Increased the explore queue batch size and now utilization is good:

      1164939060  398.379267s   0.507237%       L_COMMIT_QUEUE
          131256  0.044886s     0.000057%       L_GET_NODES_TO_EXTEND
            9562  0.003270s     0.000004%       L_GET_EXPLORE_NODE_E
           25237  0.008630s     0.000011%       L_UPDATE_TREE_A
               0  0.000000s     0.000000%       L_UPDATE_TREE_B
               3  0.000001s     0.000000%       L_UPDATE_TREE_C
               2  0.000001s     0.000000%       L_UPDATE_TREE_D
      3572417289  1221.675048s  1.555501%       L_PROCESS_EXPLORE_QUEUE_A
             533  0.000182s     0.000000%       L_PROCESS_EXPLORE_QUEUE_ES
               0  0.000000s     0.000000%       L_PROCESS_EXPLORE_QUEUE_B
           25774  0.008814s     0.000011%       L_PROCESS_EXPLORE_QUEUE_EB
               0  0.000000s     0.000000%       L_PROCESS_EXPLORE_QUEUE_C
       906923675  310.144626s   0.394892%       L_PROCESS_EXPLORE_QUEUE_D
            5692  0.001947s     0.000002%       L_PROCESS_EXPLORE_QUEUE_EZ
             362  0.000124s     0.000000%       L_PROCESS_EXPLORE_QUEUE_ED
      1112620152  380.487543s   0.484457%       L_PROCESS_EXPLORE_QUEUE_R
            3295  0.001127s     0.000001%       L_SHOULD_DIE_EQ
          113099  0.038677s     0.000049%       L_SHOULD_DIE_N
    208888700723  71434.575768s 90.954243%      EXEC
         5442099  1.861058s     0.002370%       OBSERVE
      1165209193  398.471646s   0.507355%       COMMIT

... note that 10% is being spent somewhere that's not accounted for!
