
TO-DO:
 - make regular letters version
    (should allow multiple paths? or just ugly i,j?)
Optional:
 - kerning?!
 - could improve routing in planner
 - generate tree in sml, but explore anagrams in js (table idea)

fun stuff:
 - pixels
 - 7 segment display
 - absurd letter decompositions like:

 s =    -
       c
        -
         )
        -
        
computer science stuff:
 - post correspondence problem


 video ideas:
 start with like, "if I can rearrange the letters,
                   why can't I flip them around?"
 .. then generalize.
 hamburgefontsiv
 linear logic
 loops u -> v -> u -> v -> ...
 extreme version: 7 segment display
 make a font that actually realizes this compositionality
 animate word transitions

"Self driving car-achters"

ANAGRAPH GENERATOR

very educational video

some that don't need to break apart letters:

b and p are not actually the same shape tho :(
  preposterous = obstreperous

  digital = pigtail

some nice pairs:

  impeach groper = orange diarrhea

  vulnerability = authenticity

  donald trump = unnatural clod
                 plutocrat man

  covfefe = pee off

  youtube = fun alone = worry etc

  they might be giants = gesticulating mutely

pixel font:
  facebook = priapism = typeface = ...

  donald trump = worst hair job

Unless you've been "undemonstrably workaday",
"glarorously thumbtacked", 
you know what anagrams are.

Unless you've been portmanteau wordplay shy,
you know what anagrams are.

Unless you wage a tyrannosaur mohawk,
you know what anagrams are.


INV
- donald trump = drum up a plot
- youtube = by tenon
- quad / band, 
- pd: flipping / fiddling, pigtail / digital
- qb: question / bounties, 
- un: tofu / font
- all: quip, bind

"allow rotating letters" inv:
  - gaslit waterloo turtle
  - tolerate rotting walls !
  
worst anagrams
photomicrograph
microphotograph
 jeremy's iron

stuff to try:
 - juicero



[ MORE ABOUT WORDS ! ]

$ make && time ./anagraph knowwhatanagramsare -maxwords 3 -banned raga,ragas,agar,agars,grana,skag,manana,kasha,sanka,kanas,sanga,ankara,agama,gratae,anagram,organa,ogham,swagman,tanka,astrakhan,tanager,anagrams,mananas,anna,ghana,wark,know,hank,khan,rara,rowan,wantage,goshawk,shako,rowans,warks,anther,thanes > deleteme2

Unless you warrant a swank homage,
you know what anagrams are!

Rearranging letters to make different words.


But if you're doing this with physical letters
you know, not imaginary letters but real ones,
then why not allow me to rearrange the letters
any way I want? Like I could rearrange

  jaw to jam

or speaking of jaw jams,

  bechamel     to     chewable




I call these anagraphs, because "anaglyph" is already taken
by these kind of 3d glasses (zelda graphic) for some reason.
anagraph comes from "anag", which means anagram, and "raph",
which means "rotate and pluck h______"


kerfning for the cut away part

next:
 - bad ways, like s = two cs
 - show the whole alphabet
 - show the font program
 - talk about how to generate anagraphs recursively
  - decompose a letter into its set of atoms
  - for single words, we can just check to see if they have the
    same atoms.
  - if not, pick some word that's a subset, and then recurse
  - inception rant
 - mention how this requires canonical breakdowns
   - show problem with d = "cl" or "ol"
   - this allows me to make infinite cs, os, etc.
   - the c's get kinda dumpy. conservation of matter
   - should clarify in this part that I'm thinking about the case
     that I allow for really weird letters and rules
   - note that in the real world, there's this "kerf"
      aka "kerfning".. btw kerning is impossible
   - reference other vid
 - further generalizations (pixels)
 - end with some good anagraphs I guess


** undecidability of "generalized kerning" ** 


my editorial board (THIS GUY) decided that this was "too boring"
to include in the original video

(Reference other video as intro and more fun. Tom Academy)
This video is pretty much a serious educational video about a topic
that I find interesting. If you feel you already know this stuff,
I encourage you to try to anticipate where I'm going. In particular,
I'd like to make a point about what passes for a proof and how easy
it is to be mistaken. So try to notice the places where I make
a mistep before I point them out.

So I talked about the anagraph problem, and the related kerning problem.
These are very similar but one is possible and the other is impossible!
We use a similar technique to prove both results, called reduction.
So in this episode I'm going to describe those two problems precisely,
then wield that reduction technique in two opposite ways to prove that
one problem is possible and the other is not.



   
Let's first recap the problems. (Should briefly describe anagraphing
to make this standalone from the other video)
In the other video I was a bit loose about this, but it's important
to be precise if we're going to prove something. I also want to emphasize
that these problems are both easy for English letters. So we're really
talking about the generalized problem. You have some alphabet of letters:

    a b  c   ... alien letters

Again, I'm generalizing, so I'll have letters that don't exist in
English. Also since we break apart letters, there may be some letters
in that list that aren't part of English, like the hook. In the previous
video I distinguished between "atoms" and "letters", but here they are
all the same.
Then a word is just made up of a sequence of letters. The essence
here is then these rules that let me "break apart" and "combine" letter
sequences to make other letter sequences. For example we might have

    rn <=> m

or even

    AA <=> TVT

The rules operate in either direction. There can be more than one way
to rewrite a particular sequence. We saw

    d <=> ol
    d <=> cl

and how this leads to the generation of an infinite number of Cs. So
we don't have conservation of matter or anything. But in the
generalized problem, we don't even require that the rules be
physically realistic. I could just have

    l <=> ll

which is a little bit plausible, or like

    cat <=> dog


So now the generalized anagraph problem can be stated simply as

  Given an alphabet,
  and some rewrite rules,
  and the ability to rearrange letters,
  and two words,
  can you turn one word into the other?

This problem is solvable, but in my opinion it's not obvious. Let's
start with the unsolvable problem, the generalized kerning problem:

  Given an alphabet,
  and some rewrite rules,
  BUT NOT the ability to rearrange letters,
  and two words,
  can you turn one word into the other?

The only difference is that I can't rearrange letters. I can only
use a rewrite rule on one part of the word and leave it where it is.
So I can do

     kerning <=> keming

but for example not

     kerning <=> kenimg

(these could be generated by anagraph)

BTW it's probably more accurate to call this the "generalized ligature
problem," but kerning somehow commands a certain mystique. (...)


OK so I want to show that this problem is not possible. What do I mean
by that? Well, first of all to say we can solve the problem, we have
to solve it for ALL instances, not just easy ones. In fact, we have to
solve it for really devious ones, so it's useful to think of the
inputs as being supplied by a clever adversary. Skeletor gives us
an alphabet, the rewrite rules, and a pair of tricky words. To say
that we could solve the problem means that we have a procedure that
works no matter what Skeletor gives us. It's not just enough to
believe or assert that we'd be able to do it using our cleverness
applied to each instance. We have to give a procedure. So we're going
to prove that there is no such procedure.

Incredibly, in computer science we know that any procedure can be
performed by a simple computer called a Turing Machine. A turing
machine is simple. You have an arbitrarily long tape that contains
only 0s and 1s, like this:

      ...0000111100110100011000000...

We treat the ends as having zeroes in both directions, like how we can
think of a number as having as many invisible zeroes at the start as
we want, or as many zeroes in the decimal part. And the turing
machine has a cursor

      ...0000111100110100011000000...
                   ^

like the read/write head in your VCR. The cursor always has some state,
which we'll give as a letter.

      ...0000111100110100011000000...
                   ^
                   a

And finally it has some rules, which tell the cursor what to do in
each case. The rules are the turing machine's program, and the tape is
the input.

       state   tape   write    new state   go
A0       a       0      1         f        right
A1               1      0         g        right
------------------------------------------------------
B0       b       0      1         a        left
B1               1      1         a        right         
          ...

That's it. The machine just follows the instruction at each step and
transforms the tape. It could signal when it's done by writing a special
sequence of bits to the tape, or moving the cursor to a designated spot.

It's pretty mind-blowing that something so simple could allow us to
build any procedure. If it can be done by a Pentium it can be done by
a Turing Machine. If it can be done by a quantum computer, it can be
done by a Turing Machine. We have never discovered, and don't expect
to discover, any physical procedure that allows us to solve more
problems than a turing machine. And actually there are lots of
alternatives to Turing Machines that are even simpler, and it isn't
even my favorite, but it is traditional and useful for this particular
problem!

So we could prove that there's no turing machine (really, turing
machine program) that solves this problem. It's usually pretty hard to
prove that something doesn't exist, but there's already a powerful and
ancient non-existence proof for Turing machines. Specifically, it's
impossible to devise a procedure that takes any turing machine program
(like that table of rules) and any two tapes, and decides whether
running that machine would turn the one tape into the other. Let's call
this the prediction problem.

I'm not going to explain how that proof works in this video, but I'm
sure there are hundreds of good descriptions of it out there. You
could search for the "halting problem." But this proof is utterly
beautiful. Maybe the best of all time. The basic idea relies on that
idea that any procedure can be performed by a turing machine. We prove
that no such decision turing machine could exist. Because if it did,
then Skeletor could use it to make a "test" turing machine that tricks
the "decision" machine by first running its own simulation of the
decision machine with the test program as input, and then doing the
opposite of what it says. So the decision machine is always wrong. So
therefore no such decision machine exists.

Anyway, it is well known that there is NO procedure (turing machine)
that can take the rules of a (different) turing machine as input and
decide whether it turns one tape into another.

So now we can use this to prove that the kerning problem cannot be
solved. We'll say

   1. For any instance of the turing machine prediction problem,
      we can turn it into the kerning problem.
   2. So, imagine that we had a procedure for the kerning problem.
      We could use it to solve any any instance of the prediction
      problem.
   3. But this is impossible, because the prediction problem
      can't be solved in general. So it can't be the case that
      there's a procedure for solving the kerning problem.

This is a proof by contradiction. Since we transformed one problem
(whose solvability is in question) into the other (whose solvability
is known) this is a reduction argument. We'll also use reduction
in the opposite way to prove that a problem IS solvable later.

So all that's left is to prove #1. What we'll do is show how, given
a turing machine (basically a set of rules), we can turn it into
an alphabet and rewrite rules for kerning. This will involve coming
up with a fairly devious alphabet, so we'll be playing the role of
Skeletor. We'll call this a Keming machine.

I hope some of the parallels are already clear. The input tape becomes
the input word, so we'll want the "letters" 0 and 1.

   turing machine on left                        keming machine

  input tape
       10010111                                    10010111


In order to represent where the cursor is, we'll introduce two letters
[ ], which will surround the letter where the cursor is.

       10010111                                    1001[0]111
           ^

There will always be just one [ and one ]. Next, the cursor has some
state associated with it, which we said was a letter. So we'll
include that letter right after the brackets in our word

       10010111                                    1001[0]a111
           ^
           a

and finally, we'll turn each rule in the turing machine into a rule
in the keming machine. Take this one:

       state   tape   write    new state   go
         a       0      1         f        right

it happens to apply in the example. We want it to write a 1 and move
the cursor to the right and become state f, so something like
   [0]a*  <=>    1[*]f

but we didn't say we could make wildcards like this. But there are only
two things that can be on the tape there, so we can just make two rules
   [0]a0  <=>    1[0]f
   [0]a1  <=>    1[1]f

Show a leftward rule too.

Cool! It's straightforward to translate the whole table of rules into
kerning rules. There is one problem with this, which is that we assume
the turing machine tape has an arbitrary supply of zeroes on either
side of it, but in the keming machine we just have some word of finite
length. So let's actually mark the ends of the word

       10010111                                   <1001[0]a111>
           ^
           a

and now for rules that move right, we include a case for when the
cursor is at the end of the word. This should behave exactly like when
there's a zero there, although it needs to preserve the end-of-word
marker.

   [0]a>  <=>    1[0]f>
   [0]a0  <=>    1[0]f
   [0]a1  <=>    1[1]f

That's it! To decide the turing prediction problem, we'd translate
the machine into a keming machine, use our hypothetical procedure
to determine whether the one word can be transformed into the other,
and then


Semidecision approach.

  real answer          keming procedure     run turing machine

     yes                   yes                     yes

     no                     no                (loops forever?)

 keming no => real "no"
 turing yes => real "yes"
 but we could have keming "yes" and no answer from turing, and
 we wouldn't know whether it's real.


  So we simply run the turing machine in parallel with the keming
  reduction. If keming returns "no" then we know there is no trace.
  If it returns "yes" then we can't be sure. But unless the turing
  machine loops forever (in which case the answer is "no" and the
  keming machine must return no... no wait this is wrong! There
  may be no way to do it using the forward approach.

  DOES NOT WORK.

Fancy "trace" approach.

  Word looks like this

  trace{bits[C]Sbits}

  where C is the current bit and S is the current state. What
  we're going to do is explicitly record the trace of turing
  machine instructions we used. This will prevent us from running
  "incorrect preimage" instructions.

  So now when I execute a step,

                 0101      ->       1101
                 ^b       (B0)       ^a

  I go into an intermediate state with a < shuttle.
  
                [0]b101           1<B0|1]a01

  Note that when I'm in this state there is no pair of brackets,
  so none of the normal rules apply. Instead I use this state to
  send the rule used onto the trace, like so:

     0<??    ->    <??0          (for all rule symbols)
     1<??    ->    <??1

  and when it hits the end of the word,

     {<??    ->    ??{>  

  this pushes it onto the end of the trace, and then reverses the
  shuttle. Then we have rules for the shuttle's return:

     >0      ->    0>
     >1      ->    1>

  and for returning back to the normal mode

     >|      ->    [

  which in the above example results in

    B0{...1[1]a01     as desired.


  It's still possible to run these rules in reverse. For example
  we could turn [ into >| and then run the > shuttle back to the
  trace, and pick up B0. And even bring B0 back to our word. But

  in fact in this setup there is no ambiguity about what rule
  applies at any step. There's always exactly one "forward" rule
  corresponding to the next turing machine step (or intermediate
  shuttle movements), and one backward step that corresponds
  to undoing it.

  So the proof is done right?

  J/K! Another bug. We originally said that our keming machine
  solver would tell us if one word translated into another. Given
  a turing problem like

    0111            --?-->         101
    ^a                             ^a
    
  we would ask

   {[0]a111}        <==?==>        {[1]a01}

  or something. But now there will always be some trace left, and we
  don't know the trace before we ask for it! So we don't know what
  question to ask the keming solver.

  So the last thing to do is to add a final phase. We have something
  like

  A0B1B1C0C0C0B1A0{001001[1]a010}

  and we want to turn it into a normalized string so that we know
  what question to ask. In this case it will be

  (1001[1]a01)

  and the way we accomplish this is that we allow turning { into
  finalizing parens (, like so:

  { <=> (

  (ok this is obviously pointless since then { and ( would be
   equivalent)

  So uh.. how? If I allow something simple like

  A0{ <=> {

  then this allows me to insert arbitrary steps into the trace,
  which has the same problems as before. And if deleting always
  gave us finalized parens, like

  A0{ <=> (            (for all states)

  then we could still do shenanigans by taking

  A0B1B1C0C0C0B1A0{001001[1]a010} ->
  A0B1B1C0C0C0B1(001001[1]a010} <-
  A0B1B1C0C0C0B1A1{001001[1]a010}

  again messing up the trace.


  (soooo.. we could change what we're trying to prove, to prove that
   "can w1 be kerned to any string where w2 is a substring?" That
   is undecidable because of the argument here. If we could solve
   it, then we could build a turing machine with the above argument,
   with no false positives because we actually ask if the bracketed
   string { } is a substring. (need to eliminate leading/trailing
   zeroes but this is safe.) But this is a weaker statement than
   the original kerning problem--it might be undecidable despite
   kerning an exact pair being decidable. Unless there is a reduction
   there? We'd need to prove that any instance of the substring
   problem can be turned into an exact problem. "if we can solve the
   exact problem, then we could solve the substring problem"

   so uh, how would we do that? 

   the substring problem looks like this:
   w1  <=?=>  ...w2...

   obviously if w1 <=> w2 then the answer is yes, but maybe it requires
   adding extra stuff. We could put sentinels on the ends and then allow
   deleting arbitrary characters there, i.e. we ask

   w1 <=> (w2)           (exact)
   with the rules
   (a <=> (          (for all letters a)
   a) <=> )

   which is sorta along the right lines, but fails for the same
   reason as such tricks above fail: We can use it to insert arbitrary
   strings at the ends of the word, so we might get a "yes" that's
   fake (doesn't really solve the problem) because it cheats.

   maybe some way to recover here?)

  Bidirectionality is the issue in the first place! If we had
  unidirectional kerning rules then we'd be fine (without the
  special shuttle stuff).

  So: Unidirectional kerning is undecidable.
  If we have a bidirectional kerning problem, we can't use
  unidirectional kerning to solve it, but it might still be decidable
  anyway.
  We'd have to show that if bidirectional kerning were decidable,
  unidirectional would be (a contradiction). This means taking any
  unidirectional problem and making it a bidirectional one. How?

  So we could just gloss over this in the first place and say that
  the generalized problem is the unidirectional one. This is indeed
  a generalization, and makes the turing machine reduction much more
  straightforward (we don't need the trace or anything). I do kind
  of like that trace idea though!

  So I know how to do these:
    - Prove that the bidi substring problem is undecidable
    - Prove that the uni exact problem is undecidable
    

** Older notes **



Say we have an alphabet of symbols, and then a system of "kerning"
rules that let us rewrite sequences of symbols into other sequences.
Like

     rn   <=>   m

     vv   <=>   w

     ol   <=>   d
     lo   <=>   b
     
(both directions). Then in fact it's UNDECIDABLE whether one word
is a kernogram of another! With the ability to create an arbitrary
alphabet and kerning rules, we can make a turing machine


There's a single head, which we'll write as

 [ ]

around a symbol 1 or 0. The head has associated with it some state,
which we can write as a subscript on the brackets

 aaa[1]aaa
      S

Then we give the turing machine instructions like so,

  state        symbol        write        move    new state
    S             1            1            R         T

by expanding into rewrite rules

    [1]0  <=>   1[0]
      S            T

    [1]1  <=>   1[1]
      S            T

Note:
  - we expand for every possibility of the bit to the right;
    for a two-symbol turing machine this is just 0 and 1, easy

  - The rules can be run symmetrically, but we construct the
    encoding such that only one rule ever applies

But then we need to do something about the blank ends of the
tape. It doesn't obviously work to have a rule like

    [1]   <=>    1[0]
      S             T

that allows inserting zeroes if there's no symbol there, because this
rule would also apply in the case above when there ARE symbols
(inserting a new zero!). But we can simply arrange that for a delimeter
symbol (write |) that's on each end of the string by invariant. Then
each rule that moves right has three versions:

    [1]0  <=>   1[0]
      S            T

    [1]|  <=>   1[0]|
      S            T

    [1]1  <=>   1[1]
      S            T

and the embedded problem to turn 'stringone' into 'stringtwo' is actually
|stringone| into |stringtwo|.

we could also maybe make the encoding smaller by just adding generic
rules for this

    |   <=>   0|
    |   <=>   |0

or maybe you want them to be directional

    <   <=>   <0
    >   <=>   0>

I think this works fine, but it makes the argument harder, because now
there is always ambiguity in what rules apply (adding and removing
leading and trailing zeroes is always allowed).

The argument is as simple as that: If you have local rewrite rules,
you can easily build a turing machine. Now to be fair it doesn't really
look like kerning, since a rewrite like

      [1]0  <=>  0[0]
        S           T

is not very letter-like at all? 


As an aside: Notice that, other than the fact that I can run rules
backwards in the keming machine, there's only going to be one rule
that applies at any time. So this works even when the kerning problem
does not allow ambiguous rules. (Actually, not true! The RHS can be
duplicated.) (Actually, is it a problem that rules can run backwards?
Because two steps could yield the same string, allowing transitions
that are not permitted in the turing machine. Any simple way around
this? Maybe leave a breadcrumb of which rule is used at each step,
ensuring that the RHS is unique? And then blow up the rules to
wildcard over this breadcrumb on the LHS?)

We want to make sure that every backwards step corresponds to a
backwards step on the turing machine.

First take every rule and label
it with the Turing machine rule we used to generate it. We can write
this one before the cursor.

A0.
   [0]a>  <=>    1(a0)[0]f>
   [0]a0  <=>    1(a0)[0]f
   [0]a1  <=>    1(a0)[1]f

but can't leave these in the rule. So we then take every rule and
"wildcard" over all possible symbols that can be there.

   (**)[0]a0  <=>    1(a0)[0]f
   (**)[0]a1  <=>    1(a0)[1]f

but we don't really have wildcard so this blows up into a family
of rules, where we put each turing rule number in there.

   (a0)[0]a0  <=>    1(a0)[0]f
   (a1)[0]a0  <=>    1(a0)[0]f
   (b0)[0]a0  <=>    1(a0)[0]f
   (b1)[0]a0  <=>    1(a0)[0]f
     .
     .
     .
   (z0)[0]a0  <=>    1(a0)[0]f
   (z1)[0]a0  <=>    1(a0)[0]f

   (a0)[0]a1  <=>    1(a0)[1]f
     .
     .
     .

For example, let's say that we have this turing machine

       state   tape   write    new state   go
A0       a       0      1         f        right
A1               1      0         g        right
------------------------------------------------------
B0       b       0      1         a        right
B1               1      1         a        left         
          ...
C0       c       0      1         a        right
C1               1      1         a        left         


                 0101      ->     1101
                 ^b                ^a
                 
    and we do   [0]b101           0[1]a01

    but this state matches the RHS of rule C0 (and C1!); we have

                0[1]a01    <=>    [0]c101    (an unreachable state)
       or even  0[1]a01    <=>    01[1]c01

Neither of these correspond to possible turing machine moves. What
if one let us reach the target word?

If we annotated rules with which one we used, we'd have

   (A0)[0]a0  <=>    1(B0)[1]a
   (A1)[0]a0  <=>    1(B0)[1]a
   (B0)[0]a0  <=>    1(B0)[1]a
   (B1)[0]a0  <=>    1(B0)[1]a
     .
     .
     .
   (Z0)[0]a0  <=>    1(B0)[1]a
   (Z1)[0]a0  <=>    1(B0)[1]a   

Each time recording that we used rule B0. Now the rewrite above is
something like this

                 0101      ->       1101
                 ^b       (B0)       ^a
                 
    and we do   (??)[0]b101       0(B0)[1]a01

this can only be rewound with rules from the B0 set, which all
correspond to the same real turing machine rule. However, we could
choose a different () in the previous state, which allows the same
problem, ugh!


OK, last try. Let's say that we instead record the direction we
moved and the bit that we overwrote.

                 0101      ->     1101
                 ^b                ^a
                 
    is now   (??)[0]b101       (R0)0[1]a01

No, this possibly allows us to only rewind to states that will
do the exact same thing. But the real problem is that we wildcard
over ?? on the LHS, which means that we can diverge from the
original trace in just one step. I think if we wanted to make
this work, we'd need arbitrary storage.

But don't we have arbitrary storage in the keming machine?
Well sorta. If our cursor only ever moves right, then we can
do something like


)[0]b1   ->  B0)0[1]a

which "pushes B0 on the stack" inside the parens. But without doing
something fancy, how would we 
