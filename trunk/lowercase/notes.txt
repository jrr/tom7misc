
lowercase 24 Dec 2020


on the second problem (smaller vector),
switched to a normalized point order at round 24820.

(this rapildy dropped the total err from like 17 to 12.79)

switched to permissive loss function at round 70808

(total error went up though... was like 18 and is now 51. seems wrong?)



-- sdf training started 1 Jan 2021  13:01:36 --

Got nans after about 40 rounds. Shrink to 1/4 of the examples per round
and halve the learning rate.

(23.6 eps per model with these new settings. I think it was like 40 before?)


nanned out again after 158 rounds each.
tried actually rejecting nan updates in updateweights.

22.29 eps, so the nan checks aren't THAT expensive...

made it to round 242 so far without any issues (just predicts "average" shape though)

since updateweights has protection now, putting the learning rate back up

... but then it immediately blew up (round 268)!


didn't touch it for a while but came back to about round 640.
I hit 'v' to look at some of the stimulations/errors and watched it diverge in front
of my eyes! Maybe the divergence is actually a bug caused by the UI??

Wow I'm stumped... the behavior of vlayer (which is the only key that does anything)
is really straightforward. Due to another bug it was exporting and saving on every
round, so I guess that increases the possibility of race conditions here. Added some checks
for in-boundsness. I guess technically, writing to 'dirty' with a Read lock held on
the shared-mutex is wrong, because the ExportBlahToVideo also writes to dirty, and
it's plausible that this actually has an effect since it's just a bool (conflicting
writes are probably touching other nearby values). No idea how that could lead to
the network's weights getting corrupted, though, except via general "undefined behavior".
Switched to writemutexlock to see if this ever happens again.

(Also saves a lot of time... 42 eps now)

ok nevermind, it just happened on its own after 547 rounds (each)
while unattended (and with those fixes). (It must have just been a coincidence??)


--------------------------------------------------
trying again!
New mode in updateweights (can be performance tuned) caps not the update but the
final weight value. Also not very principled. But this should make it quite hard
for it to ever produce nans, right?


even with a max weight of 8192 it quickly gets off the rails
trying again with 128!

Note that the output pedictions can be pretty big (especially early on) and that
these can then be inverted examples for the other model. This may actually be
the cause of crazy results.. so maybe we should normalize those SDFs before
inverting them (or reject them as training examples if they are too far out
of range)


even with updateweights capped to 128, we get divergence.
maybe the problem is really in the error step, so I tried clipping there
as well.

(also fixed another bug where network would not save unless you got lucky
when you stopped it, oops!)


even with a max weight of 8.0f (!) the stimulations get huge


ok then I tried again with initial weights using "He initialization". These
are bigger than what I was using before. It immediately diverged.


ran 6000 rounds over night.
these are under control at least, but making really conservative
prediction (just "average") and the error doesn't seem to be improving.
put them in sdfmodels as a good starting point?

"just predicts the average" could be what happens if we only update biases
and never weights, by the way


well, with those settings was still stalled with super low activations


okaaaaaay, trying again with fewer layers, all dense
(these have twice as many total weights though!)

31 eps, but, actually maxing out the GPU now
... aaand it blows up in 16 rounds


3 Jan 2021
Changed a few things again and now it's making progress (overnight, 7042 rounds)
I think I partly just got lucky because the lowercase model blew up,
but the uppercase one was still generating some nice average blobs, so I copied
the uppercase model oevr the lowercase one... in early stages it's fine for
them to have the same cold start (this could even be an explicit strategy in
the future to save some time training?)

 - used two hidden layers, but bigger ones (1x square, then 1.5x square, then output)
 - all dense layers
 - use sigmoid in output layer
 - "Tom initialization"
 - decay weights 0.9995f
 - 

9 Jan 2021

looking at the histograms on biases and weights, most weights are really close to zero,
on all of the layers. Both models evolved similar distributions.
Layer 0: -.05 to .069
Layer 1: -.29 to .53
Layer 2: -.04 to .025

this does suggest that the weight capping is unnecessary, at least in this epoch.

biases are also very spiky; layer 0 is close to zero, with some falloff towards the
negative side (kinda makes sense, this would be a feature that's looking for a
sum of input nodes above some threshold)
On layer 1, almost all biases are 0.0051
Layer 2 (sigmoid) has a nice histogram with lots of activity across the spectrum
from -1 to 0.5. Probably this is encoding the "average output".

At 63831 rounds, train error seems to be quite flat (around 230/example) so I'm
going to try spicing it up.

before vacuuming, we were getting around 37 eps (for each model).
on disk the model was 370,127,092 (note that dense models are more efficiently
stored)

I also increased the learning rate (it ended up at about .05). The result was
that the lowercase model seemed to diverge... it performed fine right after
vacuuming but then got bad after a few rounds of training? Again the lowercase
model is the one that exploded.
(Also, eps was similar at about 35.)


Did it again, same settings but effective training rate of .085; blew up right away.
Again with effective learning rate of 0.0319. Also blows up, at least at first?
This one looks like

A few hypotheses about this then:
 - maybe learning rates are way too high? It doesn't seem like we'd want this
   to jump around a lot once we get this deep into training.
 - Something about the inversion training examples encourages this behavior.
   A feedback loop, etc. But the sigmoid should prevent the output from
   being outside [0,1] and we think that the predictions were actually good
   in the first few rounds here.
 - Just some kind of actual bug in training?? We have seen it work
   (for example the vector version is clearly doing *something*), but there
   could still be bugs.
 - (maybe vaccuming messes up the network? but we check the inverted indices...)

It's really weird that the network actually performs well at first, but then
gets much worse error after a few rounds of training. How is that possible?

well, the lowercase side seemed to smooth out (error = 339) but the
uppercase side has a burned-in superbright region, and the eval is
bad (and looks basically constant) on both.


--------------------------------------------------


sparse model... weights go to the max (+/- clamped) by 7 rounds (maybe immediately)!
maybe we should put these histos in the training ui



after all the futzing, the thing that seemed to get it on track was to
set the learning rate (high end) to 0.01f.>



at 196961, changed decay to .999995 (from .9995) since so many weights
were almost zero.

(total error started dropping really quickly after that!)
... seems like a very worthy change. Error dropped from ~70 to ~50 after a day,
even though the learning rate has nearly bottomed out. The eval results
are almost interesting now! It's really obvious from the error history that
this got it out of the plateau.

I see a lot of weights with values at, or nearly at zero though. Seems
like we may be wasting our parameter budget then. Should think about some
kind of vacuuming + expansion?

vacuumed at round 599041, all layers with a threshold of 0.0000001.
(I got this threshold from inspecting histos and intended to set it really
conservatively. One way we could think about setting this is that weights
are always in -8,8 (when clipping is on), so if you have indices_per_node = ipn,
then the max value a relu node could take on is ipn * 8. So something like
1 / (ipn * 8) is something like the threshold in which any node is meaningful,
because if all nodes were outputting the same signal at that magnitude, it would
register a '1' in that case. Also would need to take into account biases.)

This only reduced ipn by 7 and 13 for the two models, though.
So try again with 0.00001f. Now more significant:

model 0: 259 ipn to 259, 218, 201, 208.
model 1: 259 ipn to 259, 204, 183, 218.

The most common weight value seems to be zero or almost zero, still. I guess
the rectangularity constraint causes a lot of slack in this regard. It definitely
adds to the simplicity of the code, but maybe we'd benefit in performance from
relaxing that?

maybe should be exporting eps too, but looks like it went from about 300 to about 350.
we'd expect a linear speedup here so it makes sense. how will the error fare?
Seems like it continues the trend... not even any obvious jump at the moment of
vacuuming (which then suggests we could be more aggressive?)

At round 614486, tried turning off error decay completely.
Around this time, also increased the target rounds to 1M, which increases the
effective learning rate a little.

Did a really conservative widening at 639714 rounds.
Error shot way up... 200+, then dropping quickly to ~84.
I guess this is a pretty disruptive operation.

tried again with the next layer weights being 0.
this still caused the error to shoot way up (surprising?) but I'll
let it run over night anyway...
well, it starts converging at least, but error is back up to ~50,
so what is the point of that?


OK, tried again with input weights being 0.0 and next layer weights also 0.0.
But this also has an instant bump in error.
As I understand it, this should be the same network, so it should not have ...
OHHH, the indices on the next layer actually need to be permuted if I add
width, so that's just a bug.
Tried adding height which should not be affected by this bug. The first round
does indeed look the same, then error jumps up briefly, but it's quickly down
to around 40. I thought that nodes that always output zero (which these should,
with input weights of zero and bias of zero) will always stay zero? So this
should not have effect even after some training? It's possible I'm just
attributing variance to this change, but it's the same as it always was.
Looking at the activations themselves (just from screenshots in ui) these two
new rows look totally dead, as expected. There are also a lot of other nodes
that appear dead. Probably worth trying to do a "vacuum"-like process to
remove these dead nodes. Have to be careful about useful features that are
just rarely-activated, though.
  - Nodes whose input weights are all very low, and which have low bias, are
    heuristically dead. (Could also look at the weights by which the node
    is used, which tells you something about its downstream significance.)
  - Also can just run on lots of data. Could be random input data, or could
    be training examples. If the node's maximum activation is very low, it
    is empirically dead.

How to remove nodes when there is width/height etc.? We could immediately
replace them with new nodes (keeps geometry). Or we could just ditch the
dimensions and use Factorize to make a rectangle. 
