
lowercase 24 Dec 2020


on the second problem (smaller vector),
switched to a normalized point order at round 24820.

(this rapildy dropped the total err from like 17 to 12.79)

switched to permissive loss function at round 70808

(total error went up though... was like 18 and is now 51. seems wrong?)



-- sdf training started 1 Jan 2021  13:01:36 --

Got nans after about 40 rounds. Shrink to 1/4 of the examples per round
and halve the learning rate.

(23.6 eps per model with these new settings. I think it was like 40 before?)


nanned out again after 158 rounds each.
tried actually rejecting nan updates in updateweights.

22.29 eps, so the nan checks aren't THAT expensive...

made it to round 242 so far without any issues (just predicts "average" shape though)

since updateweights has protection now, putting the learning rate back up

... but then it immediately blew up (round 268)!


didn't touch it for a while but came back to about round 640.
I hit 'v' to look at some of the stimulations/errors and watched it diverge in front
of my eyes! Maybe the divergence is actually a bug caused by the UI??

Wow I'm stumped... the behavior of vlayer (which is the only key that does anything)
is really straightforward. Due to another bug it was exporting and saving on every
round, so I guess that increases the possibility of race conditions here. Added some checks
for in-boundsness. I guess technically, writing to 'dirty' with a Read lock held on
the shared-mutex is wrong, because the ExportBlahToVideo also writes to dirty, and
it's plausible that this actually has an effect since it's just a bool (conflicting
writes are probably touching other nearby values). No idea how that could lead to
the network's weights getting corrupted, though, except via general "undefined behavior".
Switched to writemutexlock to see if this ever happens again.

(Also saves a lot of time... 42 eps now)

ok nevermind, it just happened on its own after 547 rounds (each)
while unattended (and with those fixes). (It must have just been a coincidence??)


--------------------------------------------------
trying again!
New mode in updateweights (can be performance tuned) caps not the update but the
final weight value. Also not very principled. But this should make it quite hard
for it to ever produce nans, right?


even with a max weight of 8192 it quickly gets off the rails
trying again with 128!

Note that the output pedictions can be pretty big (especially early on) and that
these can then be inverted examples for the other model. This may actually be
the cause of crazy results.. so maybe we should normalize those SDFs before
inverting them (or reject them as training examples if they are too far out
of range)


even with updateweights capped to 128, we get divergence.
maybe the problem is really in the error step, so I tried clipping there
as well.

(also fixed another bug where network would not save unless you got lucky
when you stopped it, oops!)


even with a max weight of 8.0f (!) the stimulations get huge


ok then I tried again with initial weights using "He initialization". These
are bigger than what I was using before. It immediately diverged.


ran 6000 rounds over night.
these are under control at least, but making really conservative
prediction (just "average") and the error doesn't seem to be improving.
put them in sdfmodels as a good starting point?

"just predicts the average" could be what happens if we only update biases
and never weights, by the way


well, with those settings was still stalled with super low activations


okaaaaaay, trying again with fewer layers, all dense
(these have twice as many total weights though!)

31 eps, but, actually maxing out the GPU now
... aaand it blows up in 16 rounds


3 Jan 2021
Changed a few things again and now it's making progress (overnight, 7042 rounds)
I think I partly just got lucky because the lowercase model blew up,
but the uppercase one was still generating some nice average blobs, so I copied
the uppercase model oevr the lowercase one... in early stages it's fine for
them to have the same cold start (this could even be an explicit strategy in
the future to save some time training?)

 - used two hidden layers, but bigger ones (1x square, then 1.5x square, then output)
 - all dense layers
 - use sigmoid in output layer
 - "Tom initialization"
 - decay weights 0.9995f
 -

9 Jan 2021

looking at the histograms on biases and weights, most weights are really close to zero,
on all of the layers. Both models evolved similar distributions.
Layer 0: -.05 to .069
Layer 1: -.29 to .53
Layer 2: -.04 to .025

this does suggest that the weight capping is unnecessary, at least in this epoch.

biases are also very spiky; layer 0 is close to zero, with some falloff towards the
negative side (kinda makes sense, this would be a feature that's looking for a
sum of input nodes above some threshold)
On layer 1, almost all biases are 0.0051
Layer 2 (sigmoid) has a nice histogram with lots of activity across the spectrum
from -1 to 0.5. Probably this is encoding the "average output".

At 63831 rounds, train error seems to be quite flat (around 230/example) so I'm
going to try spicing it up.

before vacuuming, we were getting around 37 eps (for each model).
on disk the model was 370,127,092 (note that dense models are more efficiently
stored)

I also increased the learning rate (it ended up at about .05). The result was
that the lowercase model seemed to diverge... it performed fine right after
vacuuming but then got bad after a few rounds of training? Again the lowercase
model is the one that exploded.
(Also, eps was similar at about 35.)


Did it again, same settings but effective training rate of .085; blew up right away.
Again with effective learning rate of 0.0319. Also blows up, at least at first?
This one looks like

A few hypotheses about this then:
 - maybe learning rates are way too high? It doesn't seem like we'd want this
   to jump around a lot once we get this deep into training.
 - Something about the inversion training examples encourages this behavior.
   A feedback loop, etc. But the sigmoid should prevent the output from
   being outside [0,1] and we think that the predictions were actually good
   in the first few rounds here.
 - Just some kind of actual bug in training?? We have seen it work
   (for example the vector version is clearly doing *something*), but there
   could still be bugs.
 - (maybe vaccuming messes up the network? but we check the inverted indices...)

It's really weird that the network actually performs well at first, but then
gets much worse error after a few rounds of training. How is that possible?

well, the lowercase side seemed to smooth out (error = 339) but the
uppercase side has a burned-in superbright region, and the eval is
bad (and looks basically constant) on both.


--------------------------------------------------


sparse model... weights go to the max (+/- clamped) by 7 rounds (maybe immediately)!
maybe we should put these histos in the training ui



after all the futzing, the thing that seemed to get it on track was to
set the learning rate (high end) to 0.01f.>



at 196961, changed decay to .999995 (from .9995) since so many weights
were almost zero.

(total error started dropping really quickly after that!)
... seems like a very worthy change. Error dropped from ~70 to ~50 after a day,
even though the learning rate has nearly bottomed out. The eval results
are almost interesting now! It's really obvious from the error history that
this got it out of the plateau.

I see a lot of weights with values at, or nearly at zero though. Seems
like we may be wasting our parameter budget then. Should think about some
kind of vacuuming + expansion?

vacuumed at round 599041, all layers with a threshold of 0.0000001.
(I got this threshold from inspecting histos and intended to set it really
conservatively. One way we could think about setting this is that weights
are always in -8,8 (when clipping is on), so if you have indices_per_node = ipn,
then the max value a relu node could take on is ipn * 8. So something like
1 / (ipn * 8) is something like the threshold in which any node is meaningful,
because if all nodes were outputting the same signal at that magnitude, it would
register a '1' in that case. Also would need to take into account biases.)

This only reduced ipn by 7 and 13 for the two models, though.
So try again with 0.00001f. Now more significant:

model 0: 259 ipn to 259, 218, 201, 208.
model 1: 259 ipn to 259, 204, 183, 218.

The most common weight value seems to be zero or almost zero, still. I guess
the rectangularity constraint causes a lot of slack in this regard. It definitely
adds to the simplicity of the code, but maybe we'd benefit in performance from
relaxing that?

maybe should be exporting eps too, but looks like it went from about 300 to about 350.
we'd expect a linear speedup here so it makes sense. how will the error fare?
Seems like it continues the trend... not even any obvious jump at the moment of
vacuuming (which then suggests we could be more aggressive?)

At round 614486, tried turning off error decay completely.
Around this time, also increased the target rounds to 1M, which increases the
effective learning rate a little.

Did a really conservative widening at 639714 rounds.
Error shot way up... 200+, then dropping quickly to ~84.
I guess this is a pretty disruptive operation.

tried again with the next layer weights being 0.
this still caused the error to shoot way up (surprising?) but I'll
let it run over night anyway...
well, it starts converging at least, but error is back up to ~50,
so what is the point of that?


OK, tried again with input weights being 0.0 and next layer weights also 0.0.
But this also has an instant bump in error.
As I understand it, this should be the same network, so it should not have ...
OHHH, the indices on the next layer actually need to be permuted if I add
width, so that's just a bug.
Tried adding height which should not be affected by this bug. The first round
does indeed look the same, then error jumps up briefly, but it's quickly down
to around 40. I thought that nodes that always output zero (which these should,
with input weights of zero and bias of zero) will always stay zero? So this
should not have effect even after some training? It's possible I'm just
attributing variance to this change, but it's the same as it always was.
Kinda looks like a visible bump at this time, but a small one at least.

Looking at the activations themselves (just from screenshots in ui) these two
new rows look totally dead, as expected. There are also a lot of other nodes
that appear dead. Probably worth trying to do a "vacuum"-like process to
remove these dead nodes. Have to be careful about useful features that are
just rarely-activated, though.
  - Nodes whose input weights are all very low, and which have low bias, are
    heuristically dead. (Could also look at the weights by which the node
    is used, which tells you something about its downstream significance.)
  - Also can just run on lots of data. Could be random input data, or could
    be training examples. If the node's maximum activation is very low, it
    is empirically dead.

How to remove nodes when there is width/height etc.? We could immediately
replace them with new nodes (keeps geometry). Or we could just ditch the
dimensions and use Factorize to make a rectangle.


====== Random activations ======

Looking at maximum activations on random inputs. Both upper- and
lowercase networks have similar shapes. It makes sense that we would
see activations towards the center and not towards the edges on early
layers, given what typical inputs look like. I don't really understand
why the third layer has a dead stripe through the middle on both
networks, though. Is there some bug with the random index assignment
during intialization? That code is crazy, so it's possible.
Maybe it's because the output layer is like 661x2 plus some off-by-one error?
(I think there is indeed something wrong here .. look at the weights for
the last layer in before-vacuum).

also interesting to see that there are some letters that are never (strongly)
predicted. These differ between the two networks.

ACTUALLY, adding the checkerboard patterns both noticeably increased the brightness
of activations throughout, and made the letter activations in the output all light
up (but one). So perhaps the random stimulation approach is not a good one, because
it might have had us conclude that these features are dead. (OTOH the checkerboard
did seem to work!) It really is the checkerboards, too... I tried just all-on and
all-off and those do not produce the effect.


at 667777, culled the first layer with an activation threshold of 0.00001.
This deleted about 500 nodes and around 30 ipn.
... doesn't seem to hurt error instantaneously. great!
note that we might see a bump in error (but then hopefully improvement) since
the next layer has a chance to refer to nodes it wasn't depending on before.

eps is a little higher, at 350. would be nice to have a running average here...


at 687269, culled layer 1 with new rerandomize option turned on.
error is quickly sub-40.
at 687510, same for layer 2.
error looks fine. Kind of surprised to see that training rate is not
much higher after doing this, since the models are nearly half the size
they used to be...? Maybe this is actually good news though... it probably
means that there is some overhead in the training process that we can get rid of.


at 760857, use new 1d widen to add 5% nodes to first layer, add_index_rate = 0.15,
INITIAL_WEIGHT = 0.00000001f.

instantaneous error seems within the normal variance, which is expected because
the weights on the new nodes start at 0.

so at 761228 I just did the same to the next two hidden layers.
(also weirdly eps is UP after this change... 380+... 400... why??)

round 853977: more aggressive. added 10% nodes with .0001 weights to all layers.
instantaneous error seems lower? saw ~34
but I think it was probably a fluke. still doesn't really seem like these nodes
are activating?

nothing seems to be happening after a day of that, so at round 879696...
10% on each layer with input weights of 0.1!

ok here I at least see error jump to 47 from the start
... oh but then immediately back down to 37.. 35... maybe just a fluke?
the nodes are really activating, at least. this much is clear from
the training UI. lesse how it fares overnight!

over the long haul we're still seeing improvement, like 1 absolute error
in 120000 rounds. If it kept going linearly (which is not expected),
we'd get to 10 error at 950000 + 26 * 120000 = 3.1 M rounds, which is
not really insane given that I'm already at 1M... something like 20
days of training time. (Could even suggest that optimization is the
right place to spend the effort.)

Or maybe it's just that the training rate is too conservative now.

at round 954412, set target rounds to 3M. This changes the learning rate
from .002366 to 0.007455. and, well, it's obviously too much: error
shoots up >100, it does come down after a few dozen rounds, but to like 70,
which is terrible. makes ya kinda wonder if maybe the opposite thing
is what we need to be doing!

so, just undid that change, and fiddled with the start and end learn
rates (still targeting 3M rounds) to make the instantenous rate 0.001403.
verified that error looked good at the start. and, wow, I saw a round
with an error of 30 right away?
let's see if it picks up overnight.

(maybe the learning rates are just way too high?)
well, that was probably wishful thinking. at round 1,047,072 it is still
making progress (err around 36.4) with training rate of 0.001329, but
halving the training rate did not create some kinda miracle!


at 1.08M rounds, vacuumed (0.00001) and culled with (only) the new
unreferenced-node criterion, mostly to test that this doesn't mess up
the network due to some bugs. It seems fine.
This only affects the last layer. On other layers we've kept every node
alive because we insert nodes with nontrivial weights (and uniform
random indices) in the new widen operation. Probably we'd have to wait
a while for these to converge (or add decay).
Also did a regular culling (rerandomize, threshold = 0.00001f) of
each layer, but only ~5 nodes were removed per layer with this pass.

Idea: In the Network file format, include each node's age (round at which
it was born), which could be used to modulate individual training rates
or weight decay. But actually, we might even need to do that at the level
of weights (many operations introduce new weights), which seems like overkill.
This of course would make training slower too, maybe a lot so.


At 1.20M, added 100 features from makefeatures.exe to net0 (but not net1 yet).
Then 1.206M, same for net1.

Not much seems to be happening here overnight, so at 1278649, increased
training rate again (by doubling rate_high),


overnight, eps 283.5, averaged last 100 rounds. boinc is running
... later on that same day, 297.68, 307
(why does this change so much? should just be a constant workload? boinc?
number of export rounds that happen in the window?)


at 1.34M increased training rate again (rate high triple instead of double)
so it is now 0.003373

at 1.355M, double round batch size to 512, also increase eps rolling average
window to 1000. It does look like this helps eps a little, now 317?
REMEMBER that rounds are now twice as expensive, so the sdf error plots etc.
no longer have the same x scale

things to try:
  - add a layer at front or back
  - increase or decrease batch size

== adding a layer ==

I've tried widening layers, so far to no avail. Maybe deeper will help.

We can insert a hidden layer without changing the network's predictions.
Just make it the same size as the layer that precedes it, make sure each
node has its corresponding node as an input with weight 1.0, and a bias
of 0. Obviously we would want to be able to create new structures, so
we should also take random other inputs, and just set their weights to
zero.

it was instructive to watch the weights change with just ipn=1 (identity
matrix)... obviously there is not anything "real" to learn, but you can
get a sense of the magnitude per round.

soooo

at 1,369,777, added a copy of the final layer to both models.
Connected every node to the 26 letter outputs, and the 26 nodes before that.
Then also a square neighborhood of 25 pixels, then random. 256 ipn.
(this power-of-two superstition might have utility here, since we actually
compile the opencl programs with this as a constant!)


Maybe this is like a cpu power/heat scaling thing, but the eps are much
higher when I started (378!). So that's good? (OK back to around 300)

Seems like error increased a lot (now at 41) but not to like a worrisome
level.. I crave some movement here! YOLO! OK j/k, now seeing results
down near 34 again.

I seem to see this a lot... maybe it's just like because I'm watching,
but it could also be that early in training we haven't loaded enough
examples, and so there are too many repeats in the training sets? Or
the zero weights behave badly somehow (indeed the derivative is not
well-defined here), but once we get small nonzero weights it stabilizes?


at 1.399M, increase the training rate again: doubling it, by setting
rate_high to .012. My thinking is that I want to get this as high as
possible without runaway, since I feel like we're stuck in some local
minimum and the weights aren't changing much either. Also increased
ENOUGH_FONTS 10x to 1000, since I'm a little superstitious now that
early on we see too many repetitions due to the smallish pool, and
that repetitions may cause updates to be too large.
... and that was enough to instantly explode

failure mode is that all the weights end up pushed to -max or +max...
it's obvious from the layerweights images too.


I tried it again and it didn't blow up this time, although the error
has clearly gotten worse. Seems like it'd be basically starting over.
same for rate high = 0.009 (.0048 at current round)
tried .0075 (giving .0040).


let that run a day or two. Now at round 1.468M (remember these rounds
are twice as many examples now). Error is about as good as it's ever
been. Again it is instructive to look at the new last layer and how it
is starting to diverge from the identity matrix. In the histogram I
can see the huge cluster of 0 weights spreading out and becoming
gaussian-looking (seems right), with its max bucket (0.0007) moving
rightward. The tower of 1s is spreading out too (this peak is now
totally flat but the values are all very near 1).

Now some weights
are a visible dim green or red. The SDF pixels have significant weights
on the A-Z predictions on the previous layer; I guess this allows us to
have a different bias for each pixel based on what letter is predicted
(makes sense, seems useful). Similarly, some clear spatial relationships
within the SDF region, with both positive and negative influence from
nearby weights.

Overnight again, layer 4's weights continue their march. max bucket is
now at 0.0011. I think the meaningful thing here is not the max
bucket's position, since we'd probably expect that thing to be
centered around 0; I think the value there is just an artifact. But
meaningful is that the new weights that started at 0 spread out and
some have a chance of reaching the values of the old weights that
started at 1... We wouldn't expect it to have a chance to "converge"
before that happens. That time scale looks like ~weeks.

Training error over this period does look to be improving, if very
slowly. uppercase is just around 40, and lowercase around 35.2.



nonlinear error:
 - before this, set output error takes about 100ms, 4.4%
 - don't see a significant difference after, maybe 110ms?
 - everything looks normal.
 - I don't really perceive a difference in the visualization of the error
   values, but maybe that's not that surprising. Maybe would be worth looking
   at the same example side by side.

Note that the total error computation for UI and tsv export is actually based
on diffs done on the CPU, so it's unaffected by the remapping change. So error
values are comparable to before (although this change should actually be less
efficient wrt that calculation, right?). What we'd hope for is that the letter
shapes after thresholding are more accurate, which is something we only test
subjectively by looking at them.

Looks good overnight.


Should probably widen layer 3 now; it was never widened and it has the most
suspicious structure to begin with.

At 1.526M: (vacumming and culling did nothing)
widened layer 3 by 10%, 15% add index rate


things to try:
  - decrease batch size instead
  - lots of parallel smaller models (perhaps would be good use case for
    global optimization of hyperparameters too)
  - could try to visualize how much each node is contributing to overall
    accuracy with knockout? might be helpful for knowing whether the
    widened regions are "actually helping"


Just let this run for a while since I was preoccupied. At round 1.634M we
are still seeing slow improvement. And so too at 1.666. At 1.666, added
another 100 generated features, and also widened every layer by 10%. This
time rather than uniformly sampling the entire previous layer, I mostly
sample from the newly introduced nodes each time (on the second layer,
I densely sample the new generated features, in fact). Let's see whether
that does anything good.

... all seems well the next day. It seems to me that eps is not obviously
reduced by these widening operations; still around 250 eps here. I guess
that is lower than I saw right after adding a new layer.

Round 1767085: still making progress. I don't like these big dead
zones that were I think mistakes in the original input assignment. So,
added a new mode to cull.exe that can just remove the X% weakest nodes
(computing the threshold dynamically). Removed the weakest 5% from the
first three layers. Expect this to hurt the error a little but should
give us budget for some more useful widening. (If error doesn't
increase at all, should consider being more aggressive...)

culled 5% again; didn't seem to make any difference in error and
it's still slowly converging (round 1.847M).

tried a bit more aggressive culling and widening, but the error
got bad. it may have been the new CANCELING_BIAS param in widen.exe
which has some non-obvious consequence (maybe the bias can get
too big and the gradient explodes? maybe I just typo'd something
when running these operations?)


at round 1.927M, widened layer 3 again by 10%
