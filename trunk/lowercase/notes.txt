
lowercase 24 Dec 2020


on the second problem (smaller vector),
switched to a normalized point order at round 24820.

(this rapildy dropped the total err from like 17 to 12.79)

switched to permissive loss function at round 70808

(total error went up though... was like 18 and is now 51. seems wrong?)



-- sdf training started 1 Jan 2021  13:01:36 --

Got nans after about 40 rounds. Shrink to 1/4 of the examples per round
and halve the learning rate.

(23.6 eps per model with these new settings. I think it was like 40 before?)


nanned out again after 158 rounds each.
tried actually rejecting nan updates in updateweights.

22.29 eps, so the nan checks aren't THAT expensive...

made it to round 242 so far without any issues (just predicts "average" shape though)

since updateweights has protection now, putting the learning rate back up

... but then it immediately blew up (round 268)!


didn't touch it for a while but came back to about round 640.
I hit 'v' to look at some of the stimulations/errors and watched it diverge in front
of my eyes! Maybe the divergence is actually a bug caused by the UI??

Wow I'm stumped... the behavior of vlayer (which is the only key that does anything)
is really straightforward. Due to another bug it was exporting and saving on every
round, so I guess that increases the possibility of race conditions here. Added some checks
for in-boundsness. I guess technically, writing to 'dirty' with a Read lock held on
the shared-mutex is wrong, because the ExportBlahToVideo also writes to dirty, and
it's plausible that this actually has an effect since it's just a bool (conflicting
writes are probably touching other nearby values). No idea how that could lead to
the network's weights getting corrupted, though, except via general "undefined behavior".
Switched to writemutexlock to see if this ever happens again.

(Also saves a lot of time... 42 eps now)

ok nevermind, it just happened on its own after 547 rounds (each)
while unattended (and with those fixes). (It must have just been a coincidence??)


--------------------------------------------------
trying again!
New mode in updateweights (can be performance tuned) caps not the update but the
final weight value. Also not very principled. But this should make it quite hard
for it to ever produce nans, right?


even with a max weight of 8192 it quickly gets off the rails
trying again with 128!

Note that the output pedictions can be pretty big (especially early on) and that
these can then be inverted examples for the other model. This may actually be
the cause of crazy results.. so maybe we should normalize those SDFs before
inverting them (or reject them as training examples if they are too far out
of range)


even with updateweights capped to 128, we get divergence.
maybe the problem is really in the error step, so I tried clipping there
as well.

(also fixed another bug where network would not save unless you got lucky
when you stopped it, oops!)


even with a max weight of 8.0f (!) the stimulations get huge


ok then I tried again with initial weights using "He initialization". These
are bigger than what I was using before. It immediately diverged.


ran 6000 rounds over night.
these are under control at least, but making really conservative
prediction (just "average") and the error doesn't seem to be improving.
put them in sdfmodels as a good starting point?

"just predicts the average" could be what happens if we only update biases
and never weights, by the way


well, with those settings was still stalled with super low activations


okaaaaaay, trying again with fewer layers, all dense
(these have twice as many total weights though!)

31 eps, but, actually maxing out the GPU now
... aaand it blows up in 16 rounds


3 Jan 2021
Changed a few things again and now it's making progress (overnight, 7042 rounds)
I think I partly just got lucky because the lowercase model blew up,
but the uppercase one was still generating some nice average blobs, so I copied
the uppercase model oevr the lowercase one... in early stages it's fine for
them to have the same cold start (this could even be an explicit strategy in
the future to save some time training?)

 - used two hidden layers, but bigger ones (1x square, then 1.5x square, then output)
 - all dense layers
 - use sigmoid in output layer
 - "Tom initialization"
 - decay weights 0.9995f
 - 

9 Jan 2021

looking at the histograms on biases and weights, most weights are really close to zero,
on all of the layers. Both models evolved similar distributions.
Layer 0: -.05 to .069
Layer 1: -.29 to .53
Layer 2: -.04 to .025

this does suggest that the weight capping is unnecessary, at least in this epoch.

biases are also very spiky; layer 0 is close to zero, with some falloff towards the
negative side (kinda makes sense, this would be a feature that's looking for a
sum of input nodes above some threshold)
On layer 1, almost all biases are 0.0051
Layer 2 (sigmoid) has a nice histogram with lots of activity across the spectrum
from -1 to 0.5. Probably this is encoding the "average output".

At 63831 rounds, train error seems to be quite flat (around 230/example) so I'm
going to try spicing it up.

before vacuuming, we were getting around 37 eps (for each model).
on disk the model was 370,127,092 (note that dense models are more efficiently
stored)

I also increased the learning rate (it ended up at about .05). The result was
that the lowercase model seemed to diverge... it performed fine right after
vacuuming but then got bad after a few rounds of training? Again the lowercase
model is the one that exploded.
(Also, eps was similar at about 35.)


Did it again, same settings but effective training rate of .085; blew up right away.
Again with effective learning rate of 0.0319. Also blows up, at least at first?
This one looks like

A few hypotheses about this then:
 - maybe learning rates are way too high? It doesn't seem like we'd want this
   to jump around a lot once we get this deep into training.
 - Something about the inversion training examples encourages this behavior.
   A feedback loop, etc. But the sigmoid should prevent the output from
   being outside [0,1] and we think that the predictions were actually good
   in the first few rounds here.
 - Just some kind of actual bug in training?? We have seen it work
   (for example the vector version is clearly doing *something*), but there
   could still be bugs.
 - (maybe vaccuming messes up the network? but we check the inverted indices...)

It's really weird that the network actually performs well at first, but then
gets much worse error after a few rounds of training. How is that possible?

well, the lowercase side seemed to smooth out (error = 339) but the
uppercase side has a burned-in superbright region, and the eval is
bad (and looks basically constant) on both.


--------------------------------------------------


sparse model... weights go to the max (+/- clamped) by 7 rounds (maybe immediately)!
maybe we should put these histos in the training ui



after all the futzing, the thing that seemed to get it on track was to
set the learning rate (high end) to 0.01f.>



at 196961, changed decay to .999995 (from .9995) since so many weights
were almost zero.

(total error started dropping really quickly after that!)
... seems like a very worthy change. Error dropped from ~70 to ~50 after a day,
even though the learning rate has nearly bottomed out. The eval results
are almost interesting now! It's really obvious from the error history that
this got it out of the plateau.

I see a lot of weights with values at, or nearly at zero though. Seems
like we may be wasting our parameter budget then. Should think about some
kind of vacuuming + expansion?

vacuumed at round 599041, all layers with a threshold of 0.0000001.
(I got this threshold from inspecting histos and intended to set it really
conservatively. One way we could think about setting this is that weights
are always in -8,8 (when clipping is on), so if you have indices_per_node = ipn,
then the max value a relu node could take on is ipn * 8. So something like
1 / (ipn * 8) is something like the threshold in which any node is meaningful,
because if all nodes were outputting the same signal at that magnitude, it would
register a '1' in that case. Also would need to take into account biases.)

This only reduced ipn by 7 and 13 for the two models, though.
So try again with 0.00001f. Now more significant:

model 0: 259 ipn to 259, 218, 201, 208.
model 1: 259 ipn to 259, 204, 183, 218.

The most common weight value seems to be zero or almost zero, still. I guess
the rectangularity constraint causes a lot of slack in this regard. It definitely
adds to the simplicity of the code, but maybe we'd benefit in performance from
relaxing that?

maybe should be exporting eps too, but looks like it went from about 300 to about 350.
we'd expect a linear speedup here so it makes sense. how will the error fare?
Seems like it continues the trend... not even any obvious jump at the moment of
vacuuming (which then suggests we could be more aggressive?)

At round 614486, tried turning off error decay completely.
Around this time, also increased the target rounds to 1M, which increases the
effective learning rate a little.

Did a really conservative widening at 639714 rounds.
Error shot way up... 200+, then dropping quickly to ~84.
I guess this is a pretty disruptive operation.

tried again with the next layer weights being 0.
this still caused the error to shoot way up (surprising?) but I'll
let it run over night anyway...
well, it starts converging at least, but error is back up to ~50,
so what is the point of that?


OK, tried again with input weights being 0.0 and next layer weights also 0.0.
But this also has an instant bump in error.
As I understand it, this should be the same network, so it should not have ...
OHHH, the indices on the next layer actually need to be permuted if I add
width, so that's just a bug.
Tried adding height which should not be affected by this bug. The first round
does indeed look the same, then error jumps up briefly, but it's quickly down
to around 40. I thought that nodes that always output zero (which these should,
with input weights of zero and bias of zero) will always stay zero? So this
should not have effect even after some training? It's possible I'm just
attributing variance to this change, but it's the same as it always was.
Kinda looks like a visible bump at this time, but a small one at least.

Looking at the activations themselves (just from screenshots in ui) these two
new rows look totally dead, as expected. There are also a lot of other nodes
that appear dead. Probably worth trying to do a "vacuum"-like process to
remove these dead nodes. Have to be careful about useful features that are
just rarely-activated, though.
  - Nodes whose input weights are all very low, and which have low bias, are
    heuristically dead. (Could also look at the weights by which the node
    is used, which tells you something about its downstream significance.)
  - Also can just run on lots of data. Could be random input data, or could
    be training examples. If the node's maximum activation is very low, it
    is empirically dead.

How to remove nodes when there is width/height etc.? We could immediately
replace them with new nodes (keeps geometry). Or we could just ditch the
dimensions and use Factorize to make a rectangle.


====== Random activations ======

Looking at maximum activations on random inputs. Both upper- and
lowercase networks have similar shapes. It makes sense that we would
see activations towards the center and not towards the edges on early
layers, given what typical inputs look like. I don't really understand
why the third layer has a dead stripe through the middle on both
networks, though. Is there some bug with the random index assignment
during intialization? That code is crazy, so it's possible.
Maybe it's because the output layer is like 661x2 plus some off-by-one error?
(I think there is indeed something wrong here .. look at the weights for
the last layer in before-vacuum).

also interesting to see that there are some letters that are never (strongly)
predicted. These differ between the two networks.

ACTUALLY, adding the checkerboard patterns both noticeably increased the brightness
of activations throughout, and made the letter activations in the output all light
up (but one). So perhaps the random stimulation approach is not a good one, because
it might have had us conclude that these features are dead. (OTOH the checkerboard
did seem to work!) It really is the checkerboards, too... I tried just all-on and
all-off and those do not produce the effect.


at 667777, culled the first layer with an activation threshold of 0.00001.
This deleted about 500 nodes and around 30 ipn.
... doesn't seem to hurt error instantaneously. great!
note that we might see a bump in error (but then hopefully improvement) since
the next layer has a chance to refer to nodes it wasn't depending on before.

eps is a little higher, at 350. would be nice to have a running average here...


at 687269, culled layer 1 with new rerandomize option turned on.
error is quickly sub-40.
at 687510, same for layer 2.
error looks fine. Kind of surprised to see that training rate is not
much higher after doing this, since the models are nearly half the size
they used to be...? Maybe this is actually good news though... it probably
means that there is some overhead in the training process that we can get rid of.


at 760857, use new 1d widen to add 5% nodes to first layer, add_index_rate = 0.15,
INITIAL_WEIGHT = 0.00000001f.

instantaneous error seems within the normal variance, which is expected because
the weights on the new nodes start at 0.

so at 761228 I just did the same to the next two hidden layers.
(also weirdly eps is UP after this change... 380+... 400... why??)

round 853977: more aggressive. added 10% nodes with .0001 weights to all layers.
instantaneous error seems lower? saw ~34
but I think it was probably a fluke. still doesn't really seem like these nodes
are activating?

nothing seems to be happening after a day of that, so at round 879696...
10% on each layer with input weights of 0.1!

ok here I at least see error jump to 47 from the start
... oh but then immediately back down to 37.. 35... maybe just a fluke?
the nodes are really activating, at least. this much is clear from
the training UI. lesse how it fares overnight!

over the long haul we're still seeing improvement, like 1 absolute error
in 120000 rounds. If it kept going linearly (which is not expected),
we'd get to 10 error at 950000 + 26 * 120000 = 3.1 M rounds, which is
not really insane given that I'm already at 1M... something like 20
days of training time. (Could even suggest that optimization is the
right place to spend the effort.)

Or maybe it's just that the training rate is too conservative now.

at round 954412, set target rounds to 3M. This changes the learning rate
from .002366 to 0.007455. and, well, it's obviously too much: error
shoots up >100, it does come down after a few dozen rounds, but to like 70,
which is terrible. makes ya kinda wonder if maybe the opposite thing
is what we need to be doing!

so, just undid that change, and fiddled with the start and end learn
rates (still targeting 3M rounds) to make the instantenous rate 0.001403.
verified that error looked good at the start. and, wow, I saw a round
with an error of 30 right away?
let's see if it picks up overnight.

(maybe the learning rates are just way too high?)
well, that was probably wishful thinking. at round 1,047,072 it is still
making progress (err around 36.4) with training rate of 0.001329, but
halving the training rate did not create some kinda miracle!


at 1.08M rounds, vacuumed (0.00001) and culled with (only) the new
unreferenced-node criterion, mostly to test that this doesn't mess up
the network due to some bugs. It seems fine. 
This only affects the last layer. On other layers we've kept every node
alive because we insert nodes with nontrivial weights (and uniform
random indices) in the new widen operation. Probably we'd have to wait
a while for these to converge (or add decay).
Also did a regular culling (rerandomize, threshold = 0.00001f) of
each layer, but only ~5 nodes were removed per layer with this pass.

Idea: In the Network file format, include each node's age (round at which
it was born), which could be used to modulate individual training rates
or weight decay. But actually, we might even need to do that at the level
of weights (many operations introduce new weights), which seems like overkill.
This of course would make training slower too, maybe a lot so.


At 1.20M, added 100 features from makefeatures.exe to net0 (but not net1 yet).
Then 1.206M, same for net1.
