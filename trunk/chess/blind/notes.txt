Initializing the network and learning rate is a dark art!
For several of my initial attempts, the network would just predict the same
thing apparently forever.


-- for these, weights and biases were initialized to .000025 * (rand() - 0.5f). --

I got it to do something useful with a single intermediate layer,
16x16, fully connected. Examples per round was a paltry 12, so it was
doing like 30+ rounds per second (and GPU utilization was only like
50%). The learning rate ended up being 0.95 (!) and it still takes 3k
rounds to get somewhere. The symptom is that the intermediate layer is
all very close to 0.5f activation, so I guess that starting with
really small random weights means that these nodes are not very
differentiated.


Increasing it to 3 channels on the intermediate layer, it was stuck
even after 30k rounds. Then I let the learning rate go up to 2.82, and
it still stayed in the grey zone. After 50k rounds it was still stuck.


Back to 1 channel, but unlimited learning rate; got stuck in 15k.

1 channel, 0.95 (same as first success): After 6k rounds it is doing
something.

With 10x10 intermediate layer (still fully connected), it's "working"
pretty much right away (100 rounds).

With 32x32 intermediate layer, still stuck after 14k rounds.

With 32x32 intermediate layer, but only 100 connections for the last
layer, immediately starts to do something.

So maybe what happens is that we sum so many input edges that the
sigmoids are totally saturated, so the derivative is nearly flat and
error propagation does very little even if the learning rate is high.
(This sounds like it's a common problem: See wikipedia on the
"Vanishing gradient problem".) According to one rando on the internet,
"there is no vanishing gradient problem for biases. they can be
safely initialized to 0."

Then I did 6 internal layers, each 32x32 with 64 incoming edges.
This took a lot longer to fire up (top two layers were grey for a
while) but by 30k rounds it is at least not grey on each layer.
But at 40k rounds it was stuck predicting the same thing always.


ok, same network, but using leaky_relu for all but the last layer.
after 30k rounds (64 examples per round) it just seems stuck...


Started drawing/printing the updates ("errors" = error * derivative).
They are indeed extremely close to zero, even after a few rounds.

Temporarily switched to leaky_relu everywhere. This yielded a different
problem, which was NaNs after just 3 rounds. What was happening was
that the biases (also weights) were getting crazy big ("gradient
explosion"?); I guess the updates can compound exponentially per layer
if there are lots of large ones. I did two things to fix:
  1. cap the update to be in [-1, 1] within updateweights.cl
  2. Set the learning rate to depend on the number of examples per
     round. Without this, the errors can easily be very large
     (e.g. if there are 100 examples, then it is common to have
     an error of 100!). 
(1) didn't help on its own, so maybe it should just be removed; it
makes the slowest step slower!
(2) Did make a huge difference; with just that change a two-hidden-layer
fully connected network of just leaky_relu is producing reasonable
outputs. (After 1200 rounds, total error = 18)

Phew!

 with 3 hidden relu layers, after 1300 rounds (64 ex per round), ~18 error
 at 2800 rounds, ~15.3 error
 at 17k rounds, ~13 error (note learning rate has dropped to 0.0020)
 so at 17900 I switched to a linear decline from 0.1 to 0.002 with a target
 of 200,000 rounds.
 at 31164 rounds, 13.74 error
 50k rounds, 13-14 error
 64k rounds, 12 error
 82k rounds, 12 error
 96k rounds, 11.5-13 error
 118k rounds, 10.9-12.5 error
 141k rounds, 10.6-12.8
 
Good writeup with some actual parameters used (although for a pretty different
problem):
https://engineering.flipboard.com/2015/05/scaling-convnets

TODO:
 - might want historic graph of total error over time?


After 64k rounds, it does seem to be making slow progress still, but
it has some common mistakes, like it loves putting multiple kings on
the board. Is it possible that

 haha omg, it's just that I was printing both knights and kings as K. :)

 wow much better.

Still, there are cases where it does silly stuff like more than 8
pawns of one color, or two same-color bishops. I don't see rampant
multiple kings any more but nothing prevents it. Actually it does seem
fairly common for there to be no kings, or for one side to be missing
a king. Could we help it produce the right features by structuring the
error more? Like we could ask it to produce the single position for
each king, or the locations of each of the 32 pieces, or something?
32 * 65 bits (one for "dead") is 2080, which is reasonable. Note that
this does not supersede the current board represenation, since it would
not be able to tell us about promotions without additional data.

Some hard constraints:
 - If all of one side's pawns are on the board,
    - No more than 2 rooks, 2 bishops (different colors), 2 knights, 1 queen
 - Exactly one king on each side
 - If check, then this determines the move
 - Both sides can't both be in check
 - No pawns in the extremal rows

There are many soft heuristics, too. Three knights in the early game
is just exceptionally rare, even if technically possible through promotion.

It also seems pretty reluctant to put white pieces in the black ranks and
vice versa. Makes sense, but this will be a weakness when used to play!


New offline bench, 126k rounds:
In 50000 positions:
  10482 exactly correct (20.96%)
  156987 piece mistakes (3.14/pos)
  1552 castling mistakes (0.03/pos)
  18623 move mistakes (0.37/pos)
  177162 total mistakes (3.54/pos)

141k rounds:
In 50000 positions:
  10579 exactly correct (21.16%)
  156156 piece mistakes (3.12/pos)
  1544 castling mistakes (0.03/pos)
  18606 move mistakes (0.37/pos)
  176306 total mistakes (3.53/pos)

...
In 50000 positions:
  10377 exactly correct (20.75%)
  156376 piece mistakes (3.13/pos)
  1556 castling mistakes (0.03/pos)
  19242 move mistakes (0.38/pos)
  177174 total mistakes (3.54/pos)


-- vacuuming --
Training speed before: ~27.6 eps
Very aggressive threshold of 0.01f.
Model size on disk:  160781584
After:  122444104

Layer 0: Unpack 1024 nodes / 64 indices per node
Layer 0: Truncate to threshold 0.010000.
Layer 0: Node 22 has a weight of -0.010150 in column 63; done.
Layer 0: Dropping 0 of 64 indices.
Layer 1: Unpack 12288 nodes / 1024 indices per node
Layer 1: Truncate to threshold 0.010000.
Layer 1: Node 4122 has a weight of -0.010077 in column 786; done.
Layer 1: Dropping 237 of 1024 indices.
Layer 2: Unpack 567 nodes / 12288 indices per node
Layer 2: Truncate to threshold 0.010000.
Layer 2: Node 530 has a weight of 0.010002 in column 9091; done.
Layer 2: Dropping 3196 of 12288 indices.
Layer 3: Unpack 837 nodes / 567 indices per node
Layer 3: Truncate to threshold 0.010000.
Layer 3: Node 655 has a weight of -0.010260 in column 485; done.
Layer 3: Dropping 81 of 567 indices.


Before In 50000 positions:
  10601 exactly correct (21.20%)
  156052 piece mistakes (3.12/pos)
  1542 castling mistakes (0.03/pos)
  18818 move mistakes (0.38/pos)
  176412 total mistakes (3.53/pos)

real    3m56.358s

After In 50000 positions:
  10555 exactly correct (21.11%)
  156213 piece mistakes (3.12/pos)
  1545 castling mistakes (0.03/pos)
  18892 move mistakes (0.38/pos)
  176650 total mistakes (3.53/pos)

real    3m53.845s
user    0m0.000s
sys     0m0.000s

This is very slightly worse... tolerable!

eps after: 27.52 (actually slightly slower?! how?)
Maybe the big layers are not actually the bottleneck
because we get lots of parallelism?


with a threshold of 0.1 (really aggressive!)
network is only 5mb now, so that's something...
475 eps!
but the performance is so bad :(

 - it got somewhat better after running a while
 
In 50000 positions:
  8239 exactly correct (16.48%)
  167477 piece mistakes (3.35/pos)
  1666 castling mistakes (0.03/pos)
  19713 move mistakes (0.39/pos)
  188856 total mistakes (3.78/pos)

24s runtime

 - increased learning rate and examples per round a lot:

with a higher ex/round, 527 eps!
... error is already down to 12
then overnight,

ModelInfo [237767 rounds, 31777088 examples]
Over 50000 positions:
  8834 exactly correct (17.67%)
  164732 piece mistakes (3.29/pos)
  1649 castling mistakes (0.03/pos)
  19391 move mistakes (0.39/pos)
  185772 total mistakes (3.72/pos)

and through the day,
ModelInfo [253767 rounds, 48161088 examples]
Over 50000 positions:
  9118 exactly correct (18.24%)
  163616 piece mistakes (3.27/pos)
  1643 castling mistakes (0.03/pos)
  19254 move mistakes (0.39/pos)
  184513 total mistakes (3.69/pos)

... so it's still improving substantially.


also, gpu-z reports pretty low GPU usage with this setup. Probably
worth looking into this. Maybe we could run multiple training examples
with the same kernel invocation. (At least, check out the timers..)

another evening:
ModelInfo [267022 rounds, 74000704 examples]
Over 50000 positions:
  9231 exactly correct (18.46%)
  162890 piece mistakes (3.26/pos)
  1630 castling mistakes (0.03/pos)
  19166 move mistakes (0.38/pos)
  183686 total mistakes (3.67/pos)

ModelInfo [276522 rounds, 93456704 examples]
Over 50000 positions:
  9316 exactly correct (18.63%)
  162603 piece mistakes (3.25/pos)
  1635 castling mistakes (0.03/pos)
  19141 move mistakes (0.38/pos)
  183379 total mistakes (3.67/pos)

Still improvin':

ModelInfo [287754 rounds, 116459840 examples]
Over 50000 positions:
  9406 exactly correct (18.81%)
  162111 piece mistakes (3.24/pos)
  1623 castling mistakes (0.03/pos)
  19079 move mistakes (0.38/pos)
  182813 total mistakes (3.66/pos)

ModelInfo [295504 rounds, 132331840 examples]
Over 50000 positions:
  9419 exactly correct (18.84%)
  162002 piece mistakes (3.24/pos)
  1631 castling mistakes (0.03/pos)
  19049 move mistakes (0.38/pos)
  182682 total mistakes (3.65/pos)

so, getting slightly better, but maybe has plateaued?
