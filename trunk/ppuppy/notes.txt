Stuff still to figure out / achieve:

 [X] Read a 5V output (e.g. ppu /rd)
 [X] Have data output depend on address read (low order bits)
 [ ] What are the PPU writes? Do I need to handle them at all?
 [ ] How often can I change sprite ram?
 [ ] Can I prevent linux from descheduling me, except in vblank?
 [ ] Enough time to render SNES frame in vblank?
 [ ] Is it actually possible to do anything?
 [ ] Boot up without having to SSH into it. (Takes several seconds to boot...)

Backup plans:
 [ ] Second pi renders SNES frames, and just sends them to "video" pi
     during its vblank (wifi??)
 [ ] Kernel module
 [ ] Separate fast GPIO microprocessor renders the frame?

This doc rules:
https://www.raspberrypi.org/app/uploads/2012/02/BCM2835-ARM-Peripherals.pdf
addendum: https://matt.ucc.asn.au/mirror/electron/GPIO-Pads-Control2.pdf

 -- timing -- 

NES:
Measured pin #38 (M2 -- this is CPU) on the saleae.
Example pulse: .36 us high .198 us low, which is 1.792 MHz.
           ie.  360 ns,    198 ns
On the pi, a for(;;) loop just calling gpio_write (one pin)
gets 80-88 ns pulse times

this is say .09 microseconds, which means we can do about
4 bit flips per NES clock. this is cutting it a bit close
but maybe in the "not insane" range?
  danger:
    - can I set multiple bits at that same rate?
       at a minimum I need to read the address bus, then
       write the data bus, then wait for the falling edge.
    - should make sure that transistors/diodes to do
      bridge between 3v3 and 5v ttl don't introduce too
      much slew etc. The analog waveform at 50MHz is 
      basically a sine wave so we may be reaching some
      electrical limits (?)
    - there are periodic dropouts. How to set realtime
      mode or whatever? We could probably cooperatively 
      schedule during ppu vblank
    - do we have enough time left to prepare CHR data?
    - do we have enough time left to do some emulation, etc.?

hmm, write_multi on 4 bits gives me 80ns/200ns off/on
cycle with a grosser waveform. 

But it looks like the ARM is just executing too many
instructions. With -O2, and removing debug test from
peri_write, I get 48ns/112ns off/on and a sinusoidal
wave. Plenty of more optimization to do...

 - inlined peri_write (this version also actually packs
   bits into the pins written, so that it's not trivial): 52ns/124
 - inlined all routines necessary to do write_mask: 44ns/120
 - memory barrier before/after: 48/80
 - barrier just before: 56/100

 -- more NES timing: PPU --

- The PPU RD bit happens as a solid square, .18 ON, .192 off.
  (but see "gap" below)
- This square wave happens for 15.305312ms.
- Then RD stays high for 1.334072ms.
  Together these add up to a period of 16.634, which is very
  close to 1/59.94, i.e. "60" FPS. So when RD is high we are
  probably in vblank. If we could manage to schedule other
  tasks here and fill the framebuffer on the pi, we'd be
  golden?
  (during the vblank, addresses are usually stable but
   sometimes there are glitches. trust RD.)
- Looks like addresses change when PPU RD is *high*, by
  the way, meaning that the address bits are safe to
  read when it's *low*. (This concurs with the docs,
  which call it /RD)
- Gap: I see a gap in reads (high cycle for .368 us) every 63.5us.
- 15305.312u (full frame) / 63.5us = 241 almost exactly,
  which I think means that this double-read is the end of
  a scanline.

 -- switching d0 --

This is not working right.

I have a 4.7k pullup from 5v to DATA.
- DATA is on the Collector of the NPN transistor. [ch 2]
- PIN 26 is on the Base of the transistor [ch 0], after a 10k resistor
- GND is on the Emitter.

We should be able to pull this close to ground (+ transistor drop) by closing
the transistor's circuit. But:

    ch 0              ch 2
 26 set = 3.3v       about 3.8v
 26 clear = 0.0v     about 4.9v

so it's like maybe I'm only turning on the voltage drop?
AHA! Perhaps the transistor is simply backwards [laughing crying emoji]
because the pinout on TO-92 IS NOT STANDARD!

fixed! now:

    ch 0              ch 2
 26 clear = 0.0v    about 4.9v
 26 set = 3.3v      about .03v


 -- how the NES works --

74LS139 (CPU side) is a simple multiplexer (2 bit address in, selecting
which of the four outputs gets LOW logic level; others are high). (Actually
it has two such circuits.) This is hooked up to CPU A13, A14, A15.
(I think it is selecting between sram, etc.?)
I think that on the schematic, the "E"nable pin is labeled C. Note how
one of the Y outputs from the left side is wired into the right half.
This means that neither Y1 (ppu reg) nor Y0 (cpu side SRAM)
will be enabled (pulled low) unless the right half is enabled by the
left half. The left half is always enabled (C to GND) and is selected
by address bits A15 and M2. Together the left half controls whether
the right half is enabled, plus outputs to pin 50 (on cartridge),
which is /ROMSEL. So the CPU can signal the A15 bit to the cartridge
with every address, clocked with M2. Not sure what this is used for
exactly (it already gets A0-A14 directly, not to mention M2).

 -- cpu/ppu communication --

To write to PPU VRAM, you use memory mapped cpu registers;
two bytes to $2006 to set the address, then bytes written to
$2007 get streamed into incrementing addresses. (It's also possible
to increment by 32, which moves "down" since the screen is 32 tiles
wide.)

On the board, cpu has 3 address bits wired to the CPU to select the
PPU register (there are 8 of them), and an 8-bit bidirectional bus.
/CS is pulled low by that 74LS139 address multiplexer (not sure
in what conditions exactly) so that $2007 selects register 0b111.
Apparently the condition is that the address is from $2000 to $3fff;
the middle bits are just dropped. 

specifically, the address is:
001* **** **** *ppp
where * is ignored, ppp selects the ppu register, and 001 is setting
the range $2000-$3fff. The 1 here is A13. If you look at how the 74139
is wired, this makes sense, since the right half has A13 and A14 as
inputs (and is only enabled when A15 is low), and PPU /DBE as one of
its outputs.

 -- ppu memory mapping --

On the PPU side, this works differently:
The 2k SRAM chip (officially CIRAM, aka VRAM) is
enabled directly from the cart. Pin 57, aka CIRAM /CE, enables the
chip when low. The PPU RD and PPU WR lines are wired directly to CIRAM,
also connected to the cart traces.
A10, the highest bit of the chip's address space, is controlled by
the cart--not hooked directly into PPU. (I guess probably some carts just
wire this directly from PPU A10 though?)

Anyway, ppuppy should disable CIRAM (wire /CE high) so that it can
just supply its own data.

 -- wiring notes --

On zelda cart, PPU /WR is wired to /WE (write enable) on the SONY chip
on the top right, which looks like SRAM?
(http://www.datasheets360.com/pdf/4386973877628639512). I had
previously assumed this chip was PPU CHR ROM. It's 8k, so it would be
enough. But it makes no sense that it would ship in SRAM and lose all
the tiles as soon as the battery dies. But maybe this is not the
battery-backed SRAM.

Chip at top left is RP231024D, a ROM. It's labeled PRG. Could verify
that it's hooked up to CPU bus.

Chip at the bottom left is LH5164D, Sharp, which also appears to be
SRAM:
http://pdf1.alldatasheet.com/datasheet-pdf/view/42972/SHARP/LH5164A.html
(Maybe this is the battery-backed one.) Also 8k.

Anyway it turns out that zelda has a fairly complicated mapper, the
MMC1. I guess perhaps the chr memory is SRAM and gets filled by CPU,
reading from PRG ROM?. That's weird because I recall the zelda pattern
tables being very stable (only switching when entering the dungeon).
Oh well, maybe this is not a great test game...


So Ice Hockey is easier to understand. Both CHR and PRG are ROM and
the only other chip is CIC. The CIRAM is hooked up as follows:

 - /CE directly wired to PPU /A13. So CIRAM will be returning data
   (for reads) whenever the address is < 0x1FFF (because then bit 13
   will be zero and these are zero-enable pins). A13 is the highest
   address bit. When the address is 0x2000-0x3FFF, A13 is high, so the
   chip is disabled, and ROM can assert its values.
 - Note PPU A13 is also passed (not negated) to the cart. I bet this
   is wired to /CE for CHR ROM. (CHECK). That would allow these two
   chips to be mutually exclusive.
 - /OE directly wired to PPU /RD. This is done on the NES motherboard.
   So CIRAM will only output when PPU is reading, duh.
 - /WE directly wired to PPU /WE; same idea. Not much to do with this.
   If we really want to avoid writes to CIRAM, a simpler thing would 
   be to just disable the chip with /CE, which we control.
 - CIRAM A10 controls mirroring.
   https://wiki.nesdev.com/w/index.php/Mirroring
   On NROM (e.g. Ice Hockey) it is hard-wired to either PPU A10 or A11.
   (Ice hockey uses Vertical mirroring, where CIRAM A10=PPU A10.) This
   is just bit logic so that reading the nametable during scrolls
   "wraps" the way you want. Should be irrelevant for ppuppy since we
   won't scroll. Can probably leave unconnected, like Gotcha! game does.

So I think the next experiment to run is to read PPU A13 (or /A13)
and disable reads when A13 is low. This would properly mimic CHR ROM,
avoiding bus conflicts with CIRAM.

 -- ppu rendering --
Nice simple explanation from nesdev wiki (PPU nametables article):


Conceptually, the PPU does this 33 times for each scanline:

A. Fetch a nametable entry from $2000-$2FBF.
B. Fetch the corresponding attribute table entry from $23C0-$2FFF and
   increment the current VRAM address within the same row.
C. Fetch the low-order byte of an 8x1 pixel sliver of pattern table from $0000-$0FF7 or $1000-$1FF7.
D. Fetch the high-order byte of this sliver from an address 8 bytes higher.
E. Turn the attribute data and the pattern table data into palette
   indices, and combine them with data from sprite data using priority.

(Note: At the beginning of each scanline, the data for the first two
tiles is already loaded into the shift registers (and ready to be
rendered), so the first tile that gets fetched is Tile 3.)

It also does a fetch of a 34th (nametable, attribute, pattern) tuple
that is never used, but some mappers rely on this fetch for timing
purposes.

(Actually it is more complicated than this, though.
 Good detailed doc: https://wiki.nesdev.com/w/index.php/PPU_rendering
 There are 340 PPU cycles in a normal scanline, and each read takes two
 cycles. So we expect to see about 170 * 240 reads per frame, which is
 40,800.)

Note nametable would usually be RAM. We could disable this and supply our own
values, though. (This is probably why we see data on the data bus D0
all throughout the scanline, even when we aren't outputting anything.
This is not a vram write, it's SRAM asserting a value on that same bus.)

So, if for every tile it fetches the attribute table entry, it would
be possible to change the palette on every tile, not just every 4x4
block of tiles, right? AND! We could actually change it for each row
of the tile; basically each 8x1 horizontal stripe has to be from the
same palette entry. The major limitation here is just that we only get
2 bits to select the palette, so the screen only has 13 different
colors (background + 3 non-background colors per palette entry). BUT,
it should be possible to flip the palette on alternating frames,
blending colors to get 13*13=169 colors. That's a lot of flexibility.
Changing the palette dynamically would require cooperation from the
CPU (and needs to be done during vblank), so probably the best thing
to do is to pick two constant palettes that mix well to cover the RGB
space, and always alternate between the two. 16 colors is also not so
bad. The additional constraints that 8 consecutive pixels need to use
the same palette definitely adds some complication. Two fun color
problems:

1. Write a function that takes two palettes and 8x1 pixels, and
determines the two two-bit 8x1 stripes that minimize the color loss.
Probably the best results are global (like it's bad if a big solid
color has the color expressed different ways within it). Another thing
this could do is take an accumulated error as input, like
floyd-steinberg dithering.

2. Write a function that finds two optimal palettes for a given
weighted set of input colors. For any palette, the attainable colors
are the averages (and there may be some NTSC theory to this
averaging?) of any pair of colors (including a color and itself). The
loss is the (weighted) distance (like delta-e in LAB space) of all
input colors to their closest attainble color. You could compute
palettes for a specific image you want to display, or just do this
offline for the whole RGB cube and have a constant pair of palettes.

 -- syncing --
We expect to see about 40,800 reads per frame. at r3476, with swapping
disabled and running sudo, I do seem to get about this many reads:
15394215 edge, 376 frames, 40848 last sync, 7596268 839325 6134896 823726.
15435103 edge, 377 frames, 40888 last sync, 7616454 841532 6151186 825931.
15475850 edge, 378 frames, 40747 last sync, 7636578 843709 61
... being under the number is totally unsurprising (missed deadlines)
and we can be over from glitches, or the 40800 estimate might just be
wrong. So actually this seems pretty decent.

 why do I get MORE pulses (on other data pins) when I write 'sync' to output than
 when I write (sync & 1) ? 255 : 0??!
 
 -- more syncing --

Now on ice hockey. It seems that the writes are just taking too long.
Only writing when A13 is high, after PPU /RD goes low, DATA goes high
after 0.156us (A). That is fast enough to make the rising edge of RD.
Then from the rising edge of RD, it takes 0.164us (B) to turn off the
data. During that time RD went low again. We apparently do see that,
because another 0.164 us (C) later, the write starts. Now again, 0.164
(D) to turn off the data. Again we're into the next read-low pulse,
and A13 is off, so we are conflicting with CIRAM.

posterity/timing-late.png

(Actually: *IS* this bad? I am asserting the correct value on the
rising edge, even if I fail to shut off before the next cycle. When is
the critical moment? Maybe the timings are okay but just the
levels/impedance is bad? Could look at timings on other data pins to
see what they do.)

All these transitions take about the same amount of time, but the
reaction to RD going high comes late, because the width of the RD high
pulse is just shorter (.136us) than the low pulse .236. The fact that the
timings are usually the same suggests that there's just some latency
between setting a GPIO pin (or perhaps, reading it) and seeing the
logic level reflected on the output. 

Before we had a write time of <0.090 though. If we can get back to
that, we'd probably be okay timing-wise?

Another option is to just try to sync an internal clock so that we
predict the edges rather than reacting to them. It would be good to
be able to read the address that the PPU is asking for, but OTOH
they should be totally predictable. We could still use the addresses
to adjust the sync, as well as to communicate to PPU from CPU, we
just wouldn't be able to have the NEXT write depend on them.

So:
 - Study the timing of a real NES game.
 - Try to get the read or write latency down. (For example, study
   whether this is about making the C code fast, or if it's just
   inherent somehow to the BCM chip?) Would it be faster as a
   kernel module? Is there low-level support for interrupt on
   falling edge, for example? Can we clock the pi faster?
 - Try rewriting the loop as a predictive sync rather than reacting
   to logic level changes. Kind of a neat problem anyway. The signal
   is very regular, and addresses could give us absolute sync.
   Note that this may be hard to reason about if we have both read
   and write latency (could just assume they are symmetric values?)
   Also note that eventually, we won't have CIRAM storing the nametable.
   (Well, maybe we could have some discrete logic that only maps the
   attribute table to "ROM"--it's the last 64 bytes of each nametable,
   and we could rely on there only being one--but this may be kinda
   messy?)
 - Try disabling CIRAM -- we don't want it anyway -- and just
   generating all ppu data. We'll certainly still have a hard timing
   problem (and now the addresses will be gibberish?) but less confusion
   about bus conflicts. (Of course, games certainly still write to
   CIRAM and we would have the potential to conflict with those writes,
   but at least the corrupted values would likely have no effect?)

   Actually j/k: With CIRAM unhooked, we still get addresses: The
   nametable and attribute table reads should be from predictable
   (more or less sequential) locations. The pattern table (CHR) reads
   (which would have reflected the contents of CIRAM) are the ones
   that would be affected. These reads will directly reflect (well
   there is some simple function) what we returned from nameable
   "RAM", so we could actually use these to check that we're in sync!
   It would look like this:

   PPU reads nametable 0000 (top left corner).
   We return, say, 0000.
   PPU reads attribute table for 0000 too.
   We consult our current sync location, and return the attribute
   bits for that group of 8 pixels.
   Now PPU reads from chr+0000 (because we returned 0000 for the first
   read). We record the address, but just return whatever graphic bits
   we want at the current sync location. It then reads chr+0000+8 for
   the second color bit, and we do the same.

   But now: We can look at the addresses read the second time. We
   controlled the values (modulo some predictable offset), and can
   compare these to what we expect to make sure we're synced up. For
   example, if we expected to get back YYYY but instead got YYYY+1,
   then we adjust our sync position accordingly. In fact, if we were
   sending a predictable stream here, we could be VERY far off (maybe
   we had a scheduling blip) and still fix our sync. (Of course, small
   drift would usually misalign the four reads so that we are actually
   getting a read for a nametable location when we expect an attribute
   table read. I think most of the time we are adjusting sync this way.
   Fortunately these all have disjoint address ranges, so it's pretty
   easy to know which one the PPU is asking for. Main complication is
   the special cases that happen at the ends of scanlines (sprites,
   which also fetch some CHR) or on the pre/postroll scanlines.
   
   BTW: Even with CIRAM disabled, we can still get info from the CPU
   if it modifies sprite locations (this determines what scanline it
   appears on) or graphics (modifies the address of a CHR fetch;
   probably better). So I think I should just disable CIRAM and get
   into that.

ALSO, there appears to be a much better library. BCM library benchmarks
at about 5.4 Mhz (this approximately matches my experience; 1 / .164us is 6.09,
and I did some speed tweaks).
http://codeandlife.com/2012/07/03/benchmarking-raspberry-pi-gpio-speed/
Supposedly "native library" is 22MHz! Hopefully not PCM...

(This same page warns that you need pretty short leads to the scope
at high bandwidth, so we may be entering the realm where these longass
bell wires in the rat's nest are actually messing things up. Beware.)

Well indeed, gpio.c is way faster (at least just strobing a single pin):
I get 28ns high, 32ns low. That would be plenty fast. Not at all clear
what the latency is, though. With -O2, it's 16ns/16ns, which is like as
fast as ROMs.

Note: bcm library falls back to /dev/gpiomem if /dev/mem is unavailable
(not root). Check if this is a performance issue!

ppuppy w/ gpiomem: latency is ~200ns
ppuppy (root) w/ /dev/mem: latency is ~154ns. better!
Note that we also can't realtime scheduling without root. So we should
really always run this way.

 - accurate timing on pi -

There is st_read(), which reads microseconds from the system timer.
microseconds are already too long for this task! Also it has some
performance issues:
    //  - st_read does three reads of the clock to avoid overflow. Since
    //    we expect a small number of ticks here (could make this a
    //    requirement), we could stop doing that and just do some modular
    //    math.
    //  - st_read probably needs to do a memory barrier because the timer
    //    is a separate peripheral from gpio.
    //  - function calls have some overhead here.

So, trying a very simple loop with "volatile", etc. to disable optimizations.
This works OK. With a delay of "3" I can get a pulse time of 120ns. This
doesn't help the latency, obviously, which on this experiment was 258ns,
missing the rd window completely.

Moving address decoding, and incrementing of edges (this seemed surprisingly
expensive. Is ++ on a 64-bit integer a lot of instructions?

 - enabling all the GPIO -

"At reset only pins GPIO 14 & 15 are assigned to the alternate
function UART,"
(I think this might be out of date though?)

Looks like you can turn off the special function of those hat pins with
force_eeprom_read=0
in config.txt.

"Pins 3 and 5 are pulled up on the board to the 3.3V supply by 1.8 KΩ
resistors to enable them to be used easily as I2C communication pins."
hmm, really?
http://www.mosaic-industries.com/embedded-systems/microcontroller-projects/raspberry-pi/gpio-pin-electrical-specifications

 - disabling CIRAM -

I wired CIRAM /CE (chip enable) to 5V directly, which should disable
it. The video output seems to reflect this: Like the ice field is
reliably the same characters, seemingly in order (because we see
012345678 although not immediately followed by 9ABCDE_GHIJ_LMNOP).
Actually, does that make sense? When reading the nametable, it'll
actually get some of CHR ROM, which just means interpreting
character bitmap lines as character indices, which shouldn't
produce such regular results? It looks more like perhaps the address
is sticking around on the data bus?

Also weird: If I just read PPU D0, which has been cut from the rom
chip, and also from my output, it has lots of distinct clean data
on it. Where is it coming from? I thought CIRAM, but I *think* I
turned it off? Could it be floating and just getting interference
from nearby wires/traces/etc.? It doesn't look like pure noise,
but maybe that's possible.

Try next: Weak pullup/down on this line?

 - bus conflicts / open bus -

Trying to really understand out what's happening here. To recap: I
disabled CIRAM by pulling the pin to 5V, which since it's /CE, should
turn it off. Also, PPU D0 has apparently clean data on it, even though
I cut it from the ROM and from my intrusion.

This page has what I needed:
  https://wiki.nesdev.com/w/index.php/Open_bus_behavior

The PPU has two data buses:

1. The I/O bus. It connects CPU and PPU directly, and the cart pins
    labeled "CPU D*". This bus has multiple devices on it: PRG ROM
    (and potentially other cart stuff), on-NES CPU SRAM U1, CPU, PPU.
    There is some address multiplexer that decodes an address
    (U3, aka 74LS139) to enable different devices on that bus. For
    example, writes to the "PPU status registers" are really just
    writes with addresses that cause U3 to enable the PPU data bus
    (PPU 13 /DBE), and also pass along the low three bits of the
    address (which are connected to three PPU pins).
    Anyway, this bus is not relevant here.
    
 2. The video memory bus. This connects CIRAM, CHR ROM, PPU. However,
    the PPU does a dirty trick. PPU pins 38-31 are both address and
    data. These pins are wired directly to the cart (PPU D0-D7), so
    we can observe their state during the first phase of the address
    load. I believe that the sequence is like:

      A. /RD is disabled (high), etc.
      B. Assert high address bits. This enables or disables chips
         through their /CE pins. But output is not yet enabled.
      C. Assert low 8 bits of address on 38-31.
      D. (Also assert high address; not sure if this happens now or
          after the next step. Nothing funny with these bits.)
      E. Enable ALE so that U2 stores the address bits.
      F. Switch pins 38-31 to "input" aka "tri-state" or
         "high impedance" mode.
      G. Now enable /RD (go low). (Similar thing for writing, although
         it would need to first assert the data on 38-31.)
      H. This enables /OE for the relevant chips, which now assert
         their data on the bus.

    So, in the case that nothing is asserted, due to bus capacitance,
    the PPU just sees the low 8 bits of the address (the same values that
    it asserted) when it reads the data pins back. This explains why
    Ice Hockey just shows me the nametable (more or less): No chip
    responds to the reads of the nametable (CIRAM is disabled, but I
    didn't enable the ROM for these addresses!) so address 17 appears to
    contain character 17, etc.

    This also explains why D0 has data on it: I'm seeing the address
    written to that shared bus during phase C above.

This is also a good explanation of these concepts for a generic shared bus:
  https://en.wikipedia.org/wiki/Three-state_logic


So the question is: What to do about it?


 - tri-state output -

OK so I made a simple tri-state output with two transistors.
It's still taking too long from PPU RD to the pulse, which is
probably why the low bit flickers on-screen (it's pretty
close so probably sometimes it's making it and others not).

Just messing with logic levels outside of ppuppy:
 - until setting the gpio pins to output, note that the 3v side
   floats, and is influenced by the 5v bus, even through the
   transistor (backwards, ugh)
 - but just ./clear.exe on 5 and 6 will make this behave as
   expected.
 - manually ./set.exe 6 drives the low bit to 1. The bus still
   goes low at times, presumably because address bits are
   driven harder than our 4.7k pullup. (This is expected.)
 - in "float state" (5=0, 6=0), we get a stable checkerboard
   pattern on bit 0. This would be the open bus continuing
   to read the address bit, as expected.
 - driving low (5=0, 6=1) both reliably clears the bit 0
   pixels, as well as doubling pixels in CHR graphics (also
   the nametable). This is because we're driving the address
   bit 0 low. The 5V bus has noise on it but never gets higher
   than about 0.4V. This is because the output circuit just
   connects the bus to 0 (no "pull-down") and I guess what
   we're seeing leftover is the voltage drop across the
   transistor.

So: This is all completely as expected.
Running ppuppy gives crap results, though. This is probably
a timing problem!

 - timing of addresses / rd / a13 -

I tapped into those address bits on ice hockey.

 address-timing.png

Address bits come live as the PPU RD edge rises. (In this screenshot
they happen both before and after the edge (eg. 28 ns before, 7 ns after);
this is presumably because the voltage level is changing on the bus and
is only recognized as logic 1 or 0 at a certain point. 

Remember that the PPU first puts the low bits of the read address on
the data bus, then shifts it to U2. I can kinda see this on the scope:

timing-shift.png

Note how D0 goes high first (address being loaded onto data bus), then
RD goes high, then A0 appears with the same value. A13 goes high at
the same time, which makes sense since it is not latched on U2 (PPU
just writes it directly at the same time). But otoh A4 also goes high
earlier; this should have been latched by U2, so maybe the first
hypothesis was right; they just take different amounts of time to
rise/fall. (To test this further, I could hook up to PPU D1-D4 and
look at how they follow A1-A4 too.)

Anyway: Address bits change close to the time RD goes high.
(So we could look for an RD high edge, wait a few ns, read and decode,
compute our value, then write. This would probably move us up ~100us)

Aha! Oops! I can't really look at when data is typically written,
because although there is a shared bus for address and data, that's the
PPU D* pins, but I'm looking at PPU A* pins here--those only have the
shifted address data. This makes the scope readings make sense. Need
to do some more soldering.

OK: So, data ends up on the bus from CHR rom about 40ns after the
falling edge:

   timing-dataout.png

and it stays there until about 196ns (32ns before the rise). Here the
PPU is driving the data bus low so that it can put a 0 on it for the
next address.

So I think the lesson is: After the falling edge, get your data
there fast ~30ns??) and then float before the next rise, because
the PPU uses that bus for shifting addresses before the cycle is
up.

So I did a total hax to try to time it on the rising edge (also inverted
the logic for A13 -- check if this is now correct, or just shifting the
timing a lot!). Now compare phenomena:

 with set.exe 5, we get a solid row on the right side of each tile,
 but also each character is doubled (because address is driven 11 for
 nametable reads as well)

 with ppuppy.exe 5, putting aside all the jitter, we get the solid
 effect on the right column of CHR, but the nametable reads are still
 just the address--unaffected by our output. great!

 on the scope, these pulses are very short:

   first-working.png

 but that seems to be enough for the ppu to see HIGH, though. It also
 works to drive low. So I think we're onto something here!

 - syncing -

In the steady state, the PPU does four reads per 8-pixel
strip (i.e., the slice from one scanline). Let's call this a packet:
  A. character from nametable
  B. attribute byte from attribute table
  C. pixel bits for corresponding CHR (low)
  D. pixel bits for corresponding CHR (high)

The address of the nametable read depends on the coarse scroll
value, and the nametable selected, but otherwise is just the
tile offset (32 tiles wide, 30 tiles high). We should be able
to avoid thinking about scroll.

The attribute bytes immediately follow the nametable, but the same
attribute byte address applies to two consecutive blocks (in both x
and y).

Then we do loads that depend on the character returned from the
first read. We fetch (tile * 16) + row, where row is like (scanline
& 7). Then we fetch that same address plus 8.

The addresses of the first two reads are computed by the PPU itself
and so we could use them as waypoints for syncing. Unfortunately, 8
consecutive scanlines read the same values, since the same
character is used for each scanline (and attributes are even
coarser). However, this can tell us roughly what scanline we are
on.

A better wayfinding technique is to "ping" the PPU to measure
latency. When the CPU asks for a character, it's going to read
something from the bus, but we don't care what value it reads,
since it's just going to use that to ask us again for CHR. So
what we could do is pass some timestamp to it, and look at the
addresses requested for reads C and D. We can compute the tile
value from these (addr / 16), and even get to double-check since
the 

Maybe this coarse jitter is not a big problem if we just prevent
ppuppy from being interrupted. But a small amount of jitter is
probably expected. It's easy to tell A/B from C/D because PPU A13
is high for the first two reads (from "VRAM") and low for the
second two. An easy way to verify a packet is that D - C should
be exactly 8 (for any packet) and (D >> 4) & 255 should be the tile
index we returned for A (when we are synchronized).

I tested out the new version of ppuppy. The timing looks decent:

  delayticks8.png

This is with delayTicks(8) called between the set_multi and the clear.
Note 96ns between the rising edge of the 3v side and the rise
on the 5v side; I guess this is just transistor slew? Should check
this again with the bus transciever.

But anyway the sync is TERRIBLE. It only counts 0-3 scanlines per
frame, usually 300-500 desyncs, and usually overrunning the packet
buffer. Are there just bugs? Time for a run.

Simple idea: use the nametable read addresses (gives us rough
scanline), then if we think our sync is good (because we get a
matching pattern read), use the low bits to establish the current row?
"good sync". Need to actually read more address bits, though.

 - kernel module -

I definitely haven't solved the timing issue above, but it's starting
to look tractable. The biggest problem is whole-screen glitches, which
happen about once per second. If I could maybe disable whatever is
doing that, I'd probably be "good enough."

 - "simple idea" -

Ugh, it still doesn't work!

 - "simplest idea" -

Just turning off the memory barriers does seem to give us enough time
to do address-dependent writes, so just do it that way.

This may actually be more stable than I thought -- if the output byte
(e.g. attribute, but also color bits) is just like fine & 1, then
there's clearly some blinking, but it's also clearly doing something
repeatable. But if it depends on the column, I get a lot of noise.
(Maybe trying the color attribute is easier to see than color bits,
for some reason?)

Although the sync is pretty bad, I suspect that this is failing more
because the output drive is not strong enough. This is why we see
a lot of noise. The voltage on the 5V d0 is only like 1.7v.

Try next: Use a stronger "pull-up".
(alas did not work)

Try: Just rudely disable interrupts??
Try: XinuPi?

 - disabling interrupts -

It's likely that doing this puts the machine in a broken state.
But it may actually be exactly what we want, to just enter a loop
where the processor is only executing instructions to serve the
gpio pins and can never return to linux.

Anyway, both the ARM core and the BCM2835 have some interrupt
capabilities. On the ARM, you can just do this:

   mrs r0, cpsr  ; save status register into r0
   cpsid i       ; disable interrupts
                 ; that's d for disable, i for interrupts?

(actually this looks sorta wrong because what if an interrupt
happens between the two instructions, and that interrupt changes
what interrupts are enabled?)

and you can then put it back:

   msr cpsr_c, r0  ; restore status register

(or explicitly enable as cpsie i)

Presumably this can't be done from user-space, but there may
be a kernel call for it. Or we write a kernel module.

Disabling IRQs from the bcm chip is actually much easier,
since it can be done from the memory mapped peripheral.

http://embedded-xinu.readthedocs.io/en/latest/arm/rpi/BCM2835-Interrupt-Controller.html

linux header lists the IRQs and other stuff:
https://github.com/raspberrypi/linux/blob/rpi-3.6.y/arch/arm/mach-bcm2708/include/mach/platform.h

might need to disable watchdog timers? It's a peripheral.

 - it's not working -

There may be a combination of issues here: For example, we may be
getting desynchronized because of timing AND because the address
lines are noisy. (Note that they are not even supposed to be stable
until the falling edge...)

OK, I think at least one problem is that these BJT transistors have
crap rise/fall times (tens of milliseconds), so my output stage is
just not able to switch fast enough. Turns out that in the 50s they
discovered this and built "baker clamp" circuits to prevent the
transistors from saturating (by using diodes to provide negative
feedback, though I don't totally understand what's going on). Later
they started building these diodes into the transistor package,
creating Schottky transistors which became part of the logic
family called Low-Power Schottky Transistor-Transistor Logic,
aka LS series chips. Note there are several LS* chips on the NES!
So I think we need to be using real chips for this, not just 
futzing around with homemade transistors. Good news is that these
chips are ubiquitous!

 - reducing jank -

Can disable HDMI etc with:
tvservice -o
Also: 
vcgencmd display_power 0
Turns off video output.

Could probably unmount the filesystem upon program startup,
 - could disable wifi I'm sure,
 - maybe also disable swap?
Turn off USB power:
echo 0 | sudo tee /sys/devices/platform/soc/20980000.usb/buspower >/dev/null
Can save 2mA (!) by turning off the LED.

can kill lots of processes:
 - bluetooth 

I think maybe killing /usr/bin/dbus-daemon --system hosed wifi though?
or dhcpd?

Good news: Forcing all of the interrupts off does leave the CPU loop
running, and I can clearly see the expected outputs happening on the
scope. This also does yield MUCH more stable behavior. Specifically,
the obvious periodic jumping at about 1 Hz is completely gone. Since
the output is not at all working, though, it's not clear if there may
be much smaller timing problems persisting.

Of course, this is a bad developer experience since Linux is
completely hosed after I do this (no debug printing, no wireless,
probably no filesystem or anything interesting; need to hard reset to
connect again, and so on). I may also need to avoid using some basic
stuff like malloc, since these often do use stuff like memory faults
in their implementation.

(It might be possible to turn them back on periodically and have the
system recover, but I don't actually know which ones were enabled to
start with, and there doesn't seem to be a documented way to get
that?).

I don't really see any problem with this for production though. It's
actually ideal to boot into linux, then once we're ready, just become
an embedded microcontroller and run instructions in a loop with simple
timing.

There may be a watchdog timer or something that resets the system if
it's in this state for a while. But I at least ran it for well over
50 minutes without any obvious failure.

 - using bus transciever -

I wired this up. It's set in the A -> B direction (DIR=1) and I just
wired pi pins directly to the inputs, and bus pins directly to the
output, which is refreshingly simple.
 - At first I didn't cut all the ROM pins on the bus. So they were still
outputting values, but the new chip seems to drive the bus harder,
which is good. (?)
 - Then I cut the low three bits, but left the fourth, which may help
   diagnose problems.
 - But also, the tiles and colors also got corrupted, which is not
   expected -- these were formerly provided by stale bits on the
   bus (from the read address). They shouldn't be affected here
   because the bus chip is only enabled when PPU /RD is low (wired to
   /OE). It's likely that the chip doesn't disable fast enough to
   avoid sinking current on these lines as the PPU sets the address
   bits. This may be okay (we shouldn't need to rely on bus capacitance)
   but it's not great...

Clues: Setting all of d0-d3 as input causes heavy bus noise. I guess
this is not that surprising, since the transciever may just be sending
"floating" values on the bus. It's weird that if I only set one to
input, the screen still looks stable.

Reading address lines doesn't cause instability, on its own.

Oops, for lots of these experiments, I had the pulldown set on the
output channels. I don't think this makes sense. Turned it off.

Works much better. At this point I have a noisy bouncing ball demo.
I suspect the major remaining problem is that the addresses are
not high quality. Let's switch to the level shifter since that
should just be a better way than the voltage divider.

Actually now that I look at it, the hex level shifter has like
140ns-280ns propagation delay times (!), which is probably too slow for
this application (?).

Looking at the scope, the voltage divider approach doesn't seem to
introduce any delay at all, which is great. But oddly, I'm seeing that
the 5V logic levels are actually peaking at like 3.8v, with the 3v3
level then down to like 3.2. PPU RD is like 2.1v on the 3v3 side, and
3.3 on the 5V side. 
 - 5V rail is 5v as expected.
 - data (d0) looks as expected: with 3v pegged high: 3.3 on 3v3, 4.5 on 5V.
 - weirdly, a0 shows 3.6v at the same time that d0 shows 4.4v. (This is
   true regardless of whether the bus transciever is off (rd high) or not.)
   
voltage.png

These lower voltages are more glitch prone, right? Why is this
happening? Maybe there is a (resistive) short somewhere? Maybe the CHR
ROM chip is sinking too much current on the address and RD pins? I
might as well cut those pins.
 - cut a0 and a1 on the ROM. didn't seem to do anything.

On the mostly unmodified zelda cart, PPU RD peaks at 4v. Digital waveform
also looks perfect.

So I guess maybe the voltage divider itself is pulling the peaks down?
It's equivalent to a 3k pulldown (even more if the 3v3 input is sinking
significant current? but that's like 40k?). If these signal lines have a
10k pullup, say, and here we have a 3k pulldown, that would be like a
23% reduction in voltage if these lines are just driven by the pullup
(i.e. in common-collector mode, which I think is the case). (And on
the NES schematic, the visible pull-ups are indeed 10k).

CONFIRMED. Zelda PPU RD was 4V before, and with 3k resistor to ground,
it's 3.4. So I think I should use higher value resistors for the
voltage divider if I go that route.

Then I tried 4.7k + 2k. In the center I get a peak of 2.37v; on the 5v
side I get 3.7v. (This was 2.1/3.3 with 1k/2k resistors.) This is also
pretty far off. 

20k + 10k: 3v3: 1.8v ?! ; 5v: 3.9v.
Not sure what went wrong here? The 5v side makes sense to me (nearly
restored to 4v) but the 3v3 side, measured between the 20k resistor
(which was two 10ks in serial) and the 10k resistor had a strangely
low voltage; the peaks seem to lag the 5v peaks, and the troughs
are at a minimum of .482v, even though on the 5v side they go as
low as negative.
Between the ground-side 10ks, a peak of .914v, trough of .216.

10k + 4k: 3v3: 2.09v   5v: 3.7v

3v3 peaks of 2.09v will only barely trigger the input, so this
is no good.
troughs are at like 0.196v, which is perfectly acceptable

spreadsheet:
https://docs.google.com/spreadsheets/d/1KEUBxf1N64e4Ou1zlXg2AiTcm0OBiHcpzixwCrBVZYQ/edit#gid=0

hmm, none of these are good.

Some options:
 - find good level shifter with fast propagation delay 

 - test the ones you bought; maybe they outperform at 3v, or maybe
   100ns is (barely) ok.

 - note that we are reliably dropping the voltage on RD to the range
   that's acceptable for CMOS. So a really crappy way to do this is to
   just put a 3k "pulldown" resistor on the RD line (and address
   lines, although we'd need to measure each one to understand its
   characteristics) and then connect the RD line directly to the
   3v3 input. This is the most dangerous (in principle this could
   go as high as 5v if driven by something with more current) and
   also just terrible engineering.

 - diode-based level shifter.
                                         3v3 out
                                          |
   GND ---<|--+--<|--+--<|--+--<|--+--<|--+---'\/\/\/'--- 5v
             0.6    1.2    1.8    2.4    3.0    4.7k?

   diodes have a like "fixed" voltage drop, so 5 (!) in series
   here would give us a reliable 3.0v reference while having
   a total series resistance of the final resistance, which
   we could choose to be pretty high so as not to pull down
   the 5v line too much. 
   Datasheet for 1N4148 (from Vishay; I have knockoffs) gives
   recovery time of 4-8ns, which would clearly be adequate.
   (These list the forward voltage as "max 1v", though..)

Tried this, with 4 diodes (multimeter read them as 7.11, which
I think is voltage drop?) and a 10k resistor.

   3v3: 2.0          5v: 3.8v
   trough 0.8v      trough negative

this is not enough; what about one more diode?

   3v3: 2.2          5v: 3.85v
   trough 0.93v      trough negative

also it's pretty obvious that this is causing some
delay and flattening of the curve, worse with each
diode drop. (Actually, honestly, the levels are low
enough that it could even be crosstalk between
channels on the scope.)

Tried the level shifter. Wired both sides to 5v
for simplicity. As promised, the delay is high
at 98ns. (5v side is 3.9V, output peaks at 5.4v,
which is even higher than VDD; a bit unexpected
and possibly dangerous for 3v3?)

This might be workable, although this delay
definitely makes the timing tough; it's more than
half of the time that RD is high (98ns before we
see the rising edge; it falls 82 ns later and
we don't notice for 124ns. Bleh. But it's still
within one cycle, and electrically it looks
pretty nice. 
Alas: If I provide 3v3 on pin 16, I don't get any
useful output! (I didn't try this with a good
quality 3v source, to be fair, so maybe I'm not
supplying enough current.)

 - deglitching inputs -

Wow, this helped so much! First I made every input loop (e.g. waiting
for RD high) vote over the last three inputs. This noticeably helped.
The big difference was just inserting several more reads after RD
goes high--I guess the addresses just aren't stable until that point.
With this, the ball is clear at any position on screen, and almost...
tolerable! I can even switch the palette every scanline and it's
(kind of) stable.

The attribute table has some blocks that are never modified, but this
makes sense, since attribute bits are packed into bytes and the
current output only affects the low 4 bits. If we ever want to change
the color of those cells, we need to be outputting all 8 bits, which
is, obviously, the plan.

Maybe I don't even need better quality inputs with this technique.

After separating the two bitplanes, I notice that the second one
(packetbyte=3, the final byte) is much sparser looking. It could
maybe just be an illusion because the colors are not as bright(?)
but it looks like it's not affecting the color bits but rather
the next nametable read?

 This is very sensitive to timing!
 - Let's pre-render a table during vsync and just index into it,
   instead of doing any computation in GetByte (this is eventually how
   it will work anyway). Can pre-encode bytes.
 - Can optimize address decoding.
 - Address glitching is probably the biggest remaining problem,
   so if the level shifter just gives us stable values right off
   the bat, this approach will likely succeed.
 - There appears to be some hysteresis control (input) for GPIO.
   (it's in the bcm header but not the peripherals doc?). Default
   is hysteresis enabled (according to bcm2835.cc).
 - Turning on or off slew-rate limiting (output) may help as well.
   Default is slew-rate unlimited (according to bcm2835.cc).
 - May also consider changing the pad drive strength. It defaults to
   8 mA. The bus transciever is in the uA range.
 - Rising and falling edge detection might be another way to get
   more solid readings without manually coding it.
   (Specifically: This may be a lower-jitter way to wait for RD to
    go high or low.) Note there are both synchronous (using system
    clock, deglitching) and asynchronous (probably using schmitt
    trigger or whatever in hardware).
    

 - rewrite to screen/demos -

 Pretty sure that I'm just missing the deadline for writing the packet
 bytes. If I just write "hi" (last phase) bits, then I see the
 bouncing ball but in the sub-columns where I'm not outputting data
 (because the writes are actually affecting the nametable address or
 read; the next packet byte). If I just write "lo" (second to last
 phase) bits, there's very good fidelity within the circle, but it's
 red, which is palette entry 2 (hi bit set, lo bit not) on this screen.
 So lo bits become hi, and hi becomes nametable.

 Optimizing:

 DEGLITCH_READ is well-optimized. Two in a row is just

        ldr     r1, [lr, #52]
        ldr     r2, [lr, #52]
        orr     r0, r2, r1
        orr     r4, r0, r4
        and     r4, r4, r3

 and three is

        ldr     r4, [lr, #52]
        ldr     r1, [lr, #52]
        ldr     r2, [lr, #52]
        orr     r0, r2, r1
        orr     r3, r0, r3
        and     r3, r3, r4

 We might want to interleave the reads more, since spreading
 out in time is the goal, not simply to do a lot of reads. This
 would probably need some inline asm. Anyway. It's generating
 fast code.

 Address decoding is crummy though. It's like 26 instructions.
 We could make this faster by:
   - hand-optimizing (some shifts can be done together)
   - using tables
   - rewiring (putting the bits in more correct order on the
     pins will help)

 even though they're all mixed around, pins 2-27 are all
 available on the board.

 so we could reserve 2-3-4-5-6-7-8-9 for data out
 and then use the next 10 for address in,
 and the address decoding would just be like 2 or 3 instructions.

 - getbyte looks reasonable, although there's like if-else for
   the packet. Could instead compute a base address and offset.
   (did it)

 addr decode before:
 	lsr	r3, r7, #14
	mov	r4, r3
	lsl	r5, r7, #17
	and	r4, r4, #4
	and	r3, r3, #2
	orr	r3, r3, r4
	lsr	r4, r7, #14
	orr	r3, r3, r5, lsr #31
	and	r4, r4, #8
	lsr	r5, r7, #14
	orr	r3, r3, r4
	and	r5, r5, #16
	lsr	r4, r7, #21
	orr	r3, r3, r5
	lsr	r5, r7, #14
	and	r5, r5, #32
	orr	r3, r3, r5
	lsl	r4, r4, #7
	lsr	r5, r7, #14
	and	r5, r5, #64
	orr	r3, r3, r5
	uxtb	r4, r4
	lsr	r5, r7, #14
	orr	r3, r3, r4
	and	r5, r5, #256
	lsr	r4, r7, #14
	orr	r3, r3, r5
	and	r4, r4, #512
	orr	r3, r3, r4

  after:
	lsr	r0, r7, #14
	uxth	r0, r0

  much better :)
  
