Stuff still to figure out / achieve:

 [X] Read a 5V output (e.g. ppu /rd)
 [X] Have data output depend on address read (low order bits)
 [ ] What are the PPU writes? Do I need to handle them at all?
 [ ] How often can I change sprite ram?
 [ ] Can I prevent linux from descheduling me, except in vblank?
 [ ] Enough time to render SNES frame in vblank?
 [ ] Is it actually possible to do anything?
 [ ] Boot up without having to SSH into it. (Takes several seconds to boot...)

Backup plans:
 [ ] Second pi renders SNES frames, and just sends them to "video" pi
     during its vblank (wifi??)
 [ ] Kernel module
 [ ] Separate fast GPIO microprocessor renders the frame?

This doc rules:
https://www.raspberrypi.org/app/uploads/2012/02/BCM2835-ARM-Peripherals.pdf

 -- timing -- 

NES:
Measured pin #38 (M2 -- this is CPU) on the saleae.
Example pulse: .36 us high .198 us low, which is 1.792 MHz.
           ie.  360 ns,    198 ns
On the pi, a for(;;) loop just calling gpio_write (one pin)
gets 80-88 ns pulse times

this is say .09 microseconds, which means we can do about
4 bit flips per NES clock. this is cutting it a bit close
but maybe in the "not insane" range?
  danger:
    - can I set multiple bits at that same rate?
       at a minimum I need to read the address bus, then
       write the data bus, then wait for the falling edge.
    - should make sure that transistors/diodes to do
      bridge between 3v3 and 5v ttl don't introduce too
      much slew etc. The analog waveform at 50MHz is 
      basically a sine wave so we may be reaching some
      electrical limits (?)
    - there are periodic dropouts. How to set realtime
      mode or whatever? We could probably cooperatively 
      schedule during ppu vblank
    - do we have enough time left to prepare CHR data?
    - do we have enough time left to do some emulation, etc.?

hmm, write_multi on 4 bits gives me 80ns/200ns off/on
cycle with a grosser waveform. 

But it looks like the ARM is just executing too many
instructions. With -O2, and removing debug test from
peri_write, I get 48ns/112ns off/on and a sinusoidal
wave. Plenty of more optimization to do...

 - inlined peri_write (this version also actually packs
   bits into the pins written, so that it's not trivial): 52ns/124
 - inlined all routines necessary to do write_mask: 44ns/120
 - memory barrier before/after: 48/80
 - barrier just before: 56/100

 -- more NES timing: PPU --

- The PPU RD bit happens as a solid square, .18 ON, .192 off.
  (but see "gap" below)
- This square wave happens for 15.305312ms.
- Then RD stays high for 1.334072ms.
  Together these add up to a period of 16.634, which is very
  close to 1/59.94, i.e. "60" FPS. So when RD is high we are
  probably in vblank. If we could manage to schedule other
  tasks here and fill the framebuffer on the pi, we'd be
  golden?
  (during the vblank, addresses are usually stable but
   sometimes there are glitches. trust RD.)
- Looks like addresses change when PPU RD is *high*, by
  the way, meaning that the address bits are safe to
  read when it's *low*. (This concurs with the docs,
  which call it /RD)
- Gap: I see a gap in reads (high cycle for .368 us) every 63.5us.
- 15305.312u (full frame) / 63.5us = 241 almost exactly,
  which I think means that this double-read is the end of
  a scanline.

 -- switching d0 --

This is not working right.

I have a 4.7k pullup from 5v to DATA.
- DATA is on the Collector of the NPN transistor. [ch 2]
- PIN 26 is on the Base of the transistor [ch 0], after a 10k resistor
- GND is on the Emitter.

We should be able to pull this close to ground (+ transistor drop) by closing
the transistor's circuit. But:

    ch 0              ch 2
 26 set = 3.3v       about 3.8v
 26 clear = 0.0v     about 4.9v

so it's like maybe I'm only turning on the voltage drop?
AHA! Perhaps the transistor is simply backwards [laughing crying emoji]
because the pinout on TO-92 IS NOT STANDARD!

fixed! now:

    ch 0              ch 2
 26 clear = 0.0v    about 4.9v
 26 set = 3.3v      about .03v


 -- how the NES works --

74LS139 (CPU side) is a simple multiplexer (2 bit address in, selecting
which of the four outputs gets LOW logic level; others are high). (Actually
it has two such circuits.) This is hooked up to CPU A13, A14, A15.
(I think it is selecting between sram, etc.?)
I think that on the schematic, the "E"nable pin is labeled C. Note how
one of the Y outputs from the left side is wired into the right half.
This means that neither Y1 (ppu reg) nor Y0 (cpu side SRAM)
will be enabled (pulled low) unless the right half is enabled by the
left half. The left half is always enabled (C to GND) and is selected
by address bits A15 and M2. Together the left half controls whether
the right half is enabled, plus outputs to pin 50 (on cartridge),
which is /ROMSEL. So the CPU can signal the A15 bit to the cartridge
with every address, clocked with M2. Not sure what this is used for
exactly (it already gets A0-A14 directly, not to mention M2).

 -- cpu/ppu communication --

To write to PPU VRAM, you use memory mapped cpu registers;
two bytes to $2006 to set the address, then bytes written to
$2007 get streamed into incrementing addresses. (It's also possible
to increment by 32, which moves "down" since the screen is 32 tiles
wide.)

On the board, cpu has 3 address bits wired to the CPU to select the
PPU register (there are 8 of them), and an 8-bit bidirectional bus.
/CS is pulled low by that 74LS139 address multiplexer (not sure
in what conditions exactly) so that $2007 selects register 0b111.
Apparently the condition is that the address is from $2000 to $3fff;
the middle bits are just dropped. 

specifically, the address is:
001* **** **** *ppp
where * is ignored, ppp selects the ppu register, and 001 is setting
the range $2000-$3fff. The 1 here is A13. If you look at how the 74139
is wired, this makes sense, since the right half has A13 and A14 as
inputs (and is only enabled when A15 is low), and PPU /DBE as one of
its outputs.

 -- ppu memory mapping --

On the PPU side, this works differently:
The 2k SRAM chip (officially CIRAM, aka VRAM) is
enabled directly from the cart. Pin 57, aka CIRAM /CE, enables the
chip when low. The PPU RD and PPU WR lines are wired directly to CIRAM,
also connected to the cart traces.
A10, the highest bit of the chip's address space, is controlled by
the cart--not hooked directly into PPU. (I guess probably some carts just
wire this directly from PPU A10 though?)

Anyway, ppuppy should disable CIRAM (wire /CE high) so that it can
just supply its own data.

 -- wiring notes --

On zelda cart, PPU /WR is wired to /WE (write enable) on the SONY chip
on the top right, which looks like SRAM?
(http://www.datasheets360.com/pdf/4386973877628639512). I had
previously assumed this chip was PPU CHR ROM. It's 8k, so it would be
enough. But it makes no sense that it would ship in SRAM and lose all
the tiles as soon as the battery dies. But maybe this is not the
battery-backed SRAM.

Chip at top left is RP231024D, a ROM. It's labeled PRG. Could verify
that it's hooked up to CPU bus.

Chip at the bottom left is LH5164D, Sharp, which also appears to be
SRAM:
http://pdf1.alldatasheet.com/datasheet-pdf/view/42972/SHARP/LH5164A.html
(Maybe this is the battery-backed one.) Also 8k.

Anyway it turns out that zelda has a fairly complicated mapper, the
MMC1. I guess perhaps the chr memory is SRAM and gets filled by CPU,
reading from PRG ROM?. That's weird because I recall the zelda pattern
tables being very stable (only switching when entering the dungeon).
Oh well, maybe this is not a great test game...


So Ice Hockey is easier to understand. Both CHR and PRG are ROM and
the only other chip is CIC. The CIRAM is hooked up as follows:

 - /CE directly wired to PPU /A13. So CIRAM will be returning data
   (for reads) whenever the address is < 0x1FFF (because then bit 13
   will be zero and these are zero-enable pins). A13 is the highest
   address bit. When the address is 0x2000-0x3FFF, A13 is high, so the
   chip is disabled, and ROM can assert its values.
 - Note PPU A13 is also passed (not negated) to the cart. I bet this
   is wired to /CE for CHR ROM. (CHECK). That would allow these two
   chips to be mutually exclusive.
 - /OE directly wired to PPU /RD. This is done on the NES motherboard.
   So CIRAM will only output when PPU is reading, duh.
 - /WE directly wired to PPU /WE; same idea. Not much to do with this.
   If we really want to avoid writes to CIRAM, a simpler thing would 
   be to just disable the chip with /CE, which we control.
 - CIRAM A10 controls mirroring.
   https://wiki.nesdev.com/w/index.php/Mirroring
   On NROM (e.g. Ice Hockey) it is hard-wired to either PPU A10 or A11.
   (Ice hockey uses Vertical mirroring, where CIRAM A10=PPU A10.) This
   is just bit logic so that reading the nametable during scrolls
   "wraps" the way you want. Should be irrelevant for ppuppy since we
   won't scroll. Can probably leave unconnected, like Gotcha! game does.

So I think the next experiment to run is to read PPU A13 (or /A13)
and disable reads when A13 is low. This would properly mimic CHR ROM,
avoiding bus conflicts with CIRAM.

 -- ppu rendering --
Nice simple explanation from nesdev wiki (PPU nametables article):


Conceptually, the PPU does this 33 times for each scanline:

A. Fetch a nametable entry from $2000-$2FBF.
B. Fetch the corresponding attribute table entry from $23C0-$2FFF and
   increment the current VRAM address within the same row.
C. Fetch the low-order byte of an 8x1 pixel sliver of pattern table from $0000-$0FF7 or $1000-$1FF7.
D. Fetch the high-order byte of this sliver from an address 8 bytes higher.
E. Turn the attribute data and the pattern table data into palette
   indices, and combine them with data from sprite data using priority.

(Note: At the beginning of each scanline, the data for the first two
tiles is already loaded into the shift registers (and ready to be
rendered), so the first tile that gets fetched is Tile 3.)

It also does a fetch of a 34th (nametable, attribute, pattern) tuple
that is never used, but some mappers rely on this fetch for timing
purposes.

(Actually it is more complicated than this, though.
 Good detailed doc: https://wiki.nesdev.com/w/index.php/PPU_rendering
 There are 340 PPU cycles in a normal scanline, and each read takes two
 cycles. So we expect to see about 170 * 240 reads per frame, which is
 40,800.)

Note nametable would usually be RAM. We could disable this and supply our own
values, though. (This is probably why we see data on the data bus D0
all throughout the scanline, even when we aren't outputting anything.
This is not a vram write, it's SRAM asserting a value on that same bus.)

So, if for every tile it fetches the attribute table entry, it would
be possible to change the palette on every tile, not just every 4x4
block of tiles, right? AND! We could actually change it for each row
of the tile; basically each 8x1 horizontal stripe has to be from the
same palette entry. The major limitation here is just that we only get
2 bits to select the palette, so the screen only has 13 different
colors (background + 3 non-background colors per palette entry). BUT,
it should be possible to flip the palette on alternating frames,
blending colors to get 13*13=169 colors. That's a lot of flexibility.
Changing the palette dynamically would require cooperation from the
CPU (and needs to be done during vblank), so probably the best thing
to do is to pick two constant palettes that mix well to cover the RGB
space, and always alternate between the two. 16 colors is also not so
bad. The additional constraints that 8 consecutive pixels need to use
the same palette definitely adds some complication. Two fun color
problems:

1. Write a function that takes two palettes and 8x1 pixels, and
determines the two two-bit 8x1 stripes that minimize the color loss.
Probably the best results are global (like it's bad if a big solid
color has the color expressed different ways within it). Another thing
this could do is take an accumulated error as input, like
floyd-steinberg dithering.

2. Write a function that finds two optimal palettes for a given
weighted set of input colors. For any palette, the attainable colors
are the averages (and there may be some NTSC theory to this
averaging?) of any pair of colors (including a color and itself). The
loss is the (weighted) distance (like delta-e in LAB space) of all
input colors to their closest attainble color. You could compute
palettes for a specific image you want to display, or just do this
offline for the whole RGB cube and have a constant pair of palettes.

 -- syncing --
We expect to see about 40,800 reads per frame. at r3476, with swapping
disabled and running sudo, I do seem to get about this many reads:
15394215 edge, 376 frames, 40848 last sync, 7596268 839325 6134896 823726.
15435103 edge, 377 frames, 40888 last sync, 7616454 841532 6151186 825931.
15475850 edge, 378 frames, 40747 last sync, 7636578 843709 61
... being under the number is totally unsurprising (missed deadlines)
and we can be over from glitches, or the 40800 estimate might just be
wrong. So actually this seems pretty decent.

 why do I get MORE pulses (on other data pins) when I write 'sync' to output than
 when I write (sync & 1) ? 255 : 0??!
 
 -- more syncing --

Now on ice hockey. It seems that the writes are just taking too long.
Only writing when A13 is high, after PPU /RD goes low, DATA goes high
after 0.156us (A). That is fast enough to make the rising edge of RD.
Then from the rising edge of RD, it takes 0.164us (B) to turn off the
data. During that time RD went low again. We apparently do see that,
because another 0.164 us (C) later, the write starts. Now again, 0.164
(D) to turn off the data. Again we're into the next read-low pulse,
and A13 is off, so we are conflicting with CIRAM.

posterity/timing-late.png

(Actually: *IS* this bad? I am asserting the correct value on the
rising edge, even if I fail to shut off before the next cycle. When is
the critical moment? Maybe the timings are okay but just the
levels/impedance is bad? Could look at timings on other data pins to
see what they do.)

All these transitions take about the same amount of time, but the
reaction to RD going high comes late, because the width of the RD high
pulse is just shorter (.136us) than the low pulse .236. The fact that the
timings are usually the same suggests that there's just some latency
between setting a GPIO pin (or perhaps, reading it) and seeing the
logic level reflected on the output. 

Before we had a write time of <0.090 though. If we can get back to
that, we'd probably be okay timing-wise?

Another option is to just try to sync an internal clock so that we
predict the edges rather than reacting to them. It would be good to
be able to read the address that the PPU is asking for, but OTOH
they should be totally predictable. We could still use the addresses
to adjust the sync, as well as to communicate to PPU from CPU, we
just wouldn't be able to have the NEXT write depend on them.

So:
 - Study the timing of a real NES game.
 - Try to get the read or write latency down. (For example, study
   whether this is about making the C code fast, or if it's just
   inherent somehow to the BCM chip?) Would it be faster as a
   kernel module? Is there low-level support for interrupt on
   falling edge, for example? Can we clock the pi faster?
 - Try rewriting the loop as a predictive sync rather than reacting
   to logic level changes. Kind of a neat problem anyway. The signal
   is very regular, and addresses could give us absolute sync.
   Note that this may be hard to reason about if we have both read
   and write latency (could just assume they are symmetric values?)
   Also note that eventually, we won't have CIRAM storing the nametable.
   (Well, maybe we could have some discrete logic that only maps the
   attribute table to "ROM"--it's the last 64 bytes of each nametable,
   and we could rely on there only being one--but this may be kinda
   messy?)
 - Try disabling CIRAM -- we don't want it anyway -- and just
   generating all ppu data. We'll certainly still have a hard timing
   problem (and now the addresses will be gibberish?) but less confusion
   about bus conflicts. (Of course, games certainly still write to
   CIRAM and we would have the potential to conflict with those writes,
   but at least the corrupted values would likely have no effect?)

   Actually j/k: With CIRAM unhooked, we still get addresses: The
   nametable and attribute table reads should be from predictable
   (more or less sequential) locations. The pattern table (CHR) reads
   (which would have reflected the contents of CIRAM) are the ones
   that would be affected. These reads will directly reflect (well
   there is some simple function) what we returned from nameable
   "RAM", so we could actually use these to check that we're in sync!
   It would look like this:

   PPU reads nametable 0000 (top left corner).
   We return, say, 0000.
   PPU reads attribute table for 0000 too.
   We consult our current sync location, and return the attribute
   bits for that group of 8 pixels.
   Now PPU reads from chr+0000 (because we returned 0000 for the first
   read). We record the address, but just return whatever graphic bits
   we want at the current sync location. It then reads chr+0000+8 for
   the second color bit, and we do the same.

   But now: We can look at the addresses read the second time. We
   controlled the values (modulo some predictable offset), and can
   compare these to what we expect to make sure we're synced up. For
   example, if we expected to get back YYYY but instead got YYYY+1,
   then we adjust our sync position accordingly. In fact, if we were
   sending a predictable stream here, we could be VERY far off (maybe
   we had a scheduling blip) and still fix our sync. (Of course, small
   drift would usually misalign the four reads so that we are actually
   getting a read for a nametable location when we expect an attribute
   table read. I think most of the time we are adjusting sync this way.
   Fortunately these all have disjoint address ranges, so it's pretty
   easy to know which one the PPU is asking for. Main complication is
   the special cases that happen at the ends of scanlines (sprites,
   which also fetch some CHR) or on the pre/postroll scanlines.
   
   BTW: Even with CIRAM disabled, we can still get info from the CPU
   if it modifies sprite locations (this determines what scanline it
   appears on) or graphics (modifies the address of a CHR fetch;
   probably better). So I think I should just disable CIRAM and get
   into that.

ALSO, there appears to be a much better library. BCM library benchmarks
at about 5.4 Mhz (this approximately matches my experience; 1 / .164us is 6.09,
and I did some speed tweaks).
http://codeandlife.com/2012/07/03/benchmarking-raspberry-pi-gpio-speed/
Supposedly "native library" is 22MHz! Hopefully not PCM...

(This same page warns that you need pretty short leads to the scope
at high bandwidth, so we may be entering the realm where these longass
bell wires in the rat's nest are actually messing things up. Beware.)

Well indeed, gpio.c is way faster (at least just strobing a single pin):
I get 28ns high, 32ns low. That would be plenty fast. Not at all clear
what the latency is, though. With -O2, it's 16ns/16ns, which is like as
fast as ROMs.

Note: bcm library falls back to /dev/gpiomem if /dev/mem is unavailable
(not root). Check if this is a performance issue!

ppuppy w/ gpiomem: latency is ~200ns
ppuppy (root) w/ /dev/mem: latency is ~154ns. better!
Note that we also can't realtime scheduling without root. So we should
really always run this way.

 - accurate timing on pi -

There is st_read(), which reads microseconds from the system timer.
microseconds are already too long for this task! Also it has some
performance issues:
    //  - st_read does three reads of the clock to avoid overflow. Since
    //    we expect a small number of ticks here (could make this a
    //    requirement), we could stop doing that and just do some modular
    //    math.
    //  - st_read probably needs to do a memory barrier because the timer
    //    is a separate peripheral from gpio.
    //  - function calls have some overhead here.

So, trying a very simple loop with "volatile", etc. to disable optimizations.
This works OK. With a delay of "3" I can get a pulse time of 120ns. This
doesn't help the latency, obviously, which on this experiment was 258ns,
missing the rd window completely.

Moving address decoding, and incrementing of edges (this seemed surprisingly
expensive. Is ++ on a 64-bit integer a lot of instructions?

 - enabling all the GPIO -

"At reset only pins GPIO 14 & 15 are assigned to the alternate
function UART,"
(I think this might be out of date though?)

 - disabling CIRAM -

I wired CIRAM /CE (chip enable) to 5V directly, which should disable
it. The video output seems to reflect this: Like the ice field is
reliably the same characters, seemingly in order (because we see
012345678 although not immediately followed by 9ABCDE_GHIJ_LMNOP).
Actually, does that make sense? When reading the nametable, it'll
actually get some of CHR ROM, which just means interpreting
character bitmap lines as character indices, which shouldn't
produce such regular results? It looks more like perhaps the address
is sticking around on the data bus?

Also weird: If I just read PPU D0, which has been cut from the rom
chip, and also from my output, it has lots of distinct clean data
on it. Where is it coming from? I thought CIRAM, but I *think* I
turned it off? Could it be floating and just getting interference
from nearby wires/traces/etc.? It doesn't look like pure noise,
but maybe that's possible.

Try next: Weak pullup/down on this line?

 - bus conflicts / open bus -

Trying to really understand out what's happening here. To recap: I
disabled CIRAM by pulling the pin to 5V, which since it's /CE, should
turn it off. Also, PPU D0 has apparently clean data on it, even though
I cut it from the ROM and from my intrusion.

This page has what I needed:
  https://wiki.nesdev.com/w/index.php/Open_bus_behavior

The PPU has two data buses:

1. The I/O bus. It connects CPU and PPU directly, and the cart pins
    labeled "CPU D*". This bus has multiple devices on it: PRG ROM
    (and potentially other cart stuff), on-NES CPU SRAM U1, CPU, PPU.
    There is some address multiplexer that decodes an address
    (U3, aka 74LS139) to enable different devices on that bus. For
    example, writes to the "PPU status registers" are really just
    writes with addresses that cause U3 to enable the PPU data bus
    (PPU 13 /DBE), and also pass along the low three bits of the
    address (which are connected to three PPU pins).
    Anyway, this bus is not relevant here.
    
 2. The video memory bus. This connects CIRAM, CHR ROM, PPU. However,
    the PPU does a dirty trick. PPU pins 38-31 are both address and
    data. These pins are wired directly to the cart (PPU D0-D7), so
    we can observe their state during the first phase of the address
    load. I believe that the sequence is like:

      A. /RD is disabled (high), etc.
      B. Assert high address bits. This enables or disables chips
         through their /CE pins. But output is not yet enabled.
      C. Assert low 8 bits of address on 38-31.
      D. (Also assert high address; not sure if this happens now or
          after the next step. Nothing funny with these bits.)
      E. Enable ALE so that U2 stores the address bits.
      F. Switch pins 38-31 to "input" aka "tri-state" or
         "high impedance" mode.
      G. Now enable /RD (go low). (Similar thing for writing, although
         it would need to first assert the data on 38-31.)
      H. This enables /OE for the relevant chips, which now assert
         their data on the bus.

    So, in the case that nothing is asserted, due to bus capacitance,
    the PPU just sees the low 8 bits of the address (the same values that
    it asserted) when it reads the data pins back. This explains why
    Ice Hockey just shows me the nametable (more or less): No chip
    responds to the reads of the nametable (CIRAM is disabled, but I
    didn't enable the ROM for these addresses!) so address 17 appears to
    contain character 17, etc.

    This also explains why D0 has data on it: I'm seeing the address
    written to that shared bus during phase C above.

This is also a good explanation of these concepts for a generic shared bus:
  https://en.wikipedia.org/wiki/Three-state_logic


So the question is: What to do about it?


