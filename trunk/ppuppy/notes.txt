Stuff still to figure out / achieve:

 [X] Read a 5V output (e.g. ppu /rd)
 [X] Have data output depend on address read (low order bits)
 [X] Can I prevent linux from descheduling me, except in vblank?
 [ ] Enough time to render SNES frame in vblank?
 [X] Is it actually possible to do anything?
 [ ] Add GPIO switch to enable boot directly to ppuppy
 [ ] Boot up without having to SSH into it. (Takes several seconds to boot...)
 [X] Single board with CPU and PPU interfacing.
 [ ] Consider moving the bus transciever /OE to the 5v side, since this
     input should be 5v tolerant, and it will be slightly better synchronized
     with the actual bus (right?)
 [x] Flow for talk
 [x] wing-it dry run
 [ ] Slides
 [x] Measure proper footprint/keepout of PCB
 [x] Order boards
 [x] Clear sprites once we get any knock
 [x] Use sprites (or bg color cycle?) or something pre-knock
     to indicate that pi is booting
 [x] Assemble and test a board
 [ ] Make backup offline versions of slides
 [ ] Make demo and a video of it, in case it breaks!
 [x] try filter capacitors
 [x] scan converter working
 [ ] 
 [x] slide metadata / parser
 [x] non-hostile version of cart.nes (doesn't make screen red etc.)
  [x] for example, knock sequence can disable this?

Icing:
 [x] Version of PCB with keepout/drill for center screw?
 [ ] "fine tune" controls for timing, accessible from controller?
 [ ] on-screen laser pointer during slideshow mode (sprite?)
 [ ] music??

Backup plans:
 [ ] Second pi renders SNES frames, and just sends them to "video" pi
     during its vblank (wifi??)
 [ ] Kernel module
 [ ] Separate fast GPIO microprocessor renders the frame?

Worries and mitigations:
 [x] Run out of RAM for slides even?
   [x] Should be fine. Zero W has 512MB.
       300mb fits about 13,000 slides.
       Even with some animations and multi-screens, this is plenty of
       headroom to keep everything in ram.
 [ ] Pins on NES get pretty broken.
   [x] Order replacement 72-pin connector
   [ ] Backup NES?
 [ ] Operation is temperature sensitive?
   [ ] bag of ice / hair dryer (kinda funny anyway)

This doc rules:
https://www.raspberrypi.org/app/uploads/2012/02/BCM2835-ARM-Peripherals.pdf
addendum: https://matt.ucc.asn.au/mirror/electron/GPIO-Pads-Control2.pdf

Obsolete "todos":
Obsolete:
 How often can I change sprite ram?
 What are the PPU writes? Do I need to handle them at all?


 -- timing -- 

NES:
Measured pin #38 (M2 -- this is CPU) on the saleae.
Example pulse: .36 us high .198 us low, which is 1.792 MHz.
           ie.  360 ns,    198 ns
On the pi, a for(;;) loop just calling gpio_write (one pin)
gets 80-88 ns pulse times

this is say .09 microseconds, which means we can do about
4 bit flips per NES clock. this is cutting it a bit close
but maybe in the "not insane" range?
  danger:
    - can I set multiple bits at that same rate?
       at a minimum I need to read the address bus, then
       write the data bus, then wait for the falling edge.
    - should make sure that transistors/diodes to do
      bridge between 3v3 and 5v ttl don't introduce too
      much slew etc. The analog waveform at 50MHz is 
      basically a sine wave so we may be reaching some
      electrical limits (?)
    - there are periodic dropouts. How to set realtime
      mode or whatever? We could probably cooperatively 
      schedule during ppu vblank
    - do we have enough time left to prepare CHR data?
    - do we have enough time left to do some emulation, etc.?

hmm, write_multi on 4 bits gives me 80ns/200ns off/on
cycle with a grosser waveform. 

But it looks like the ARM is just executing too many
instructions. With -O2, and removing debug test from
peri_write, I get 48ns/112ns off/on and a sinusoidal
wave. Plenty of more optimization to do...

 - inlined peri_write (this version also actually packs
   bits into the pins written, so that it's not trivial): 52ns/124
 - inlined all routines necessary to do write_mask: 44ns/120
 - memory barrier before/after: 48/80
 - barrier just before: 56/100

 -- more NES timing: PPU --

- The PPU RD bit happens as a solid square, .18 ON, .192 off.
  (but see "gap" below)
- This square wave happens for 15.305312ms.
- Then RD stays high for 1.334072ms.
  Together these add up to a period of 16.634, which is very
  close to 1/59.94, i.e. "60" FPS. So when RD is high we are
  probably in vblank. If we could manage to schedule other
  tasks here and fill the framebuffer on the pi, we'd be
  golden?
  (during the vblank, addresses are usually stable but
   sometimes there are glitches. trust RD.)
- Looks like addresses change when PPU RD is *high*, by
  the way, meaning that the address bits are safe to
  read when it's *low*. (This concurs with the docs,
  which call it /RD)
- Gap: I see a gap in reads (high cycle for .368 us) every 63.5us.
- 15305.312u (full frame) / 63.5us = 241 almost exactly,
  which I think means that this double-read is the end of
  a scanline.

 -- switching d0 --

This is not working right.

I have a 4.7k pullup from 5v to DATA.
- DATA is on the Collector of the NPN transistor. [ch 2]
- PIN 26 is on the Base of the transistor [ch 0], after a 10k resistor
- GND is on the Emitter.

We should be able to pull this close to ground (+ transistor drop) by closing
the transistor's circuit. But:

    ch 0              ch 2
 26 set = 3.3v       about 3.8v
 26 clear = 0.0v     about 4.9v

so it's like maybe I'm only turning on the voltage drop?
AHA! Perhaps the transistor is simply backwards [laughing crying emoji]
because the pinout on TO-92 IS NOT STANDARD!

fixed! now:

    ch 0              ch 2
 26 clear = 0.0v    about 4.9v
 26 set = 3.3v      about .03v


 -- how the NES works --

74LS139 (CPU side) is a simple multiplexer (2 bit address in, selecting
which of the four outputs gets LOW logic level; others are high). (Actually
it has two such circuits.) This is hooked up to CPU A13, A14, A15.
(I think it is selecting between sram, etc.?)
I think that on the schematic, the "E"nable pin is labeled C. Note how
one of the Y outputs from the left side is wired into the right half.
This means that neither Y1 (ppu reg) nor Y0 (cpu side SRAM)
will be enabled (pulled low) unless the right half is enabled by the
left half. The left half is always enabled (C to GND) and is selected
by address bits A15 and M2. Together the left half controls whether
the right half is enabled, plus outputs to pin 50 (on cartridge),
which is /ROMSEL. So the CPU can signal the A15 bit to the cartridge
with every address, clocked with M2. Not sure what this is used for
exactly (it already gets A0-A14 directly, not to mention M2).

 -- cpu/ppu communication --

To write to PPU VRAM, you use memory mapped cpu registers;
two bytes to $2006 to set the address, then bytes written to
$2007 get streamed into incrementing addresses. (It's also possible
to increment by 32, which moves "down" since the screen is 32 tiles
wide.)

On the board, cpu has 3 address bits wired to the CPU to select the
PPU register (there are 8 of them), and an 8-bit bidirectional bus.
/CS is pulled low by that 74LS139 address multiplexer (not sure
in what conditions exactly) so that $2007 selects register 0b111.
Apparently the condition is that the address is from $2000 to $3fff;
the middle bits are just dropped. 

specifically, the address is:
001* **** **** *ppp
where * is ignored, ppp selects the ppu register, and 001 is setting
the range $2000-$3fff. The 1 here is A13. If you look at how the 74139
is wired, this makes sense, since the right half has A13 and A14 as
inputs (and is only enabled when A15 is low), and PPU /DBE as one of
its outputs.

 -- ppu memory mapping --

On the PPU side, this works differently:
The 2k SRAM chip (officially CIRAM, aka VRAM) is
enabled directly from the cart. Pin 57, aka CIRAM /CE, enables the
chip when low. The PPU RD and PPU WR lines are wired directly to CIRAM,
also connected to the cart traces.
A10, the highest bit of the chip's address space, is controlled by
the cart--not hooked directly into PPU. (I guess probably some carts just
wire this directly from PPU A10 though?)

Anyway, ppuppy should disable CIRAM (wire /CE high) so that it can
just supply its own data.

 -- wiring notes --

On zelda cart, PPU /WR is wired to /WE (write enable) on the SONY chip
on the top right, which looks like SRAM?
(http://www.datasheets360.com/pdf/4386973877628639512). I had
previously assumed this chip was PPU CHR ROM. It's 8k, so it would be
enough. But it makes no sense that it would ship in SRAM and lose all
the tiles as soon as the battery dies. But maybe this is not the
battery-backed SRAM.

Chip at top left is RP231024D, a ROM. It's labeled PRG. Could verify
that it's hooked up to CPU bus.

Chip at the bottom left is LH5164D, Sharp, which also appears to be
SRAM:
http://pdf1.alldatasheet.com/datasheet-pdf/view/42972/SHARP/LH5164A.html
(Maybe this is the battery-backed one.) Also 8k.

Anyway it turns out that zelda has a fairly complicated mapper, the
MMC1. I guess perhaps the chr memory is SRAM and gets filled by CPU,
reading from PRG ROM?. That's weird because I recall the zelda pattern
tables being very stable (only switching when entering the dungeon).
Oh well, maybe this is not a great test game...


So Ice Hockey is easier to understand. Both CHR and PRG are ROM and
the only other chip is CIC. The CIRAM is hooked up as follows:

 - /CE directly wired to PPU /A13. So CIRAM will be returning data
   (for reads) whenever the address is < 0x1FFF (because then bit 13
   will be zero and these are zero-enable pins). A13 is the highest
   address bit. When the address is 0x2000-0x3FFF, A13 is high, so the
   chip is disabled, and ROM can assert its values.
 - Note PPU A13 is also passed (not negated) to the cart. I bet this
   is wired to /CE for CHR ROM. (CHECK). That would allow these two
   chips to be mutually exclusive.
 - /OE directly wired to PPU /RD. This is done on the NES motherboard.
   So CIRAM will only output when PPU is reading, duh.
 - /WE directly wired to PPU /WE; same idea. Not much to do with this.
   If we really want to avoid writes to CIRAM, a simpler thing would 
   be to just disable the chip with /CE, which we control.
 - CIRAM A10 controls mirroring.
   https://wiki.nesdev.com/w/index.php/Mirroring
   On NROM (e.g. Ice Hockey) it is hard-wired to either PPU A10 or A11.
   (Ice hockey uses Vertical mirroring, where CIRAM A10=PPU A10.) This
   is just bit logic so that reading the nametable during scrolls
   "wraps" the way you want. Should be irrelevant for ppuppy since we
   won't scroll. Can probably leave unconnected, like Gotcha! game does.

So I think the next experiment to run is to read PPU A13 (or /A13)
and disable reads when A13 is low. This would properly mimic CHR ROM,
avoiding bus conflicts with CIRAM.

 -- ppu rendering --
Nice simple explanation from nesdev wiki (PPU nametables article):


Conceptually, the PPU does this 33 times for each scanline:

A. Fetch a nametable entry from $2000-$2FBF.
B. Fetch the corresponding attribute table entry from $23C0-$2FFF and
   increment the current VRAM address within the same row.
C. Fetch the low-order byte of an 8x1 pixel sliver of pattern table from $0000-$0FF7 or $1000-$1FF7.
D. Fetch the high-order byte of this sliver from an address 8 bytes higher.
E. Turn the attribute data and the pattern table data into palette
   indices, and combine them with data from sprite data using priority.

(Note: At the beginning of each scanline, the data for the first two
tiles is already loaded into the shift registers (and ready to be
rendered), so the first tile that gets fetched is Tile 3.)

It also does a fetch of a 34th (nametable, attribute, pattern) tuple
that is never used, but some mappers rely on this fetch for timing
purposes.

(Actually it is more complicated than this, though.
 Good detailed doc: https://wiki.nesdev.com/w/index.php/PPU_rendering
 There are 340 PPU cycles in a normal scanline, and each read takes two
 cycles. So we expect to see about 170 * 240 reads per frame, which is
 40,800.)

Note nametable would usually be RAM. We could disable this and supply our own
values, though. (This is probably why we see data on the data bus D0
all throughout the scanline, even when we aren't outputting anything.
This is not a vram write, it's SRAM asserting a value on that same bus.)

So, if for every tile it fetches the attribute table entry, it would
be possible to change the palette on every tile, not just every 4x4
block of tiles, right? AND! We could actually change it for each row
of the tile; basically each 8x1 horizontal stripe has to be from the
same palette entry. The major limitation here is just that we only get
2 bits to select the palette, so the screen only has 13 different
colors (background + 3 non-background colors per palette entry). BUT,
it should be possible to flip the palette on alternating frames,
blending colors to get 13*13=169 colors. That's a lot of flexibility.
Changing the palette dynamically would require cooperation from the
CPU (and needs to be done during vblank), so probably the best thing
to do is to pick two constant palettes that mix well to cover the RGB
space, and always alternate between the two. 16 colors is also not so
bad. The additional constraints that 8 consecutive pixels need to use
the same palette definitely adds some complication. Two fun color
problems:

1. Write a function that takes two palettes and 8x1 pixels, and
determines the two two-bit 8x1 stripes that minimize the color loss.
Probably the best results are global (like it's bad if a big solid
color has the color expressed different ways within it). Another thing
this could do is take an accumulated error as input, like
floyd-steinberg dithering.

2. Write a function that finds two optimal palettes for a given
weighted set of input colors. For any palette, the attainable colors
are the averages (and there may be some NTSC theory to this
averaging?) of any pair of colors (including a color and itself). The
loss is the (weighted) distance (like delta-e in LAB space) of all
input colors to their closest attainble color. You could compute
palettes for a specific image you want to display, or just do this
offline for the whole RGB cube and have a constant pair of palettes.

 -- syncing --
We expect to see about 40,800 reads per frame. at r3476, with swapping
disabled and running sudo, I do seem to get about this many reads:
15394215 edge, 376 frames, 40848 last sync, 7596268 839325 6134896 823726.
15435103 edge, 377 frames, 40888 last sync, 7616454 841532 6151186 825931.
15475850 edge, 378 frames, 40747 last sync, 7636578 843709 61
... being under the number is totally unsurprising (missed deadlines)
and we can be over from glitches, or the 40800 estimate might just be
wrong. So actually this seems pretty decent.

 why do I get MORE pulses (on other data pins) when I write 'sync' to output than
 when I write (sync & 1) ? 255 : 0??!
 
 -- more syncing --

Now on ice hockey. It seems that the writes are just taking too long.
Only writing when A13 is high, after PPU /RD goes low, DATA goes high
after 0.156us (A). That is fast enough to make the rising edge of RD.
Then from the rising edge of RD, it takes 0.164us (B) to turn off the
data. During that time RD went low again. We apparently do see that,
because another 0.164 us (C) later, the write starts. Now again, 0.164
(D) to turn off the data. Again we're into the next read-low pulse,
and A13 is off, so we are conflicting with CIRAM.

posterity/timing-late.png

(Actually: *IS* this bad? I am asserting the correct value on the
rising edge, even if I fail to shut off before the next cycle. When is
the critical moment? Maybe the timings are okay but just the
levels/impedance is bad? Could look at timings on other data pins to
see what they do.)

All these transitions take about the same amount of time, but the
reaction to RD going high comes late, because the width of the RD high
pulse is just shorter (.136us) than the low pulse .236. The fact that the
timings are usually the same suggests that there's just some latency
between setting a GPIO pin (or perhaps, reading it) and seeing the
logic level reflected on the output. 

Before we had a write time of <0.090 though. If we can get back to
that, we'd probably be okay timing-wise?

Another option is to just try to sync an internal clock so that we
predict the edges rather than reacting to them. It would be good to
be able to read the address that the PPU is asking for, but OTOH
they should be totally predictable. We could still use the addresses
to adjust the sync, as well as to communicate to PPU from CPU, we
just wouldn't be able to have the NEXT write depend on them.

So:
 - Study the timing of a real NES game.
 - Try to get the read or write latency down. (For example, study
   whether this is about making the C code fast, or if it's just
   inherent somehow to the BCM chip?) Would it be faster as a
   kernel module? Is there low-level support for interrupt on
   falling edge, for example? Can we clock the pi faster?
 - Try rewriting the loop as a predictive sync rather than reacting
   to logic level changes. Kind of a neat problem anyway. The signal
   is very regular, and addresses could give us absolute sync.
   Note that this may be hard to reason about if we have both read
   and write latency (could just assume they are symmetric values?)
   Also note that eventually, we won't have CIRAM storing the nametable.
   (Well, maybe we could have some discrete logic that only maps the
   attribute table to "ROM"--it's the last 64 bytes of each nametable,
   and we could rely on there only being one--but this may be kinda
   messy?)
 - Try disabling CIRAM -- we don't want it anyway -- and just
   generating all ppu data. We'll certainly still have a hard timing
   problem (and now the addresses will be gibberish?) but less confusion
   about bus conflicts. (Of course, games certainly still write to
   CIRAM and we would have the potential to conflict with those writes,
   but at least the corrupted values would likely have no effect?)

   Actually j/k: With CIRAM unhooked, we still get addresses: The
   nametable and attribute table reads should be from predictable
   (more or less sequential) locations. The pattern table (CHR) reads
   (which would have reflected the contents of CIRAM) are the ones
   that would be affected. These reads will directly reflect (well
   there is some simple function) what we returned from nameable
   "RAM", so we could actually use these to check that we're in sync!
   It would look like this:

   PPU reads nametable 0000 (top left corner).
   We return, say, 0000.
   PPU reads attribute table for 0000 too.
   We consult our current sync location, and return the attribute
   bits for that group of 8 pixels.
   Now PPU reads from chr+0000 (because we returned 0000 for the first
   read). We record the address, but just return whatever graphic bits
   we want at the current sync location. It then reads chr+0000+8 for
   the second color bit, and we do the same.

   But now: We can look at the addresses read the second time. We
   controlled the values (modulo some predictable offset), and can
   compare these to what we expect to make sure we're synced up. For
   example, if we expected to get back YYYY but instead got YYYY+1,
   then we adjust our sync position accordingly. In fact, if we were
   sending a predictable stream here, we could be VERY far off (maybe
   we had a scheduling blip) and still fix our sync. (Of course, small
   drift would usually misalign the four reads so that we are actually
   getting a read for a nametable location when we expect an attribute
   table read. I think most of the time we are adjusting sync this way.
   Fortunately these all have disjoint address ranges, so it's pretty
   easy to know which one the PPU is asking for. Main complication is
   the special cases that happen at the ends of scanlines (sprites,
   which also fetch some CHR) or on the pre/postroll scanlines.
   
   BTW: Even with CIRAM disabled, we can still get info from the CPU
   if it modifies sprite locations (this determines what scanline it
   appears on) or graphics (modifies the address of a CHR fetch;
   probably better). So I think I should just disable CIRAM and get
   into that.

ALSO, there appears to be a much better library. BCM library benchmarks
at about 5.4 Mhz (this approximately matches my experience; 1 / .164us is 6.09,
and I did some speed tweaks).
http://codeandlife.com/2012/07/03/benchmarking-raspberry-pi-gpio-speed/
Supposedly "native library" is 22MHz! Hopefully not PCM...

(This same page warns that you need pretty short leads to the scope
at high bandwidth, so we may be entering the realm where these longass
bell wires in the rat's nest are actually messing things up. Beware.)

Well indeed, gpio.c is way faster (at least just strobing a single pin):
I get 28ns high, 32ns low. That would be plenty fast. Not at all clear
what the latency is, though. With -O2, it's 16ns/16ns, which is like as
fast as ROMs.

Note: bcm library falls back to /dev/gpiomem if /dev/mem is unavailable
(not root). Check if this is a performance issue!

ppuppy w/ gpiomem: latency is ~200ns
ppuppy (root) w/ /dev/mem: latency is ~154ns. better!
Note that we also can't enable realtime scheduling without root. So we
should really always run this way.

 - accurate timing on pi -

There is st_read(), which reads microseconds from the system timer.
microseconds are already too long for this task! Also it has some
performance issues:
    //  - st_read does three reads of the clock to avoid overflow. Since
    //    we expect a small number of ticks here (could make this a
    //    requirement), we could stop doing that and just do some modular
    //    math.
    //  - st_read probably needs to do a memory barrier because the timer
    //    is a separate peripheral from gpio.
    //  - function calls have some overhead here.

So, trying a very simple loop with "volatile", etc. to disable optimizations.
This works OK. With a delay of "3" I can get a pulse time of 120ns. This
doesn't help the latency, obviously, which on this experiment was 258ns,
missing the rd window completely.

Moving address decoding, and incrementing of edges (this seemed surprisingly
expensive. Is ++ on a 64-bit integer a lot of instructions?

 - enabling all the GPIO -

"At reset only pins GPIO 14 & 15 are assigned to the alternate
function UART,"
(I think this might be out of date though?)

Looks like you can turn off the special function of those hat pins with
force_eeprom_read=0
in config.txt.

"Pins 3 and 5 are pulled up on the board to the 3.3V supply by 1.8 KΩ
resistors to enable them to be used easily as I2C communication pins."
hmm, really?
http://www.mosaic-industries.com/embedded-systems/microcontroller-projects/raspberry-pi/gpio-pin-electrical-specifications

 - disabling CIRAM -

I wired CIRAM /CE (chip enable) to 5V directly, which should disable
it. The video output seems to reflect this: Like the ice field is
reliably the same characters, seemingly in order (because we see
012345678 although not immediately followed by 9ABCDE_GHIJ_LMNOP).
Actually, does that make sense? When reading the nametable, it'll
actually get some of CHR ROM, which just means interpreting
character bitmap lines as character indices, which shouldn't
produce such regular results? It looks more like perhaps the address
is sticking around on the data bus?

Also weird: If I just read PPU D0, which has been cut from the rom
chip, and also from my output, it has lots of distinct clean data
on it. Where is it coming from? I thought CIRAM, but I *think* I
turned it off? Could it be floating and just getting interference
from nearby wires/traces/etc.? It doesn't look like pure noise,
but maybe that's possible.

Try next: Weak pullup/down on this line?

 - bus conflicts / open bus -

Trying to really understand out what's happening here. To recap: I
disabled CIRAM by pulling the pin to 5V, which since it's /CE, should
turn it off. Also, PPU D0 has apparently clean data on it, even though
I cut it from the ROM and from my intrusion.

This page has what I needed:
  https://wiki.nesdev.com/w/index.php/Open_bus_behavior

The PPU has two data buses:

1. The I/O bus. It connects CPU and PPU directly, and the cart pins
    labeled "CPU D*". This bus has multiple devices on it: PRG ROM
    (and potentially other cart stuff), on-NES CPU SRAM U1, CPU, PPU.
    There is some address multiplexer that decodes an address
    (U3, aka 74LS139) to enable different devices on that bus. For
    example, writes to the "PPU status registers" are really just
    writes with addresses that cause U3 to enable the PPU data bus
    (PPU 13 /DBE), and also pass along the low three bits of the
    address (which are connected to three PPU pins).
    Anyway, this bus is not relevant here.
    
 2. The video memory bus. This connects CIRAM, CHR ROM, PPU. However,
    the PPU does a dirty trick. PPU pins 38-31 are both address and
    data. These pins are wired directly to the cart (PPU D0-D7), so
    we can observe their state during the first phase of the address
    load. I believe that the sequence is like:

      A. /RD is disabled (high), etc.
      B. Assert high address bits. This enables or disables chips
         through their /CE pins. But output is not yet enabled.
      C. Assert low 8 bits of address on 38-31.
      D. (Also assert high address; not sure if this happens now or
          after the next step. Nothing funny with these bits.)
      E. Enable ALE so that U2 stores the address bits.
      F. Switch pins 38-31 to "input" aka "tri-state" or
         "high impedance" mode.
      G. Now enable /RD (go low). (Similar thing for writing, although
         it would need to first assert the data on 38-31.)
      H. This enables /OE for the relevant chips, which now assert
         their data on the bus.

    So, in the case that nothing is asserted, due to bus capacitance,
    the PPU just sees the low 8 bits of the address (the same values that
    it asserted) when it reads the data pins back. This explains why
    Ice Hockey just shows me the nametable (more or less): No chip
    responds to the reads of the nametable (CIRAM is disabled, but I
    didn't enable the ROM for these addresses!) so address 17 appears to
    contain character 17, etc.

    This also explains why D0 has data on it: I'm seeing the address
    written to that shared bus during phase C above.

This is also a good explanation of these concepts for a generic shared bus:
  https://en.wikipedia.org/wiki/Three-state_logic


So the question is: What to do about it?


 - tri-state output -

OK so I made a simple tri-state output with two transistors.
It's still taking too long from PPU RD to the pulse, which is
probably why the low bit flickers on-screen (it's pretty
close so probably sometimes it's making it and others not).

Just messing with logic levels outside of ppuppy:
 - until setting the gpio pins to output, note that the 3v side
   floats, and is influenced by the 5v bus, even through the
   transistor (backwards, ugh)
 - but just ./clear.exe on 5 and 6 will make this behave as
   expected.
 - manually ./set.exe 6 drives the low bit to 1. The bus still
   goes low at times, presumably because address bits are
   driven harder than our 4.7k pullup. (This is expected.)
 - in "float state" (5=0, 6=0), we get a stable checkerboard
   pattern on bit 0. This would be the open bus continuing
   to read the address bit, as expected.
 - driving low (5=0, 6=1) both reliably clears the bit 0
   pixels, as well as doubling pixels in CHR graphics (also
   the nametable). This is because we're driving the address
   bit 0 low. The 5V bus has noise on it but never gets higher
   than about 0.4V. This is because the output circuit just
   connects the bus to 0 (no "pull-down") and I guess what
   we're seeing leftover is the voltage drop across the
   transistor.

So: This is all completely as expected.
Running ppuppy gives crap results, though. This is probably
a timing problem!

 - timing of addresses / rd / a13 -

I tapped into those address bits on ice hockey.

 address-timing.png

Address bits come live as the PPU RD edge rises. (In this screenshot
they happen both before and after the edge (eg. 28 ns before, 7 ns after);
this is presumably because the voltage level is changing on the bus and
is only recognized as logic 1 or 0 at a certain point. 

Remember that the PPU first puts the low bits of the read address on
the data bus, then shifts it to U2. I can kinda see this on the scope:

timing-shift.png

Note how D0 goes high first (address being loaded onto data bus), then
RD goes high, then A0 appears with the same value. A13 goes high at
the same time, which makes sense since it is not latched on U2 (PPU
just writes it directly at the same time). But otoh A4 also goes high
earlier; this should have been latched by U2, so maybe the first
hypothesis was right; they just take different amounts of time to
rise/fall. (To test this further, I could hook up to PPU D1-D4 and
look at how they follow A1-A4 too.)

Anyway: Address bits change close to the time RD goes high.
(So we could look for an RD high edge, wait a few ns, read and decode,
compute our value, then write. This would probably move us up ~100us)

Aha! Oops! I can't really look at when data is typically written,
because although there is a shared bus for address and data, that's the
PPU D* pins, but I'm looking at PPU A* pins here--those only have the
shifted address data. This makes the scope readings make sense. Need
to do some more soldering.

OK: So, data ends up on the bus from CHR rom about 40ns after the
falling edge:

   timing-dataout.png

and it stays there until about 196ns (32ns before the rise). Here the
PPU is driving the data bus low so that it can put a 0 on it for the
next address.

So I think the lesson is: After the falling edge, get your data
there fast ~30ns??) and then float before the next rise, because
the PPU uses that bus for shifting addresses before the cycle is
up.

So I did a total hax to try to time it on the rising edge (also inverted
the logic for A13 -- check if this is now correct, or just shifting the
timing a lot!). Now compare phenomena:

 with set.exe 5, we get a solid row on the right side of each tile,
 but also each character is doubled (because address is driven 11 for
 nametable reads as well)

 with ppuppy.exe 5, putting aside all the jitter, we get the solid
 effect on the right column of CHR, but the nametable reads are still
 just the address--unaffected by our output. great!

 on the scope, these pulses are very short:

   first-working.png

 but that seems to be enough for the ppu to see HIGH, though. It also
 works to drive low. So I think we're onto something here!

 - syncing -

In the steady state, the PPU does four reads per 8-pixel
strip (i.e., the slice from one scanline). Let's call this a packet:
  A. character from nametable
  B. attribute byte from attribute table
  C. pixel bits for corresponding CHR (low)
  D. pixel bits for corresponding CHR (high)

The address of the nametable read depends on the coarse scroll
value, and the nametable selected, but otherwise is just the
tile offset (32 tiles wide, 30 tiles high). We should be able
to avoid thinking about scroll.

The attribute bytes immediately follow the nametable, but the same
attribute byte address applies to two consecutive blocks (in both x
and y).

Then we do loads that depend on the character returned from the
first read. We fetch (tile * 16) + row, where row is like (scanline
& 7). Then we fetch that same address plus 8.

The addresses of the first two reads are computed by the PPU itself
and so we could use them as waypoints for syncing. Unfortunately, 8
consecutive scanlines read the same values, since the same
character is used for each scanline (and attributes are even
coarser). However, this can tell us roughly what scanline we are
on.

A better wayfinding technique is to "ping" the PPU to measure
latency. When the CPU asks for a character, it's going to read
something from the bus, but we don't care what value it reads,
since it's just going to use that to ask us again for CHR. So
what we could do is pass some timestamp to it, and look at the
addresses requested for reads C and D. We can compute the tile
value from these (addr / 16), and even get to double-check since
the 

Maybe this coarse jitter is not a big problem if we just prevent
ppuppy from being interrupted. But a small amount of jitter is
probably expected. It's easy to tell A/B from C/D because PPU A13
is high for the first two reads (from "VRAM") and low for the
second two. An easy way to verify a packet is that D - C should
be exactly 8 (for any packet) and (D >> 4) & 255 should be the tile
index we returned for A (when we are synchronized).

I tested out the new version of ppuppy. The timing looks decent:

  delayticks8.png

This is with delayTicks(8) called between the set_multi and the clear.
Note 96ns between the rising edge of the 3v side and the rise
on the 5v side; I guess this is just transistor slew? Should check
this again with the bus transciever.

But anyway the sync is TERRIBLE. It only counts 0-3 scanlines per
frame, usually 300-500 desyncs, and usually overrunning the packet
buffer. Are there just bugs? Time for a run.

Simple idea: use the nametable read addresses (gives us rough
scanline), then if we think our sync is good (because we get a
matching pattern read), use the low bits to establish the current row?
"good sync". Need to actually read more address bits, though.

 - kernel module -

I definitely haven't solved the timing issue above, but it's starting
to look tractable. The biggest problem is whole-screen glitches, which
happen about once per second. If I could maybe disable whatever is
doing that, I'd probably be "good enough."

 - "simple idea" -

Ugh, it still doesn't work!

 - "simplest idea" -

Just turning off the memory barriers does seem to give us enough time
to do address-dependent writes, so just do it that way.

This may actually be more stable than I thought -- if the output byte
(e.g. attribute, but also color bits) is just like fine & 1, then
there's clearly some blinking, but it's also clearly doing something
repeatable. But if it depends on the column, I get a lot of noise.
(Maybe trying the color attribute is easier to see than color bits,
for some reason?)

Although the sync is pretty bad, I suspect that this is failing more
because the output drive is not strong enough. This is why we see
a lot of noise. The voltage on the 5V d0 is only like 1.7v.

Try next: Use a stronger "pull-up".
(alas did not work)

Try: Just rudely disable interrupts??
Try: XinuPi?

 - disabling interrupts -

It's likely that doing this puts the machine in a broken state.
But it may actually be exactly what we want, to just enter a loop
where the processor is only executing instructions to serve the
gpio pins and can never return to linux.

Anyway, both the ARM core and the BCM2835 have some interrupt
capabilities. On the ARM, you can just do this:

   mrs r0, cpsr  ; save status register into r0
   cpsid i       ; disable interrupts
                 ; that's d for disable, i for interrupts?

(actually this looks sorta wrong because what if an interrupt
happens between the two instructions, and that interrupt changes
what interrupts are enabled?)

and you can then put it back:

   msr cpsr_c, r0  ; restore status register

(or explicitly enable as cpsie i)

Presumably this can't be done from user-space, but there may
be a kernel call for it. Or we write a kernel module.

Disabling IRQs from the bcm chip is actually much easier,
since it can be done from the memory mapped peripheral.

http://embedded-xinu.readthedocs.io/en/latest/arm/rpi/BCM2835-Interrupt-Controller.html

linux header lists the IRQs and other stuff:
https://github.com/raspberrypi/linux/blob/rpi-3.6.y/arch/arm/mach-bcm2708/include/mach/platform.h

might need to disable watchdog timers? It's a peripheral.

 - it's not working -

There may be a combination of issues here: For example, we may be
getting desynchronized because of timing AND because the address
lines are noisy. (Note that they are not even supposed to be stable
until the falling edge...)

OK, I think at least one problem is that these BJT transistors have
crap rise/fall times (tens of milliseconds), so my output stage is
just not able to switch fast enough. Turns out that in the 50s they
discovered this and built "baker clamp" circuits to prevent the
transistors from saturating (by using diodes to provide negative
feedback, though I don't totally understand what's going on). Later
they started building these diodes into the transistor package,
creating Schottky transistors which became part of the logic
family called Low-Power Schottky Transistor-Transistor Logic,
aka LS series chips. Note there are several LS* chips on the NES!
So I think we need to be using real chips for this, not just 
futzing around with homemade transistors. Good news is that these
chips are ubiquitous!

 - reducing jank -

Can disable HDMI etc with:
tvservice -o
Also: 
vcgencmd display_power 0
Turns off video output.

Could probably unmount the filesystem upon program startup,
 - could disable wifi I'm sure,
 - maybe also disable swap?
Turn off USB power:
echo 0 | sudo tee /sys/devices/platform/soc/20980000.usb/buspower >/dev/null
Can save 2mA (!) by turning off the LED.

can kill lots of processes:
 - bluetooth 

I think maybe killing /usr/bin/dbus-daemon --system hosed wifi though?
or dhcpd?

Good news: Forcing all of the interrupts off does leave the CPU loop
running, and I can clearly see the expected outputs happening on the
scope. This also does yield MUCH more stable behavior. Specifically,
the obvious periodic jumping at about 1 Hz is completely gone. Since
the output is not at all working, though, it's not clear if there may
be much smaller timing problems persisting.

Of course, this is a bad developer experience since Linux is
completely hosed after I do this (no debug printing, no wireless,
probably no filesystem or anything interesting; need to hard reset to
connect again, and so on). I may also need to avoid using some basic
stuff like malloc, since these often do use stuff like memory faults
in their implementation.

(It might be possible to turn them back on periodically and have the
system recover, but I don't actually know which ones were enabled to
start with, and there doesn't seem to be a documented way to get
that?).

I don't really see any problem with this for production though. It's
actually ideal to boot into linux, then once we're ready, just become
an embedded microcontroller and run instructions in a loop with simple
timing.

There may be a watchdog timer or something that resets the system if
it's in this state for a while. But I at least ran it for well over
50 minutes without any obvious failure.

 - using bus transciever -

I wired this up. It's set in the A -> B direction (DIR=1) and I just
wired pi pins directly to the inputs, and bus pins directly to the
output, which is refreshingly simple.
 - At first I didn't cut all the ROM pins on the bus. So they were still
outputting values, but the new chip seems to drive the bus harder,
which is good. (?)
 - Then I cut the low three bits, but left the fourth, which may help
   diagnose problems.
 - But also, the tiles and colors also got corrupted, which is not
   expected -- these were formerly provided by stale bits on the
   bus (from the read address). They shouldn't be affected here
   because the bus chip is only enabled when PPU /RD is low (wired to
   /OE). It's likely that the chip doesn't disable fast enough to
   avoid sinking current on these lines as the PPU sets the address
   bits. This may be okay (we shouldn't need to rely on bus capacitance)
   but it's not great...

Clues: Setting all of d0-d3 as input causes heavy bus noise. I guess
this is not that surprising, since the transciever may just be sending
"floating" values on the bus. It's weird that if I only set one to
input, the screen still looks stable.

Reading address lines doesn't cause instability, on its own.

Oops, for lots of these experiments, I had the pulldown set on the
output channels. I don't think this makes sense. Turned it off.

Works much better. At this point I have a noisy bouncing ball demo.
I suspect the major remaining problem is that the addresses are
not high quality. Let's switch to the level shifter since that
should just be a better way than the voltage divider.

Actually now that I look at it, the hex level shifter has like
140ns-280ns propagation delay times (!), which is probably too slow for
this application (?).

Looking at the scope, the voltage divider approach doesn't seem to
introduce any delay at all, which is great. But oddly, I'm seeing that
the 5V logic levels are actually peaking at like 3.8v, with the 3v3
level then down to like 3.2. PPU RD is like 2.1v on the 3v3 side, and
3.3 on the 5V side. 
 - 5V rail is 5v as expected.
 - data (d0) looks as expected: with 3v pegged high: 3.3 on 3v3, 4.5 on 5V.
 - weirdly, a0 shows 3.6v at the same time that d0 shows 4.4v. (This is
   true regardless of whether the bus transciever is off (rd high) or not.)
   
voltage.png

These lower voltages are more glitch prone, right? Why is this
happening? Maybe there is a (resistive) short somewhere? Maybe the CHR
ROM chip is sinking too much current on the address and RD pins? I
might as well cut those pins.
 - cut a0 and a1 on the ROM. didn't seem to do anything.

On the mostly unmodified zelda cart, PPU RD peaks at 4v. Digital waveform
also looks perfect.

So I guess maybe the voltage divider itself is pulling the peaks down?
It's equivalent to a 3k pulldown (even more if the 3v3 input is sinking
significant current? but that's like 40k?). If these signal lines have a
10k pullup, say, and here we have a 3k pulldown, that would be like a
23% reduction in voltage if these lines are just driven by the pullup
(i.e. in common-collector mode, which I think is the case). (And on
the NES schematic, the visible pull-ups are indeed 10k).

CONFIRMED. Zelda PPU RD was 4V before, and with 3k resistor to ground,
it's 3.4. So I think I should use higher value resistors for the
voltage divider if I go that route.

Then I tried 4.7k + 2k. In the center I get a peak of 2.37v; on the 5v
side I get 3.7v. (This was 2.1/3.3 with 1k/2k resistors.) This is also
pretty far off. 

20k + 10k: 3v3: 1.8v ?! ; 5v: 3.9v.
Not sure what went wrong here? The 5v side makes sense to me (nearly
restored to 4v) but the 3v3 side, measured between the 20k resistor
(which was two 10ks in serial) and the 10k resistor had a strangely
low voltage; the peaks seem to lag the 5v peaks, and the troughs
are at a minimum of .482v, even though on the 5v side they go as
low as negative.
Between the ground-side 10ks, a peak of .914v, trough of .216.

10k + 4k: 3v3: 2.09v   5v: 3.7v

3v3 peaks of 2.09v will only barely trigger the input, so this
is no good.
troughs are at like 0.196v, which is perfectly acceptable

spreadsheet:
https://docs.google.com/spreadsheets/d/1KEUBxf1N64e4Ou1zlXg2AiTcm0OBiHcpzixwCrBVZYQ/edit#gid=0

hmm, none of these are good.

Some options:
 - find good level shifter with fast propagation delay 

 - test the ones you bought; maybe they outperform at 3v, or maybe
   100ns is (barely) ok.

 - note that we are reliably dropping the voltage on RD to the range
   that's acceptable for CMOS. So a really crappy way to do this is to
   just put a 3k "pulldown" resistor on the RD line (and address
   lines, although we'd need to measure each one to understand its
   characteristics) and then connect the RD line directly to the
   3v3 input. This is the most dangerous (in principle this could
   go as high as 5v if driven by something with more current) and
   also just terrible engineering.

 - diode-based level shifter.
                                         3v3 out
                                          |
   GND ---<|--+--<|--+--<|--+--<|--+--<|--+---'\/\/\/'--- 5v
             0.6    1.2    1.8    2.4    3.0    4.7k?

   diodes have a like "fixed" voltage drop, so 5 (!) in series
   here would give us a reliable 3.0v reference while having
   a total series resistance of the final resistance, which
   we could choose to be pretty high so as not to pull down
   the 5v line too much. 
   Datasheet for 1N4148 (from Vishay; I have knockoffs) gives
   recovery time of 4-8ns, which would clearly be adequate.
   (These list the forward voltage as "max 1v", though..)

Tried this, with 4 diodes (multimeter read them as 7.11, which
I think is voltage drop?) and a 10k resistor.

   3v3: 2.0          5v: 3.8v
   trough 0.8v      trough negative

this is not enough; what about one more diode?

   3v3: 2.2          5v: 3.85v
   trough 0.93v      trough negative

also it's pretty obvious that this is causing some
delay and flattening of the curve, worse with each
diode drop. (Actually, honestly, the levels are low
enough that it could even be crosstalk between
channels on the scope.)

Tried the level shifter. Wired both sides to 5v
for simplicity. As promised, the delay is high
at 98ns. (5v side is 3.9V, output peaks at 5.4v,
which is even higher than VDD; a bit unexpected
and possibly dangerous for 3v3?)

This might be workable, although this delay
definitely makes the timing tough; it's more than
half of the time that RD is high (98ns before we
see the rising edge; it falls 82 ns later and
we don't notice for 124ns. Bleh. But it's still
within one cycle, and electrically it looks
pretty nice. 
Alas: If I provide 3v3 on pin 16, I don't get any
useful output! (I didn't try this with a good
quality 3v source, to be fair, so maybe I'm not
supplying enough current.)

 - deglitching inputs -

Wow, this helped so much! First I made every input loop (e.g. waiting
for RD high) vote over the last three inputs. This noticeably helped.
The big difference was just inserting several more reads after RD
goes high--I guess the addresses just aren't stable until that point.
With this, the ball is clear at any position on screen, and almost...
tolerable! I can even switch the palette every scanline and it's
(kind of) stable.

The attribute table has some blocks that are never modified, but this
makes sense, since attribute bits are packed into bytes and the
current output only affects the low 4 bits. If we ever want to change
the color of those cells, we need to be outputting all 8 bits, which
is, obviously, the plan.

Maybe I don't even need better quality inputs with this technique.

After separating the two bitplanes, I notice that the second one
(packetbyte=3, the final byte) is much sparser looking. It could
maybe just be an illusion because the colors are not as bright(?)
but it looks like it's not affecting the color bits but rather
the next nametable read?

 This is very sensitive to timing!
 - Let's pre-render a table during vsync and just index into it,
   instead of doing any computation in GetByte (this is eventually how
   it will work anyway). Can pre-encode bytes.
 - Can optimize address decoding.
 - Address glitching is probably the biggest remaining problem,
   so if the level shifter just gives us stable values right off
   the bat, this approach will likely succeed.
 - There appears to be some hysteresis control (input) for GPIO.
   (it's in the bcm header but not the peripherals doc?). Default
   is hysteresis enabled (according to bcm2835.cc).
 - Turning on or off slew-rate limiting (output) may help as well.
   Default is slew-rate unlimited (according to bcm2835.cc).
 - May also consider changing the pad drive strength. It defaults to
   8 mA. The bus transciever is in the uA range.
 - Rising and falling edge detection might be another way to get
   more solid readings without manually coding it.
   (Specifically: This may be a lower-jitter way to wait for RD to
    go high or low.) Note there are both synchronous (using system
    clock, deglitching) and asynchronous (probably using schmitt
    trigger or whatever in hardware).
    

 - rewrite to screen/demos -

 Pretty sure that I'm just missing the deadline for writing the packet
 bytes. If I just write "hi" (last phase) bits, then I see the
 bouncing ball but in the sub-columns where I'm not outputting data
 (because the writes are actually affecting the nametable address or
 read; the next packet byte). If I just write "lo" (second to last
 phase) bits, there's very good fidelity within the circle, but it's
 red, which is palette entry 2 (hi bit set, lo bit not) on this screen.
 So lo bits become hi, and hi becomes nametable.

 Optimizing:

 DEGLITCH_READ is well-optimized. Two in a row is just

        ldr     r1, [lr, #52]
        ldr     r2, [lr, #52]
        orr     r0, r2, r1
        orr     r4, r0, r4
        and     r4, r4, r3

 and three is

        ldr     r4, [lr, #52]
        ldr     r1, [lr, #52]
        ldr     r2, [lr, #52]
        orr     r0, r2, r1
        orr     r3, r0, r3
        and     r3, r3, r4

 We might want to interleave the reads more, since spreading
 out in time is the goal, not simply to do a lot of reads. This
 would probably need some inline asm. Anyway. It's generating
 fast code.

 Address decoding is crummy though. It's like 26 instructions.
 We could make this faster by:
   - hand-optimizing (some shifts can be done together)
   - using tables
   - rewiring (putting the bits in more correct order on the
     pins will help)

 even though they're all mixed around, pins 2-27 are all
 available on the board.

 so we could reserve 2-3-4-5-6-7-8-9 for data out
 and then use the next 10 for address in,
 and the address decoding would just be like 2 or 3 instructions.

 - getbyte looks reasonable, although there's like if-else for
   the packet. Could instead compute a base address and offset.
   (did it)

 Tried a "best-case" timing experiment. Always write FF if A13
 is low. Takes only about 8 instructions from the deglitch to
 the write.
  - Result: goes high 152ns out of 192ns window. Doesn't
    turn off until the very end of the next window. Screen is
    a mess.
 Next, moved the output clear right after the RD loop; before
 the deglitching reads. This goes low 248ns after the RD edge
 goes high, which is 72ns after RD goes low (good enough to
 make the window). But that is literally the first instruction
 after exiting the RD loop.

 So this tells me that I need to go back to that old approach
 of computing the NEXT write byte.

 OK so that kinda works! I get a circle that's clearly grey
 (color 1 so low bits are working) and one that's magenta
 (color 2 so high bits are working) and in the intersection,
 orange (color 3 = both). There is significant noise in
 the color columns, though.

 at r3518 I have a pretty stable two-bouncing-balls demo. the top few
 rows are unaffected (am I missing vsync? kinda looks like it because
 if I turn off interrupts--which skips the call to Yield--it gets
 better) and there are some columns with noise in my color bits. They
 seem fairly regular so they are probably due to bad address inputs.
 Also worked well to modify the palette.

 addr decode before:
  lsr r3, r7, #14
  mov r4, r3
  lsl r5, r7, #17
  and r4, r4, #4
  and r3, r3, #2
  orr r3, r3, r4
  lsr r4, r7, #14
  orr r3, r3, r5, lsr #31
  and r4, r4, #8
  lsr r5, r7, #14
  orr r3, r3, r4
  and r5, r5, #16
  lsr r4, r7, #21
  orr r3, r3, r5
  lsr r5, r7, #14
  and r5, r5, #32
  orr r3, r3, r5
  lsl r4, r4, #7
  lsr r5, r7, #14
  and r5, r5, #64
  orr r3, r3, r5
  uxtb  r4, r4
  lsr r5, r7, #14
  orr r3, r3, r4
  and r5, r5, #256
  lsr r4, r7, #14
  orr r3, r3, r5
  and r4, r4, #512
  orr r3, r3, r4

  after:
  lsr r0, r7, #14
  uxth  r0, r0

  much better :)

 - remaining blinking:
   every eight columns, there are two columns that don't work right.
   I can't quite tell if this is the rightmost two? Actually just
   naively counting, there is like - - - - - X O -
   where X is the worst and O is also bad (it shows a sort of repeating
   vertical structure as well.)

   whenever the grey ball is on the top two instances of these, the glitching
   areas become solid magenta or orange. The bottom instance is instead black
   (inner part) or magenta (outer part). This kinda suggests a periodic
   timing problem, but it's pretty weird to have a timing problem here,
   since what is different about these columns (or anything other than the
   edges?). And since these columns are pretty blinky, I think they are
   likely caused by something more annoying, like electrically noisy 
   address (??) lines. What do these locations have in common?
   Could it just be as simple as I'm unable to set those palette entries to
   0 because I don't have the data lines outputting?

   I wired up the rest of the data lines, but the CHR pins are still wired
   too. The overall image quality is quite good, with still some noise
   lines (interrupts?) and now just two bad (but not disastrous) columns.
   Could it be like jitter that just happens to catch up with us there?
   If I could get rid of this, I'd be completely satisfied with the video
   output.

   I disabled the entire CHR chip: Its CE pin is usually wired to A13,
   so that means it's /CE (active low). So I wired this directly to 5v,
   which should leave it permanently disabled (might still have some
   electrical effects on the address pins that are hooked up?)
   
   This did not work right -- it looks like it ruined the A0 pin somehow,
   fixing it high or low (my own output looked doubled). Could have been
   some other strange effect from electrical differences, I guess?
   Disturbing how brittle it is.

 -  Palettes and sprites  -

  Where is palette ram exactly?
  Supposedly 0x3F00-0x3F1F are the palette entries (16, first the
  background palettes, then the sprites).
  According to PPU memory map,
   "$3F00-3FFF is not configurable, always mapped to the internal
    palette control."
  So I guess it's actually in the PPU chip? Makes sense since
  disabling CIRAM and PRG still gives me the standard Ice Hockey
  palettes.

  Additionally, there's the OAM internal memory. This is not mapped
  to any address.

  Anyway, we'll make use of these. The only way to change palettes
  is from the CPU, 

 - PRG mapping -

  With the example ROM, the program gets mapped at 0x8000 as well as
  0xC000, which is probably some mirroring thing.

  https://wiki.nesdev.com/w/index.php/CPU_memory_map

  Short version: ROM is usually from 8000-FFFF.
  You need to put interrupt vectors at the end of address space:
   - 0xFFFA (16 bits): NMI
   - 0xFFFC (16 bits): Reset
   - 0xFFFE (16 bits): IRQ

  So I guess that the "reset" vector actualy contains 0x8000.

  Actually j/k it contains 0xC000, but we also know the program
  is there. 0xC091 contains both the IRQ and NMI handler.

  And indeed: reset.s contains the code for these, plus a segment
  called VECTORS with their three addresses. nmi/irq just rti
  (return from interrupt, I assume). The reset one does some
  initialization and then calls main.

  OK, so this totally makes sense.

  8000-FFFF is 32k of address space, and since the PRG is only 16k,
  this must be why we see it mirrored inside FCEUX. The ROM only
  contains it twice, duh.

 - Reading controllers -

  There are memory-mapped registers for these:
   $4016  JOY1  Joystick 1 data (R) and joystick strobe (W)
   $4017  JOY2  Joystick 2 data (R) and frame counter control (W)
  (Note that they don't just read a byte out -- you have to shift
   out all 8 bits.)

 -- it stopped working --

   EEPROM stuff working well, although there's something weird about
   the PPU side on this cart. Pushing on it--or even just putting my
   finger on the CHR ROM--causes visible artifacts. Could be bad
   contacts, or maybe I even broke the PPU? Zelda works fine, though.

   I didn't use it for a few days (working on EEPROM) and now it
   stopped working. It looks like I don't get the PPU RD signal,
   because ppuppy.exe hangs, and the only way this would happen is if
   PPU isn't toggling high and low. I also observed this on the scope.
   It could be because this particular cart is now busted (?) but it
   still renders Atari-style hockey before I start ppuppy, so that
   sounds unlikely. Something happened to the resistors? Are they
   dissipating too much power? Is there too much "fan-out" post
   voltage dividing? Just a bad connection that suddenly failed, or
   solder/wire piece causing a short downstream?

   I verified that it still toggles on zelda cart.

   Anyway I was going to switch to using the level shifters, so let's
   just forge ahead on that (?).

   I wired up just /RD. I can verify on the scope exactly 4ns delay on
   the rising edge and 6ns on the falling edge. This should work fine
   timing-wise. Annoyingly, the waves peak at like 3.66v (over 3v3 for
   ~60ns), which is not supposed to happen -- I checked that vcc is
   3.35 here. 3.66 is a little out of spec. It's only hooked up to /OE
   on the bus transciever, so maybe it gets a little lower when hooked
   up to the pi too. Let's just try it I guess.

   Well, it boots and now shows a picture, although it's a bit jumbled
   maybe because some other address bits are not working any more. Maybe
   the same problem I had with RD happened to other addresses?

   Tested reading pins with read.exe:

.... .000 0000 0001 01.1 .... .... ....
.... .100 1111 0000 10.1 .... .... ....
.... .100 0010 1000 00.0 .... .... ....
.... .100 0011 1110 11.0 .... .... ....
.... .000 0000 0001 00.0 .... .... ....
.... .000 1000 1101 10.1 .... .... ....
.... .000 0000 0010 00.1 .... .... ....
.... .000 1111 1100 11.0 .... .... ....

    Of course pins 24 and 25 are always 0 (these are not hooked up
    to anything). All the rest at least show the ability to toggle
    high and low, so that rules out a pure disconnect problem. I
    don't even see any adjacent pins that seem to always share the
    same value or anything like that.

    Continued wiring up to use the level shifter. Definitely still
    seeing noise, but maybe it is getting better (or maybe the console
    or air is just getting warmer / more humid / etc., ugh). Remember
    that just because the on-screen noise is "high frequency", it
    does not mean that errors are necessarily on the low bits of the 
    address line--an error in high bits would just cause us to return
    the wrong part of the screen. I'm not sure I can really characterize
    this noise, though. Is it scanline, column, other?

    With all the level shifters wired up, the noise is still there,
    though it is tolerable. Just pushing on the cart a little can
    cause various effects, so I'm thinking that some pins on the
    connector might be busted (I mean, I know I bent some) or that
    I've even damaged the ppu. Oh, well.

    Looking at the scope, there are small notches on address lines
    near (even shortly after) the rising edge of /RD; looks like some
    temporary small voltage drop. This may be confusing our address
    reading code, since that's about when we read it. Voltage drops to
    like 2.2v on the 5v side, which is a logic 0! There may also be
    spikes. Otherwise these look like you'd expect.

    Let's just live with the noise for now. Maybe it just goes away
    with a PCB or a new NES. It shouldn't get in the way of developing
    and in fact forces the code to be more robust.

 - with PRG -

    It works as expected, but the frames toggle really quickly?
    Does the PPU count many frames during vblank because it takes a
    long time for us to do writes, maybe? Time to get the scope.

    Well, yeah. The good news is that everything is finishing in
    vblank, at least. There are 21 reads here, as expected: 3 knocks,
    2 joy writes, 16 palette reads. Note that 2 of those palette
    reads can be skipped if we complicate the CPU code a little.

    Timing: 90.704us (that's 90,704 ns!) before the first read.
    10.616us between knock reads, 23.468us between palette reads.
    This is long enough that ppuppy assumes it's entered another
    vblank, I think!

    After that are palette writes (and some debugging nametable
    writes). We finish around 0.81948 ms into vblank, with only 0.51
    ms left. This is all fine as far as the NES is concerned; I don't
    think we need to do anything else (sprite OAM?). But it means that
    ppuppy (PI1) is going to be mostly tied up in loops waiting for
    the RD signal even during the time when we'd normally be able to
    idle. If we're going to do any communication with another pi,
    then we may struggle to find time. :/

    knocking.png

    Anyway: We can improve the performance of this part by unrolling
    loops, at a minimum. But doing this stuff from the CPU is just
    slow. I'll come back to it.

    
    Next up is the vblank knocking.

    First attempt at syncing yielded this:

  16320 frames. joy: 00 / 00
    332840830 145865 16255 16237 16237 16237 16236 16236 16235 16235 16234 16234 16234 16234 16234 16234 16234 16234 16234 16229 16135 0 0 0 0

    Joystick reading (at least joy1) seems to be working, yay.

    Of course once we get to the palette writes, we usually make it
    all the way through the knock. The fact that we sometimes don't
    must be because we got desynced and then entered vblank? Probably
    it's bad for the palette reads to happen in knock addresses, since
    they can then be confused for the original knock sequence.

    This is very hard to debug! It's hard to think about the
    sequencing of writes with the delays, and the timing of the rising
    edge relative to the next real read is suuuper different in
    vblank.

    Note that if I change it to always write 0x27 during the normal
    sequence, it does accept the knock sequence in cart, but also the
    palette is not all the same color (right?) so it's not a total
    desync. It's like only the knock acknowledgement doesn't make it?
    Just a fallthrough bug?

    Clues:

     - It's really sending the palette, but it's not aligned.
       Incrementing index 4 doesn't change fine scroll, but if I
       increment the entire palette, then I get dancing colors and
       scrolling.

     - OK, palette index 2 is being treated as the scroll byte, so
       that means I'm two bytes where I think I should be. So weird. I
       guess it makes sense that I need to be sending the ack all the
       time, so that the last byte read is actually a leftover from
       the normal frame.

    It would be smart to figure out what's going on here, since it's
    possible that I'm being dumber (or luckier?!) than I think with
    the normal frame rendering. But for the sake of progress, I think
    the simplest thing is to add an additional knock at the beginning
    of the cart sequence, and always reply with KNOCK_ACK when in
    range (this is harmless because the knock address is a nametable
    read, which is ignored). Then I can just manually account for the
    large delta.
    

    (Interestingly: scrolling is completely smooth when interrupts are
     disabled. Maybe the reads in this phase are slow enough that we
     can completely avoid noise? that would be nice since errors here
     cause full-screen disruption)

    OK, now cart knocks two more times. I was able to get it to
    synchronize by just offsetting the writes until they're in the
    right slot. Please come back to this and figure out why it's
    delayed so much!

    Joystick reading is good; pretty stable actually.

  - SNES -

   armsnes is reasonable to use. Benchmarking SMW (intro movie) with
   snestest.cc, I get 7.8ms per frame by default on the Zero W, which
   is workable. At 60fps I have about 16ms per frame, so this would be
   half of it (and it is not at all clear how I would interleave this
   with ppuppy's basic operation--unless we got it reliably under
   1.6ms so the whole thing could run in vblank).

     - If I could get this running on a separate core (e.g. Pi 3) and
       just like communicate with shared memory, but also be able to
       disable interrupts, I'd be done. (Does armsnes need interrupts to
       run?) Alternately, some other way of preventing linux from
       descheduling the ppuppy core would work, and perhaps be ideal.

     - I can certainly optimize by turning off sound, and also
       probably by making the video routines faster. (But don't forget
       to account for time to convert raw colors into PPU colors. SNES
       probably uses palettized rendering, so outputting this format
       directly would save an intermediate step, plus speed up the
       dominant/similar color code.) I could also overclock the ARM
       (I think all the way to 1ghz is supposed to work) to squeeze
       a bit more. But I'm off by like a factor of 8 here, and most
       of the time is probably spent in the emulation core!

     - Perhaps emulation can be manually interleaved, like, run a
       single scanline while ppuppy is waiting on I/O. Since nametable
       reads aren't used, we could perhaps execute the scanline instead
       of bothering to write, multiple times per frame. There are also
       many reads at the end of each NES scanline that we can ignore.
     
   Turned off audio mixing in libretro: 7.02ms/f
   Looking at strace, there don't appear to be any syscalls during
   emulation. Awesome. APUEnabled = false, etc.: 6.5ms/f
   
   Same on Pi 3b+: 1.7724ms (this is very nearly in range of what we
   could do in one vblank, if we improve the performance a little
   and/or overclock! In fact we could probably just do this and be
   a little late when rendering the top of the screen...)
   With -O2: 1.7087

 - pi3 on nes -

   Ugh! 80 wire ATA ribbon cables are NOT suitable for this! They
   connect a bunch of pins together because they are ground wires
   in ATA/IDE, and also one of the pins is cut for master/slave.

   40-pin IDE should work, though, if I can find one of those. The
   key pin is GND on a pi header (and not the one that I connect to),
   so it's okay to just cut this pin away (blocked pin should be on
   the OUTER edge of the board).

   Powering the pi 3 from the NES 5V rail seems to work fine once
   I'm not shorting 5v and ground [shocked/embarrased face emoji].
   It may have some trouble under load, but like at least make -j 12
   and snestest.exe run fine (and we don't need to use any
   peripherals anyway).

   isolcpus might be the right way to get a dedicated CPU or two.
   also take a look at: man 2 sched_setaffinity

   Shout-out to william for a 40-pin IDE cable from his server farm.
   Works great. Palette was a little flashy when it started up, but
   once it got "warm", I think it works better out of the box than
   the zero did. This is a bit reassuring that I haven't just
   overfit to the timing on that thing.

   I think that there are a bunch of sprites still on the screen,
   in the left column and one near the right. Should clear these
   in CPU code.

  - snes on pi3 -

   It works! I'm definitely not hitting the deadlines, so if I try
   to emulate during vblank it basically doesn't work. But if I only
   do it once every 4 frames, then I can see that it's obviously
   emulating, taking input, and converting. It actually looks fairly
   decent... if I could get rid of the flashing maybe it'd be in an
   acceptable state without any new Convert code.

   I ran it in another thread. It "worked", but:
     - It's clearly running slower than real-time. Was my benchmark
       inaccurate because it was the opening of the game, which
       is very graphically simple? Hmm
     - We get a lot of glitches, I think more than for static
       images (although it's a bit hard to distinguish it from Convert
       noise).
     - Sometimes the screen gets totally messed up, and all red
       (so probably we are not responding to knocks correctly and
       the CPU is telling us this.
     - It does run (once at least) with disable_interrupts, which is
       "good", although still very noisy in that condition.
     - possibly the sched/mlock stuff is hurting us here?
     - After playing for a while, it'll sometimes drop the ethernet
       connection, which I think happens when interrupts are enabled
       but ppuppy never yields to the OS (I guess it is always
       schedulable with highest priority so it starves all other
       tasks, including sshd?). This seems to corroborate the previous.
     - Aha, in fact: Since these two processes share a mutex, it's
       possible that priority inversion is happening.
     
    - Actually I think I am getting a lot of dropouts because the
      Convert code is too slow. Even switching MakePalette to use
      constant greyscale palette gave a noticeable speedup (still
      plenty of jank).       

    - Disabling palette search (always use palette 0) made it play
      apparently realtime. So I guess we wanna make convert much
      faster (probably direct from 565, maybe using a table for
      closest-color lookup?) when in SNES mode. Seems doable.

    - There's still something fishy here though..

   to do: Check that it's actually utilizing multiple cores?
    - mlock/fifo on the snes thread (didn't make noticeable difference)
    - set processor affinity (same)
    - isolcpu on the CPUs we're using. (only tried #3)

 - it's crapping out again -

   I get a gibberish screen, noisy, plus all red/pink because we're
   not responding to knocking correctly. Occasionally, it syncs and
   looks fine.

   One obvious clue is that the red light on the pi blinks: It comes
   on when the image is OK, and goes off when it's bad. This is
   the power light, and blinking is supposedly when the 5v supply is
   under voltage (<4.6v). 

   By default it looks like the pi is set to "ondemand" scaling
   between 600mhz and 1.4ghz.
   cat /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_cur_freq
   cat /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_min_freq
   cat /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq

   echo powersave > /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
   echo performance > /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
   (or powersave ondemand conservative)

   Turn off hdmi:
   tvservice -o

   I turned off bluetooth (and uart I guess?):

   In boot.cfg:
   dtoverlay=pi3-disable-bt

   sudo systemctl disable hciuart.service
   sudo systemctl disable bluealsa.service
   sudo systemctl disable bluetooth.service

   And actually you can just uninstall it:
   sudo apt-get purge bluez -y
   sudo apt-get autoremove -y

   The voltage is definitely lower on the pi side than it is straight off
   the cart. I think maybe the ribbon cable is actually not able to keep
   up with the current requirements of the pi (!). 

   OMG I think that was really it. switching back to the old cable works
   much better. I also think the additional current draw when emulating
   explains why the snes demo look so bad sometimes.

   It may be important to fix the CPU clock rate, btw. This may be why
   we see periods where it seems to be great and others where it's
   terrible.

   Hmm ok, well I soldered in both 5V wires in the ide cable and three
   grounds (now I have superstitions about ground loops though), and
   measured on the pi with the scope, and the voltage looks very stable
   around 4.88v when ppuppy is running (noint). 
   
   cpu freq scaling = powersave:

   SNES demo (w/ interrupts)! voltage is stable 4.82, light is solid
   red.

   (Secret sauce: Use william's cable. set frequency scaling to powersave.
    
   during make -j 4, I do see power sag on pi to like 4.68v.
   then SNES demo with performance: 4.8, solid red, weird

   also now working fine with ondemand...
   well hmm, it's working now. but why? 

   (I got a 5A power supply. This seems to have done it, at least on
   the breadboard -- light stays red even when running SNES demo. I
   also got a very short IDE cable ($11 ugh), which should reduce the
   voltage drop, but haven't tested that yet.

  - noise! -

   With the pi3, adjusting the timing definitely affects noise, but I
   was unable to get it down to where it's been before. On the other
   hand, there is clearly nose even as the thing boots, with just the
   pre-connection animation playing. This could easily just be
   electrical noise on top of a perfectly-timed ppuppy. PCB may help
   this with short traces and fewer dead-end antennas (also I think
   some of the chr rom pins are still connected, and that I never
   successfully disabled that chip?)

   Danger: Another thing it could be is memory reordering problems
   because I'm not using memory barriers in my code any more, and
   now that it's multicore, this might actually create problems
   (if true, though, disabling interrupts would help a lot, because
   no other code should be accessing peripherals if that happens?
   I guess some other app/kernel code could be running code, even
   waiting for an interrupt to happen while it spinlocks using
   a peripheral, at the moment that I disable interrupts, which
   would be bad.

 - ppuppy v2 board! -

   Tried with just CIC and EEPROM. It doesn't work. There's no reset
   blinking so I think the CIC chip is OK (and could test this by
   running another board with no CIC?). Looks like it's maybe just
   not finding the program. Did I mess something up with "mirroring"?
   15 bits (A0-A14) are wired, which would be 32768 addressable bytes.
   That's the rom size I made (manual mirroring), but wasn't exactly
   what I was running on the breadboards.

   Once again solved with the multimeter!
   I had wired /WE on the EEPROM to GND. This is wrong. It needs to
   be connected to 5V, because the sense is "enable write when low."
   This can be fixed on the ppuppy v2 board by cutting the trace
   coming off this (when looking at the back side of the board, the
   pin next to the top right pin connects to the ground plane, just
   downward. Cut that away, and wire it to the top right pin (which
   is VDD so already hooked to 5v).

   Ugh, the 40 pin header with cable actually doesn't physically fit
   under the silver crossbar thing. Should move the header up on
   future versions where I want to use ribbon cable. Also, should
   check before solding a pi there directly (maybe better to go under
   cart).

   Anyway, now it boots and sorta (?) works. It is clearly sending
   data but doesn't handshake.

   I tried just always sending 0x27. Doesn't see it as handshake.

   Tried sending all values. 0xE7 works.

   E4 binary:    1110 0100
   27 binary:    0010 0111

   so uh, somehow I got the bits all wired up exactly backwards?

   Wow, omg, they're just wired up backwards on the schematic. Check your work!

   OK, fixed. :)

 - "talk" cart -

   SysRq: The board / NES is somehow managing to send "break" on the serial port,
   which causes linux to go crazy (even corrupted the SD card). Make sure this
   is disabled (raspi-config)!

   I got reasonable output with:
    - this is pi zero w
    - two calls to DEGLITCH_READ with a for (..deglitches..) NOP in between
    - cpu scaling governor is "performance"! 1 GHz! This was noticeably the
      right choice for the zero w.
    - disable_interrupts was false

   It's definitely the case that increasing the interstitial loop yields more
   noise. On this platform, setting it to zero (but note that there's still the
   loop overhead, plus it may inhibit optimizations between the two deglitches)
   seems to be best. Also works well with disable_interrupts. Assuming that 
   this doesn't have other randomly-occurring problems I don't know about, 
   this is good enough for talk, and stop messing with it! r3586.
