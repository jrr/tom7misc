
lowercase 24 Dec 2020


on the second problem (smaller vector),
switched to a normalized point order at round 24820.

(this rapildy dropped the total err from like 17 to 12.79)

switched to permissive loss function at round 70808

(total error went up though... was like 18 and is now 51. seems wrong?)



-- sdf training started 1 Jan 2021  13:01:36 --

Got nans after about 40 rounds. Shrink to 1/4 of the examples per round
and halve the learning rate.

(23.6 eps per model with these new settings. I think it was like 40 before?)


nanned out again after 158 rounds each.
tried actually rejecting nan updates in updateweights.

22.29 eps, so the nan checks aren't THAT expensive...

made it to round 242 so far without any issues (just predicts "average" shape though)

since updateweights has protection now, putting the learning rate back up

... but then it immediately blew up (round 268)!


didn't touch it for a while but came back to about round 640.
I hit 'v' to look at some of the stimulations/errors and watched it diverge in front
of my eyes! Maybe the divergence is actually a bug caused by the UI??

Wow I'm stumped... the behavior of vlayer (which is the only key that does anything)
is really straightforward. Due to another bug it was exporting and saving on every
round, so I guess that increases the possibility of race conditions here. Added some checks
for in-boundsness. I guess technically, writing to 'dirty' with a Read lock held on
the shared-mutex is wrong, because the ExportBlahToVideo also writes to dirty, and
it's plausible that this actually has an effect since it's just a bool (conflicting
writes are probably touching other nearby values). No idea how that could lead to
the network's weights getting corrupted, though, except via general "undefined behavior".
Switched to writemutexlock to see if this ever happens again.

(Also saves a lot of time... 42 eps now)

ok nevermind, it just happened on its own after 547 rounds (each)
while unattended (and with those fixes). (It must have just been a coincidence??)


--------------------------------------------------
trying again!
New mode in updateweights (can be performance tuned) caps not the update but the
final weight value. Also not very principled. But this should make it quite hard
for it to ever produce nans, right?


even with a max weight of 8192 it quickly gets off the rails
trying again with 128!

Note that the output pedictions can be pretty big (especially early on) and that
these can then be inverted examples for the other model. This may actually be
the cause of crazy results.. so maybe we should normalize those SDFs before
inverting them (or reject them as training examples if they are too far out
of range)


even with updateweights capped to 128, we get divergence.
maybe the problem is really in the error step, so I tried clipping there
as well.

(also fixed another bug where network would not save unless you got lucky
when you stopped it, oops!)


even with a max weight of 8.0f (!) the stimulations get huge


ok then I tried again with initial weights using "He initialization". These
are bigger than what I was using before. It immediately diverged.


ran 6000 rounds over night.
these are under control at least, but making really conservative
prediction (just "average") and the error doesn't seem to be improving.
put them in sdfmodels as a good starting point?

"just predicts the average" could be what happens if we only update biases
and never weights, by the way


well, with those settings was still stalled with super low activations


okaaaaaay, trying again with fewer layers, all dense
(these have twice as many total weights though!)

31 eps, but, actually maxing out the GPU now
... aaand it blows up in 16 rounds


3 Jan 2021
Changed a few things again and now it's making progress (overnight, 7042 rounds)
I think I partly just got lucky because the lowercase model blew up,
but the uppercase one was still generating some nice average blobs, so I copied
the uppercase model oevr the lowercase one... in early stages it's fine for
them to have the same cold start (this could even be an explicit strategy in
the future to save some time training?)

 - used two hidden layers, but bigger ones (1x square, then 1.5x square, then output)
 - all dense layers
 - use sigmoid in output layer
 - "Tom initialization"
 - decay weights 0.9995f
 - 

9 Jan 2021

looking at the histograms on biases and weights, most weights are really close to zero,
on all of the layers. Both models evolved similar distributions.
Layer 0: -.05 to .069
Layer 1: -.29 to .53
Layer 2: -.04 to .025

this does suggest that the weight capping is unnecessary, at least in this epoch.

biases are also very spiky; layer 0 is close to zero, with some falloff towards the
negative side (kinda makes sense, this would be a feature that's looking for a
sum of input nodes above some threshold)
On layer 1, almost all biases are 0.0051
Layer 2 (sigmoid) has a nice histogram with lots of activity across the spectrum
from -1 to 0.5. Probably this is encoding the "average output".

At 63831 rounds, train error seems to be quite flat (around 230/example) so I'm
going to try spicing it up.

before vacuuming, we were getting around 37 eps (for each model).
on disk the model was 370,127,092 (note that dense models are more efficiently
stored)

I also increased the learning rate (it ended up at about .05). The result was
that the lowercase model seemed to diverge... it performed fine right after
vacuuming but then got bad after a few rounds of training? Again the lowercase
model is the one that exploded.
(Also, eps was similar at about 35.)


Did it again, same settings but effective training rate of .085; blew up right away.
Again with effective learning rate of 0.0319. Also blows up, at least at first?
This one looks like

A few hypotheses about this then:
 - maybe learning rates are way too high? It doesn't seem like we'd want this
   to jump around a lot once we get this deep into training.
 - Something about the inversion training examples encourages this behavior.
   A feedback loop, etc. But the sigmoid should prevent the output from
   being outside [0,1] and we think that the predictions were actually good
   in the first few rounds here.
 - Just some kind of actual bug in training?? We have seen it work
   (for example the vector version is clearly doing *something*), but there
   could still be bugs.
 - (maybe vaccuming messes up the network? but we check the inverted indices...)

It's really weird that the network actually performs well at first, but then
gets much worse error after a few rounds of training. How is that possible?

well, the lowercase side seemed to smooth out (error = 339) but the
uppercase side has a burned-in superbright region, and the eval is
bad (and looks basically constant) on both.


--------------------------------------------------


sparse model... weights go to the max (+/- clamped) by 7 rounds (maybe immediately)!
maybe we should put these histos in the training ui



after all the futzing, the thing that seemed to get it on track was to
set the learning rate (high end) to 0.01f.>



at 196961, changed decay to .999995 (from .9995) since so many weights
were almost zero.

(total error started dropping really quickly after that!)
... seems like a very worthy change. Error dropped from ~70 to ~50 after a day,
even though the learning rate has nearly bottomed out. The eval results
are almost interesting now! It's really obvious from the error history that
this got it out of the plateau.

I see a lot of weights with values at, or nearly at zero though. Seems
like we may be wasting our parameter budget then. Should think about some
kind of vacuuming + expansion?
