%

% TODO: SDF vs SFD typo factory!

\documentclass[twocolumn]{article} % {amsart}
\usepackage[top=0.75in, left=0.65in, right=0.65in, bottom=0.6in]{geometry}

\usepackage{float}

% \usepackage{url}
% \usepackage{xurl}
\usepackage[pdftex]{hyperref}


% \usepackage{code}
% \usepackage{cite}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{chessboard}
% IPA symbols. safe turns off overrides for like \! which I still want
\usepackage[safe]{tipa}

% \usepackage{chessfs}
\usepackage{adjustbox}

\usepackage[most]{tcolorbox}

\interfootnotelinepenalty=0

% lets me explicitly set a. or 1. etc. as enum label
\usepackage{enumitem}

\pagestyle{empty}

\usepackage{ulem}
% go back to italics for emphasis, though
\normalem

\usepackage{natbib}

\setlength{\footnotesep}{2em}

% \newcommand\comment[1]{}
\newcommand\sfrac[2]{\!{}\,^{#1}\!/{}\!_{#2}}

\begin{document}

\title{Lowestcase and Uppestcase letters: Further adventures in Derp Learning}
\author{Dr.~Tom~Murphy~VII~Ph.D.}\thanks{
Copyright \copyright\ 2021 the Regents of the Wikiplia Foundation.
Appears in SIGBOVIK~2021 with the
OS2TypoLinegap of the Association for Computational Heresy; {\em IEEEEEE!}
press, Verlag-Verlag volume no.~0x40-2A. 1 em}

\setchessboard{showmover=false}

\newcommand\makelowercase{{\sf make\_lowercase}}
\newcommand\makeuppercase{{\sf make\_uppercase}}

\renewcommand\th{\ensuremath{{}^{\textrm{th}}}}
\newcommand\st{\ensuremath{{}^{\textrm{st}}}}
\newcommand\rd{\ensuremath{{}^{\textrm{rd}}}}
\newcommand\nd{\ensuremath{{}^{\textrm{nd}}}}
\newcommand\at{\ensuremath{\scriptstyle @}}

\renewcommand\paragraph[1]{\smallskip{\bf #1}\enspace}

\date{1 April 2021}

\maketitle \thispagestyle{empty}

\sloppypar


\section{Introduction}

Have you ever been writing something on the internet and wanted to convey
that you ARE FEELING ANGRY? Conversely, have you ever fired back a super
quick dm and u wanted to make it clear that it was like super ca\textipa{Z}
and so u didnt use ne capitals or punctuation dots except 4 that one place
where u needed to use the international phonetic alphabet because u dont
no how to write ca\textipa{Z} as in short for casual without it lol

If so, you made use of the fact that all letters have UPPERCASE
VERSIONS (e.g.~signifying ANGER) and lowercase versions
(e.g.~signifying u dont care lol). These dimensions have other uses,
for example, it is polite to start a person's name with a capital
letter to show that you took the time to regard their humanity (as it
takes extra work to press the caps lock key, press the first letter of
their name, and then press ther caps lock key again to turn it off).
In German, Nouns start with uppercase Letters, signifying Superiority
over other grammatical Categories like Verbs and Adjectives. Lowercase
letters can be used to conserve printer ink. Actually, I'm not sure that
lowercase letters have any other uses, but let's just roll with it.

The thing is: What if I'm even MORE ANGRY THAN I WAS BEFORE? There are
some standard sorts of typographic emphasis, like I can be {\bf BOLD
  ANGRY} or \textbf{\textit{\large BIG BOLD ITALIC UNDERLINE ANGRY}}
or { \Large \textbf{\textit{\uuline{COMBINE A LOT OF THESE ANGERS}}}},
each with its own nuances, depending on the cascading style sheet or
LaTeX class file. To be even more casual than lowercase, u can learn 2
write like this, and {\scriptsize shrink away} and also \sout{cross
  out ur words in shame in advance of them even being read}, but there
are few other options for de-emphasis. Plus, when I'm FEELING PRETTY
ANGRY, TOM, how do I capitalize that already-capitalized T in order to
show the proper reverence for your humanity?

This paper is about unshackling this dimension of human expression by
introducing letterforms further along the uppercase and lowercase
dimensions. Basically, we want to know what the upper{\it er}case
version of uppercase T is, and a lower{\it er}case version of
lowercase t is.

\subsection{Induction}

Today we're just concerned with English letters, of which there are
only 26. To create an upperercase and lowerercase alphabet by hand is
O(52 pick up), which for a guy who likes drawing letters anyway and
who alphabetized Star~Wars for fun, is not much to ask. In fact I
drew such alphabets in Figure~\ref{fig:manual} just now.

\begin{figure}[ht]
% \includegraphics[width=0.9 \linewidth]{manual}
\caption{TODO} \label{fig:manual}
\end{figure}

But, why do easy fun things by hand when you can build a complicated
automatic solution which produces much worse results? Well, there is
no good reason. I could claim that this allows us to automatically
upperercase any font, which is true, but the results are at best
moderately letter-like lumps. In principle there are several other
interesting things we can do, like apply the function over and over to
approach the uppestcase and lowestcase letters. This sounds fun, but
the results themselves are not going to impress. But the story of
getting there may be interesting, and even as it turns out to be
``derp learning,'' there will be opportunities for more good puns. So
let's just roll with it!


\section{Capital A Artificial Intelligence}

% XXX introduce the meaning of the \letterform syntax somewhere?

\newcommand\letterform[1]{\tcbox[
    nobeforeafter,
    tcbox raise base,
    top=0pt,bottom=0pt,left=-1pt,right=-1pt,
    left skip=1pt,
    right skip=1pt,
    arc=2pt,outer arc=2pt,
    boxrule=0.15mm,
    colback=white,
    colframe=white!50!black
    ]{\textrm{#1}}}

We want to machine-learn~\cite{neuralnetwork} two functions,
\makelowercase\ and \makeuppercase. Each takes a letterform and
returns a letterform (we can choose how these are represented) and
does the needful, e.g. \makelowercase(\letterform{A}) should return
\letterform{a}. In order to learn this function, we'll at least need a
lot of examples to use as training data. A training example for
\makelowercase\ is a letterform and its expected corresponding
lowercase one. We can ``easily'' find a large amount of examples by
using existing fonts, and pairing their \letterform{A} with their
\letterform{a}, and so on for all 26 letters, and symmetrically for
\makeuppercase.

However, if we only give uppercase letters to \makelowercase, it
may very well learn how to generate the corresponding lowercase letter
but be unable to do anything interesting for other letterforms. This
is a problem because we want to use this function to see
what e.g.~\makelowercase(\letterform{a}) is.

This is not (only) the problem of overfitting. An overfit model could
work well on the letter \letterform{A} from one font (because it has
seen that font before) but fail on \letterform{A} from a new font. The
property that we want is that the learned function can also produce an
interesting result on a shape it's never seen before, like
\letterform{\textipa{Z}}\,. That is, it has generalized the idea of
``how to make a shape lowercase,'' not simply ``how to make a capital
A shape lowercase.''

The problem with this is that we don't have any training data other
than existing fonts to tell us what the lowercase of some arbitrary
shape should look like. Without examples of this form, the problem is
unconstrained. \makelowercase\ could learn to generate empty output
for anything it doesn't recognize as a capital letter, and still have
perfect performance on the training and test set. It is hard to
generate training data of this form (even by hand) as we don't have
much idea {\em a priori} of what a lowerercase \letterform{a} should
look like (except for e.g.~One Artist's Impression from
Figure~\ref{fig:manual}).

\newcommand\trainingexample[2]{$\langle$\letterform{#1}$,\,$\letterform{#2}$\rangle$}

\newcommand\weirdcharlo{\includegraphics[width=0.75em]{weirdchar-lo}}
\newcommand\weirdcharup{\includegraphics[width=0.75em]{weirdchar-up}}
\newcommand\lowerlowera{\includegraphics[width=0.6em]{lowerlowera}}

This brings us to the one decent idea in this paper (which by the way
only sort of works, but let's just roll with it). We can at least
express one characteristic property of the \makelowercase\ function
that ought to be true even for letterforms we don't have examples of:
It ought to be the inverse of \makeuppercase. So, we train these two
models in tandem. \makelowercase\ is fed training examples from the
sample fonts like \trainingexample{Q}{q} etc.~and \makeuppercase\ gets
\trainingexample{e}{E} etc.~as expected. We also run the current
version of \makeuppercase\ on some letter-like shapes, which produces
some other shape. For example, say that
\makeuppercase(\letterform{\weirdcharlo}) outputs
\letterform{\weirdcharup}. We have no idea if this is good or not, so
we don't update the model. However, we {\em do} provide the training
example to \trainingexample{\weirdcharup}{\weirdcharlo} to the
\makelowercase\ training queue and penalize {\em it} if it did not
predict \letterform{\weirdcharup}. In this way, whatever \makeuppercase\ is
doing, we ask \makelowercase\ to learn the inverse. We of course also
simultaneously do the symmetric thing, using the output of
\makelowercase\ to create training examples for
\makeuppercase\ (Figure~\ref{fig:cotraining}).



\begin{figure}[ht]
\includegraphics[width=0.9 \linewidth]{training}
\caption{ Simultaneously training the two models. This example
  illustrates how a pair of letterforms \letterform{A} and
  \letterform{a} from the same font becomes four training examples.
  The pair straightforwardly generates an example
  \trainingexample{A}{a} for the \makelowercase\ queue, and an example
  \trainingexample{a}{A} for the \makeuppercase\ queue. Separately, we
  supply \letterform{a} to the \makelowercase\ model, simply to get
  the current output \letterform{\lowerlowera} (no model updates are
  performed). But this pair reversed becomes a training example
  \trainingexample{\lowerlowera}{a} for the \makeuppercase\ queue.
} \label{fig:cotraining}
\end{figure}

Because \makelowercase\ is getting training examples of
uppercase/lowercase pairs from real fonts, it remains grounded on real
letters. It is also free to generate new shapes for the open domain
(outside \letterform{A}--\letterform{Z}). However, it is penalized if
its behavior is not the inverse of whatever \makeuppercase\ is
currently doing. And since we do the symmetric thing for
\makeuppercase\, there is a (slow) feedback loop between the two
models that keeps them from straying too far from the grounded
examples. The idea is that this allows them to do some creative
generalization outside their native domains, but in a way that
still has some constraint.

In practice, we don't feed arbitrary shapes to the models. We just
need something letter-like, and in fact we have a large collection of
letter-like shapes among our existing fonts! We pass already-lowercase
shapes to \makelowercase, in order to generate inversion examples for
training \makeuppercase. These shapes are clearly letter-like (they
{\em are} letters) and are also of interest to us anyway, since we
want to try to generate lowerercase and upperercase letters from
the trained models.


\section{1000001 Free Fonts}

Sprechen of Fonts, I downloaded every font I could find on the whole
internet. This was overkill. The resulting directory tree contained
over 100,000 files, many of which were duplicates. Exact duplicates
are easy to find, but since many of these files were the result of 30
years of community transmission, they had acquired various mutations.
One of the first things I did was write software to automatically
remove files that were essentially duplicates even if they weren't
exactly the same bytes.

Next, my lord, do people have bad taste! And I say this as someone who
made dozens of amateurish fonts\cite{dbzfonts} as a high school and
college student and who is contributing several new questionable
fonts\label{sec:newfonts} as a result of this paper. The database is
just filled with garbage that is unusable for this project: Fonts that
are completely illegible, fonts that are missing most of their
characters, fonts with millions of control points, Comic Sans MS,
fonts where every glyph is a drawing of a train, fonts where
everything is fine except that just the lowercase r has a width of
{\tt MAX\_INT}, and so on. So I built a UI
(Figure~\ref{fig:sortition}) for efficiently and mind-numbingly
cleaning up the database by marking fonts as broken or suitable (and
also categorizing them as serif, sans-serif, decorative, techno, etc.,
which I never used). In doing this I noticed another extremely common
problem, which was that many fonts had the same letter shapes for
uppercase and lowercase letters. This would not do for the current
application!

\begin{figure*}[ht]
\centering
  \includegraphics[width=0.9 \linewidth]{sortition}
\caption{ The interactive font data-cleaning UI. A seemingly endless
  series of fonts presents, with single keypresses putting the fonts
  into common categories such as (b)roken.
  % XXX maybe write more?
} \label{fig:sortition}
\end{figure*}

But why manually mark fonts with nearly the same upper- and
lowercase letters, when you could build a complicated automatic
solution? The first pass identified fonts whose letters were
exactly the same, but this was only a small fraction of the
problematic fonts. A common issue was that the lowercase characters
were very slightly modified versions of the uppercase ones, often
scaled and translated and then perhaps ``optimized'' during the
font export.

So, for a given font, I want to reject it if for most pairs of cased
letters \letterform{A},\letterform{a}, \letterform{a} is close to a
linear transformation of \letterform{A}. This problem can probably be
solved with math, but it didn't sound that fun. Instead I tried out
a new tool, and it worked well enough that I've now added it to the
permanent rotation: Black-box function optimizers.

\paragraph{Black-box optimization.} If you have a function and want
to find arguments that minimize its output, the most efficient
techniques are generally those like gradient descent. (In fact, the
backpropagation algorithm we use to training the neural network in
Section~\ref{sec:neural} is gradient descent on the function that
takes the model weights and produces an error value for each output
node.) The problem with this is that you need to do some math to
compute the gradient function, and anyway you need to deal with fiddly
bits (Section~\ref{sec:fiddly}) unless the function is convex and
smooth, which it will not be. If you don't want to deal with that,
and have a fast computer (and who doesn't?), black-box optimization
algorithms are worth considering. Here, the interface\footnote{
  Here a simplfied wrapper around BiteOpt~\cite{biteopt} in my {\tt cc-lib}
  library. See \url{https://sourceforge.net/p/tom7misc/svn/HEAD/tree/trunk/cc-lib/opt/}.}
  is just something like (C++):

\begin{verbatim}
  double Minimize1D(
      const std::function<double(double)> &f,
      double lower_bound,
      double upper_bound,
      int iters);
\end{verbatim}

which takes a function \verb+f+ of type \verb+double+ $\rightarrow$
\verb+double+, finite bounds on the argument's value, the maximum
number of times to call that function, and returns the argument it
found that produced the minimal value. Not as fast as gradient
descent, but in practice ``if the function is kinda smooth'' these
optimizers produce excellent results! The chief selling point for me
is that I don't need to think about anything except for writing the
function that I want minimized, which I just express in normal code.

\begin{figure}[ht]
\centering
  \includegraphics[width=0.9 \linewidth]{casealignment}
\caption{ Example alignment to reject the font {\tt
    DrippingGooExtended}. At left, \letterform{A} (red) and
  \letterform{a} (green) rendered with the identity transform, and
  their alignment ($35\%$ difference) below. At right, the transform
  found by the black-box optimizer and the resulting alignment with
  $1.7\%$ difference. Note that the shapes are still not an exact
  match (probably errors introduced in the font export process, which
  has to round the data to integers and might apply other non-linear
  transformations like curve simplification), but these are clearly
  not a useful pair for the current
  problem.} \label{fig:casealignment}
\end{figure}

In this case, I render the letterform \letterform{A} and then optimize
a four argument function taking {\tt xoff}, {\tt yoff}, {\tt xscale},
{\tt yscale}. This function renders \letterform{a} with those
parameters, then just computes the difference in the two rendered
bitmaps. This finds the best alignment of the two letterforms (under
the linear transformation) in a few hundred milliseconds
(Figure~\ref{fig:casealignment}). If the disagreement is low as a
function of the total pixels, then we say that the letters have the
same case. If enough of them have the same case, we reject the font. I
set the thresholds by looking at the P/R curve computed on random
hand-labeled examples (Figure~\ref{fig:samecasepr}).

\begin{figure}[ht]
\centering
  \includegraphics[width=0.9 \linewidth]{samecasepr}
\caption{ Precision--recall curve for automatically detecting fonts
  that have basically the same upper- and lowercase shapes. It's good!
  This is how you want 'em to look!
} \label{fig:samecasepr}
\end{figure}

\medskip

I labeled fonts using the UI until I had 10,000 that were clean enough
for a training set and passed the same-case heuristic.

\section{The simplest thing that might work} \label{sec:vectorversion}

Before getting fancy (which we we will) it's good engineering hygiene
to try the simplest thing that might just work (it doesn't). Fonts
are represented as vector data (lines and quadratic B\'ezier curves).
Can we just train a network that takes these lines and curves as input
and predicts the lower- or uppercased letter in the same format? (No.)

We'll at least put the data in a somewhat normalized form. The neural
network will take a fixed number of inputs to a fixed number of
outputs, so a simple approach is to decide on some maximum number
control points per letter, and only try training on fonts whose
letterforms fit within that budget. Letterforms can be made of
multiple contours (e.g. a stacked \letterform{g} typically has two
holes in it, and \letterform{j} has two disjoint parts). I found that
most clean fonts had three or fewer contours, and when sorting them by
descending length, basically all of them fit within 100, 25, and 16
endpoints for the three. So, I only train on fonts where all of the
letters fit within this budget.\footnote{It would not be a good idea
  to reject only the letters that don't fit, because it might result
  in the network being trained on more \letterform{l}s (tends to be
  simple) than \letterform{g}s (tends to be complex).}

% TODO: Illustrate a letterform and its contours?

Rather than try to work with both lines and B\'ezier curves, I
normalize each contour to only contain B\'eziers, by turning a line
segment into an equivalent B\'ezier with its control point at the
midpoint. This frees us from having to distinguish the two types in
the data. We also need each of the three contours to not be {\em too
  short}, so I fill out the fixed-size buffers by repeating the last
point. This is not great but does have the correct meaning (bunch of
useless zero-length edges). It has the property that any predicted
data can be rendered and has a straightforward point-by-point error
function (which might not be the case if we were predicting a dynamic
number of points).

The network I trained has an input layer size of $570 = (100 + 25 + 16)
\times 4 + 3 \times 2$ (one control point and one end point per
B\'ezier curve), plus a starting point for each of the three contours.
The output layer is the same size, plus 26 (see below). There are
three hidden layers of size 308, 308, 360. The first layer is dense
and the remainder are sparse,
%(two thirds to a half of the weights are zero),
for just about 1 million total parameters. All layers are leaky
rectified linear units ({\tt x > 0 ? x : 0.1 * x}), which is fast to
compute and better than sigmoids in the output since correct values
will not just be 0 and 1. If you're taking notes, don't, as again this
does not work well, and I don't know how people figure out what the
right network size is anyway. I just made it up. You can give {\em me}
your notes.

\paragraph{Bonus outputs.} The output includes the predicted shape,
and also 26 individual predictors for each of the 26 letters. So a
training example is actually like \letterform{C} $\rightarrow$
\letterform{c} $[0, 0, 1, 0, 0, \ldots, 0]$, with the 1 in the third
place because C is the third letter. We don't need these outputs for
the problem itself (e.g. to lowercase new letter shapes), but there
are several ideas behind this. First, the lowercasing function we're
trying to learn does depend on the letter of the alphabet being
lowercased (in an extreme case, consider the lowercase-L
\letterform{l} and the uppercase-i \letterform{I}, which look the same
in many fonts but have different lowercase letterforms). By asking the
network to learn this as well (it is penalized when it gets the
prediction wrong), it must learn features that allow it to distinguish
different letters, and those features are available for use by outputs
we {\em do} care about. This is an easy way to coax it to learn
features that I know are meaningful without having to actually
engineer feature extractors by hand (or first train separate models,
etc.). Similarly, I could have asked it to predict whether the font
is italic, serif, the character's width, or anything else I have on
hand. Perhaps the most useful thing is that it's very clear what the
right answer is, so it gives me an easy way to see if the network is
learning anything {\em at all}. (It does.) Finally, we can do some
silly stuff with these; see Section~\ref{sec:hallucination}.

\newcommand\nan{\textsf{NaN}}
\renewcommand\inf{\textsf{inf}}

I trained the network using a home-grown (why??) GPU-based package
that I wrote for {\em Red i removal with artificial retina
  networks}~\cite{murphy2015redi}---an example of ``lowercase i
artificial intelligence''---and have improved as I repurposed it for
other projects, such as {\em Color- and piece-blind
  chess}~\cite{murphy2019blind}. It is ``tried and true'' in the
sense that ``every time I tried using it, I truly wanted to throw
my computer out the window, and retire to a hermitage in the glade
whenceforth I shall nevermore be haunted by a model which has
overnight become a sea of \inf{}s and \nan{}s.''

\begin{figure}[ht]
\centering
  \includegraphics[width=0.9 \linewidth]{trainingvector}
\caption{ Screenshot of {\tt train.exe} running on the first vector-based
  version of the problem. Shown is the \makelowercase\ model's output
  (bottom) on four shapes from four fonts (top). Some dust in between
  is the activation of the network's layers. At the very bottom, the
  26 predictions for ``what letter is this?''. The output for
  \letterform{j} is not too bad; you can see the distinct dot (a
  separate contour) and it sort of looks like a \letterform{j}. The
  \letterform{e} also has two pieces as expected but is otherwise
  garbage. The model is unsure whether the second input is an H
  or a K, and has predicted a shape sort of ambiguously between those
  two. The \lowercase{z} is also an embarrassment.
} \label{fig:trainingvector}
\end{figure}

I was so {\tiny confident} that this wouldn't work that I only trained
a \makelowercase\ model and didn't even worry about the more
complicated simultaneous training setup yet. I ran this for about
22,000 rounds, some 90 million training examples. Indeed it does not
work (Figure~\ref{fig:trainingvector}). It is not a total catastrophe,
though. We can tell from the 26 bonus outputs that the model can
clearly recognize letters (though perhaps just by memorization). Some
of the shapes it generates are along the right lines. ({\em Along the
  right lines}, get it??) I did not feel ANGRY at these results
because I expected it to not really work. Still, it ``has output'' and
so it can be used to generate a font. I made every glyph in Comic Sans
MS~\cite{comicsans} lowercase using the model (with the exception of the
\letterform{\%} character, which has too many contours---{\it five}!).
Mostly this model produces small, non-confident scrawls, like little
grains of sand, so this font is called {\bf Comic Sands}
(Figure~\ref{fig:comicsands}). The TrueType version can be downloaded
from my website and installed for your desktop publishing
needs.\footnote{Font downloads are available at
  \url{http://tom7.org/lowercase/}.}

\begin{figure*}[tp]
\centering
  \includegraphics[width=0.95 \linewidth]{comicsands}
\caption{
  Type specimen for the generated font {\bf Comic Sands}. This is
  the hateful Comic Sans MS run through an early vector-based lowercasing
  model (Section~\ref{sec:vectorversion}). At top are Comic Sans's letterforms
  \letterform{A}--\letterform{Z} run through the model and so ``made
  lowercase'' (it's obviously garbage). Next are
  \letterform{a}--\letterform{z},
  made even more lowercase. Also rubbish.
  At the bottom are the illegible pangrams
  ``Dr.~Jock, TV Quiz Ph.D., bags few lynx''
  and ``Sphinx of black quartz, judge my vow!'' Although the output
  barely resembles letters, it does have a certain dynamic
  Rorschach aesthetic, like a collection of delicate moths pinned
  to paperboard, that one could consider framing or publishing in
  the proceedings of SIGBOVIK 2021. It is certainly an improvement on
  the original font.
} \label{fig:comicsands}
\end{figure*}


\subsection{Just try making it more complicated!} \label{sec:complicated}

This problem of predicting the vector shape directly is a lot to ask
of a neural network, at least set up this way. One thing that did not
sit well with me is that the network could in principle generate a
perfect-looking result, but because it didn't have the points in the
expected order, it would be penalized. This makes it harder to learn,
and more prone to overfitting.\footnote{For example, imagine if the database
contains two versions of Helvetica that just have their points in a
different order---which is very likely the case btw---the model will
have to learn how to distinguish between these, but using information
we just don't care about.} This was one case where my questionable
reflex to make things more complicated did pay off!

First, I reduced the number of points in the input and output.
Reducing the dimension of the function being learned generally makes
learning a lot faster. This had the side-effect of reducing the number
of eligible fonts (by about half), and by nature these fonts are
simpler shapes. These effects alone could be responsible for the
improved performance of this second try.

I also output each contour's points in a normalized order, starting
from the point closest to the origin. This removes one needless
degree of freedom.\footnote{We can see how this manifests in
  the biases on the output layer, which are a proxy for the
  ``average prediction''. In the first model, because of the
  unstructured order, these are mostly near 0.5 (center of the
  character) or 0.0 (degenerate, unused contours). In this new model,
  the distribution of biases is much more flat; it can learn that
  ``the first point tends to be near 0.25,0.25'' and ``the seventh
  point tends to be near 0.64,0.3.''}

Aside from the changes in the input (now 254 nodes) and output (280),
this second version has three sparse hidden layers of size 508, 508,
and 560 nodes; the first two are dense and the latter sparse. The
final model after some pruning had 609k parameters.

As this was training, I worked on another improvement. Ideally we
would compute the difference between the predicted shape and the
expected shape, regardless of how they're drawn. Aside from being a
bit computationally challenging, this won't really work because we
need to attribute error directly to each output in order to actually
update the model in training. I spent a valuable vacation day writing
a routine to compute the best alignment of points between the actual
and expected outputs (Figure~\ref{fig:trainingrotate}). Aside from being
harder than it looked, my alignment code ended up being pretty slow
relative to the rest of training, even worse since it ran on the CPU
instead of GPU, which reduced the training speed by $50\%$. I let it
run for 80,000 rounds, some 331 million training examples, but
eventually got bored of waiting on this approach that was slow to
train and seemed like a complicated version of an bad, oversimplified
approach. So, I control-C'd that thing and threw this whole endeavour
in the trash! But I must have confused the Recycle~Bin icon with the
fairly complicated export-to-TrueType Font process that I built,
because I ran the model on the venerable Futura~\cite{futura} font and
generated {\bf Futurda} (Figure~\ref{fig:futurda}).
% 
% \begin{figure}[ht]
% \centering
%   \includegraphics[width=0.5 \linewidth]{rotatej}
% \caption{ Example alignment (from debug UI) of predicted shape (blue)
%   to expected shape (green). We require each point to be mapped (red)
%   to a point from the expected contour in a monotonic order (but
%   several can be mapped to the same one), so that we can attribute
%   error to each point. I hope it was fun to write this code because
%   it was too slow to do pretty much anything but generate this figure!
% } \label{fig:rotatej}
% \end{figure}
%

\begin{figure}[ht]
\centering
  \includegraphics[width=0.95 \linewidth]{trainingrotate}
\caption{ Screenshot (somewhat compacted) of training from near the
  final round of the vector model's training, illustrating the
  permissive loss function that finds the best alignment. At the bottom are
  the predicted lowercase shapes (blue), also shown with their
  expected shape (green). We require each point to be mapped (red)
  to a point from the expected contour in a monotonic order (but
  several can be mapped to the same one), so that we can attribute
  error to each point.
} \label{fig:trainingrotate}
\end{figure}

% XXX downloadable

\begin{figure*}[tp]
\centering
  \includegraphics[width=0.95 \linewidth]{futurda}
\caption{ Type specimen for the generated font {\bf Futurda}. This is
  the classic font Futura, run through the final, improved
  vector-based model (Section~\ref{sec:complicated}) to make each
  letter lowercase. The letterforms \letterform{A}--\letterform{Z}
  (top) become quite readable lowercase versions. The extra-lowercase
  \letterform{a}--\letterform{z} are also almost legible, but are
  mostly just scaled-down and screwed up versions of the lowercase
  letterforms. Could definitely imagine this appear in the
  ``distressed fonts'' category of a 10001 Free TrueType Fonts CD-ROM
  in the 1990s, though.
} \label{fig:futurda}
\end{figure*}

% lowerercase Franklin Gothic. Franklin mint... not guaranteed
% to go up in value


% obviously, need to express different cases of anger at various
% problems or successes?

% the sdf problem

\section{SDFs}

Don't give up! The fixed-size input/output of neural networks is
better suited to something like an array of pixels, and fonts can of
course be represented this way as well. To stay in the realm of what
my desktop computer with a single GeForce 1080 can do, I wanted to
keep the number of inputs and outputs pretty small. There's already an
excellent technique for representing font data as compact bitmaps,
which comes from computer graphics, called Signed Distance Fields
(SDFs)~\cite{green2007improved}. In a standard rasterization of a
font, each pixel of a bitmap contains 1 or 0, or perhaps an
anti-aliased value in-between. In an SDF, the bitmap instead contains
the distance to the nearest edge, and is signed (e.g. values inside
the shape are $> 0$, value outside are $< 0$). Actually in
practice we offset and saturate the values so that they are all in
$[0,1]$ (or bytes in $[0,255]$), with some nonzero ``on-edge value''
(say, 0.5) standing for ``distance 0''. In order to display the font
at the size of your choice, you then resize the SDF image with bilinear
interpolation, and then simply threshold the image. This works
surprisingly well (Figure~\ref{fig:sdf}).

\begin{figure}[ht]
\centering
  \includegraphics[width=0.95 \linewidth]{sdf-figure}
\caption{
  The signed distance function representation of a letterform.
  At the very left, a $36\times 36$ pixel rasterization of
  the character without anti-aliasing, for comparison. Any
  scaling of this will have chunky pixel artifacts. Next,
  a $36\times 36$ pixel SDF of same. Third, simply scaling
  that $36\times 36$ image to $180 \times 180$ pixels with
  bilinear sampling. Finally, that image thresholded to
  produce a $180 \times 180$ pixel rasterization, which is
  far superior despite being derived from a $36 \times 36$ pixel
  bitmap. Typically this process is performed at an even higher
  scale and then downsampled to produce an anti-aliased image.
} \label{fig:sdf}
\end{figure}

SDFs seem well-suited for machine learning. They contain more
information per pixel than a plain bitmap, so we can use a smaller
input and output size. On the input side, extremal pixels that would
almost never be set in a bitmap still have significant information
(distance to the character). The error function is just pixel-by-pixel
difference. The rendering of the output is inherently tolerant of some
noise because of the sampling and thresholding. So, this seemed like
it might work really well! (It doesn't work that well.)

I computed some stats on the font database, and determined the
following parameters for the fixed-size SDFs we train on. The images
are $36 \times 36$ pixels. The character box is placed such that there
are $2$ pixels of top padding, and $9$ pixels of left and bottom
padding. The character box is only ``nominal'' in the sense that the
font's contours can exceed its bounds, and this is completely normal
for a letter like \letterform{j} (which goes below the baseline and
often hangs to the left of the origin as well). I used an ``on-edge
value'' of 220 (because much more of the SDF is outside the letter
than inside) and the distance is scaled as 15 units per pixel (so that
pixels on the outer edge often have non-zero values). Compared to the
first version, I was somewhat more permissive in what fonts I trained
on, since there was no inherent limit to the number of contours or their
complexity. I did exclude fonts whose rasterizations exceeded the
bounds of the SDF, which is possible (very wide \letterform{W} or
low-descending \letterform{j} perhaps) but rare.

\section{The care and feeding of sparse matrices} \label{sec:neural}

Having committed to the representation, again it is ``just'' a matter
of heating up the GPU to apply some linear and non-linear transforms.
The initial network had an input size of $36 \times 36 = 1296$ for the
SDF, and the output the same plus $26$ bonus outputs (one for each
letter, as before). I started with three hidden layers of $1296$, $1296$,
and $2916$ nodes, each sparse ($80\%$ of the weights are zero). Again,
don't take notes. This one works a bit better than before, but still
not impressive. The node references are assigned spatially (something
like the $20\%$ of the nodes on the previous layer that are closest to
the next layer's node) but due to a bug the spatial locality is
actually pretty strange.
% XXX if I show layer weights, forward-reference them here:
% It's responsible for the XXX
Every layer's transfer function is ``leaky relu'' again. It would
definitely make sense to use convolutional layers for this problem, as
features like serifs, lines, curves, and so on could appear throughout
the input and output. I just haven't built support for that in my
weird home-grown software, yet.

I also adapted my weird home-grown software to train the
\makeuppercase\ and \makelowercase\ models simultanously. Two models
fit easily in GPU memory, with plenty of space for a stream of
training data (one training instance is only about 10kb). The only
challenging thing is arranging for them to feed each other generated
``inversion'' examples (Figure~\ref{fig:cotraining}), but this is just
a matter of programming, thank God. I should remember to do projects
that are mostly a matter of programming. Each round, $25\%$ of the
batch consists of inverted examples from the symmetric model's output
from a recent round. Training happens asynchronously, but I make sure
that one model is not allowed to get more than 2 rounds ahead of the
other, because I want this feedback loop to be somewhat tight.

So I did that and let it run for a month. Actually I had to start over
several times with different parameters and initialization weights
because it would get stuck (Figure~\ref{fig:nans}) right away. I
prayed to the dark wizard of hyperparameter tuning until he smiled
upon my initial conditions, knowing that somewhere he was adding
another tick-mark next to my name in a tidy but ultimately scary
Moleskine notebook that he bought on a whim in the Norman Y.~Mineta
San Jose International Airport on a business trip, and still feels
was overpriced for what it is.

\begin{figure}[ht]
\centering
  \includegraphics[width=0.9 \linewidth]{nans}
\caption{
  Divergent training after only 29 rounds. We have \nan{} total error
  (is that good?). The example in column one is an inversion example
  generated by the \makeuppercase\ model, which is why it also looks
  like the Tunguska event, just of the opposite sign. The other two
  are regular inputs, whose predicted outputs are black holes. Start
  over!
} \label{fig:nans}
\end{figure}

The training error over time appears in
Figure~\ref{fig:sdfmergederror}. It looks like the ones I have seen in
machine learning papers, although I don't like to read other people's
papers because it just seems like spoilers.

\begin{figure}[ht]
\centering
  \includegraphics[width=0.99 \linewidth]{sdfmergederror}
\caption{ The training error for the SDF models. The red curve is the
  \makeuppercase\ model, which generally has a higher error rate
  (perhaps simply because uppercase letters usually have more pixels
  set) and blue is \makelowercase. The first few rounds have error
  that's off the charts, well above 100. The most dramatic event is
  around round 200,000, where I reduced the weight decay factor to
  $0.999995$ (from $0.9995$). I guess you just need more nines to be
  more reliable. There are some other visible peaks, which occur when
  I do things like remove nodes with very low weights or which are
  almost never activated (Section~\ref{sec:fiddly}). These momentarily
  increase error but it is easily fine-tuned away (e.g. by learning
  new biases). The peak at around 1.4M rounds is when I added a new
  layer to the end of the model, which does seem to create a new
  training regime (clear downward slope now); but this also
  significantly increases the training cost per round. Even after
  2,000,000 rounds, the network is still apparently improving, but at
  a speed of about 1 pixel per several weeks. Eventually the extremely
  strict SIGBOVIK deadlines mean you just have to call it done.
} \label{fig:sdfmergederror}
\end{figure}


% you propagate my back, I'll propagate yours


\label{sec:fiddly}

% TODO loss over time

% some other things that are worth a mention:
%   autoparallel
%   efficiency improvements (restrict, compile-time constants)

% no copyright intended

% stretch the loss function near the onedge value

%

\section{Perfect letters, hallucinated} \label{sec:hallucination}


lorem ipsum (Figure~\ref{fig:perfect})


\begin{figure*}[tp]
\centering
  \includegraphics[width=0.9 \linewidth]{perfecthallucination}
\caption{
%  XXX Why doesn't this caption come out the way I'm expecting?
%  (in amsart style... can't have two figure*s on the same page?)
   Type specimen for the generated font {\bf Perfect Hallucination}.
  TODO write a description!
} \label{fig:perfect}
\end{figure*}

% LSDF


dolor sit amet

\section{Chess-playing}

% autotracing

% CAPS LOCK, LOCK, CAPTAIN'S LOCK

% when two letters coincide, a shift-reduce conflict

% cool s

% lowercase uppercase letters etc.

\section{Acknowledgements}

For this project I used \verb+stb_truetype.h+ from the excellent stb
collection of single-file libraries.~\cite{stb} I did push its limits
somewhat and ``automatically discovered'' assertion failures and other
crashes (e.g. during blackbox optimization on all 100k fonts), but it
saved a lot of time. This library only helps with reading the fonts
and generating SDFs. To generate TTFs, I had to write my own pipeline,
which generates FontForge's .SFD (a typo factory right there) files,
and then did the final export with FontForge~\cite{fontforge}. Thanks
Jason Reed for this suggestion.


\nocite{murphy2019blind}
\nocite{murphy2019eloworld}
\nocite{stb}

\bibliography{lowercase}{}
\bibliographystyle{plain}

\end{document}

